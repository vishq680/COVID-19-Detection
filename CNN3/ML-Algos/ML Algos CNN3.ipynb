{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6eb635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import copy\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5149f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9fd948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['Covid','No Covid']\n",
    "num_classes=2\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91a2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=\"./data\",transform=transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Resize([227,227]),\n",
    "#                                                             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "                                                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "673ba27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8088\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "trainset,testset=torch.utils.data.random_split(dataset,[round(0.8*len(dataset)),round(0.2*len(dataset))],generator=torch.Generator().manual_seed(42))\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15cedc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def get_first_FC_Layer(self,x):\n",
    "            x=self.representation_network(x).flatten(1)\n",
    "            x=self.classification_network[0](x)\n",
    "            return x;\n",
    "    def get_Representation_Net(self,x):\n",
    "            x=self.representation_network(x).flatten(1)\n",
    "            return  x;\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.representation_network=nn.Sequential(\n",
    "            nn.Conv2d(3,32,3), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "            nn.Conv2d(32,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "        )\n",
    "        self.classification_network=nn.Sequential(\n",
    "            nn.Linear(18432,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        x=self.representation_network(x)\n",
    "#         print(x.shape)\n",
    "        # flattening of the vector=> same dimension of first index(batch size) , everythign else is flattened(-1)\n",
    "        x=x.view(x.size(0),-1)\n",
    "#         print(x.shape)\n",
    "        x=self.classification_network(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c5ce63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CNN()\n",
    "net.load_state_dict(torch.load(\"CNN-3_final.pth\").state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387788ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 512)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "Done with the batch: 405\n",
      "Done with the batch: 406\n",
      "Done with the batch: 407\n",
      "Done with the batch: 408\n",
      "Done with the batch: 409\n",
      "Done with the batch: 410\n",
      "Done with the batch: 411\n",
      "Done with the batch: 412\n",
      "Done with the batch: 413\n",
      "Done with the batch: 414\n",
      "Done with the batch: 415\n",
      "Done with the batch: 416\n",
      "Done with the batch: 417\n",
      "Done with the batch: 418\n",
      "Done with the batch: 419\n",
      "Done with the batch: 420\n",
      "Done with the batch: 421\n",
      "Done with the batch: 422\n",
      "Done with the batch: 423\n",
      "Done with the batch: 424\n",
      "Done with the batch: 425\n",
      "Done with the batch: 426\n",
      "Done with the batch: 427\n",
      "Done with the batch: 428\n",
      "Done with the batch: 429\n",
      "Done with the batch: 430\n",
      "Done with the batch: 431\n",
      "Done with the batch: 432\n",
      "Done with the batch: 433\n",
      "Done with the batch: 434\n",
      "Done with the batch: 435\n",
      "Done with the batch: 436\n",
      "Done with the batch: 437\n",
      "Done with the batch: 438\n",
      "Done with the batch: 439\n",
      "Done with the batch: 440\n",
      "Done with the batch: 441\n",
      "Done with the batch: 442\n",
      "Done with the batch: 443\n",
      "Done with the batch: 444\n",
      "Done with the batch: 445\n",
      "Done with the batch: 446\n",
      "Done with the batch: 447\n",
      "Done with the batch: 448\n",
      "Done with the batch: 449\n",
      "Done with the batch: 450\n",
      "Done with the batch: 451\n",
      "Done with the batch: 452\n",
      "Done with the batch: 453\n",
      "Done with the batch: 454\n",
      "Done with the batch: 455\n",
      "Done with the batch: 456\n",
      "Done with the batch: 457\n",
      "Done with the batch: 458\n",
      "Done with the batch: 459\n",
      "Done with the batch: 460\n",
      "Done with the batch: 461\n",
      "Done with the batch: 462\n",
      "Done with the batch: 463\n",
      "Done with the batch: 464\n",
      "Done with the batch: 465\n",
      "Done with the batch: 466\n",
      "Done with the batch: 467\n",
      "Done with the batch: 468\n",
      "Done with the batch: 469\n",
      "Done with the batch: 470\n",
      "Done with the batch: 471\n",
      "Done with the batch: 472\n",
      "Done with the batch: 473\n",
      "Done with the batch: 474\n",
      "Done with the batch: 475\n",
      "Done with the batch: 476\n",
      "Done with the batch: 477\n",
      "Done with the batch: 478\n",
      "Done with the batch: 479\n",
      "Done with the batch: 480\n",
      "Done with the batch: 481\n",
      "Done with the batch: 482\n",
      "Done with the batch: 483\n",
      "Done with the batch: 484\n",
      "Done with the batch: 485\n",
      "Done with the batch: 486\n",
      "Done with the batch: 487\n",
      "Done with the batch: 488\n",
      "Done with the batch: 489\n",
      "Done with the batch: 490\n",
      "Done with the batch: 491\n",
      "Done with the batch: 492\n",
      "Done with the batch: 493\n",
      "Done with the batch: 494\n",
      "Done with the batch: 495\n",
      "Done with the batch: 496\n",
      "Done with the batch: 497\n",
      "Done with the batch: 498\n",
      "Done with the batch: 499\n",
      "Done with the batch: 500\n",
      "Done with the batch: 501\n",
      "Done with the batch: 502\n",
      "Done with the batch: 503\n",
      "Done with the batch: 504\n",
      "Done with the batch: 505\n",
      "Done with the batch: 506\n",
      "Done with the batch: 507\n",
      "Done with the batch: 508\n",
      "Done with the batch: 509\n",
      "Done with the batch: 510\n",
      "Done with the batch: 511\n",
      "Done with the batch: 512\n",
      "Done with the batch: 513\n",
      "Done with the batch: 514\n",
      "Done with the batch: 515\n",
      "Done with the batch: 516\n",
      "Done with the batch: 517\n",
      "Done with the batch: 518\n",
      "Done with the batch: 519\n",
      "Done with the batch: 520\n",
      "Done with the batch: 521\n",
      "Done with the batch: 522\n",
      "Done with the batch: 523\n",
      "Done with the batch: 524\n",
      "Done with the batch: 525\n",
      "Done with the batch: 526\n",
      "Done with the batch: 527\n",
      "Done with the batch: 528\n",
      "Done with the batch: 529\n",
      "Done with the batch: 530\n",
      "Done with the batch: 531\n",
      "Done with the batch: 532\n",
      "Done with the batch: 533\n",
      "Done with the batch: 534\n",
      "Done with the batch: 535\n",
      "Done with the batch: 536\n",
      "Done with the batch: 537\n",
      "Done with the batch: 538\n",
      "Done with the batch: 539\n",
      "Done with the batch: 540\n",
      "Done with the batch: 541\n",
      "Done with the batch: 542\n",
      "Done with the batch: 543\n",
      "Done with the batch: 544\n",
      "Done with the batch: 545\n",
      "Done with the batch: 546\n",
      "Done with the batch: 547\n",
      "Done with the batch: 548\n",
      "Done with the batch: 549\n",
      "Done with the batch: 550\n",
      "Done with the batch: 551\n",
      "Done with the batch: 552\n",
      "Done with the batch: 553\n",
      "Done with the batch: 554\n",
      "Done with the batch: 555\n",
      "Done with the batch: 556\n",
      "Done with the batch: 557\n",
      "Done with the batch: 558\n",
      "Done with the batch: 559\n",
      "Done with the batch: 560\n",
      "Done with the batch: 561\n",
      "Done with the batch: 562\n",
      "Done with the batch: 563\n",
      "Done with the batch: 564\n",
      "Done with the batch: 565\n",
      "Done with the batch: 566\n",
      "Done with the batch: 567\n",
      "Done with the batch: 568\n",
      "Done with the batch: 569\n",
      "Done with the batch: 570\n",
      "Done with the batch: 571\n",
      "Done with the batch: 572\n",
      "Done with the batch: 573\n",
      "Done with the batch: 574\n",
      "Done with the batch: 575\n",
      "Done with the batch: 576\n",
      "Done with the batch: 577\n",
      "Done with the batch: 578\n",
      "Done with the batch: 579\n",
      "Done with the batch: 580\n",
      "Done with the batch: 581\n",
      "Done with the batch: 582\n",
      "Done with the batch: 583\n",
      "Done with the batch: 584\n",
      "Done with the batch: 585\n",
      "Done with the batch: 586\n",
      "Done with the batch: 587\n",
      "Done with the batch: 588\n",
      "Done with the batch: 589\n",
      "Done with the batch: 590\n",
      "Done with the batch: 591\n",
      "Done with the batch: 592\n",
      "Done with the batch: 593\n",
      "Done with the batch: 594\n",
      "Done with the batch: 595\n",
      "Done with the batch: 596\n",
      "Done with the batch: 597\n",
      "Done with the batch: 598\n",
      "Done with the batch: 599\n",
      "Done with the batch: 600\n",
      "Done with the batch: 601\n",
      "Done with the batch: 602\n",
      "Done with the batch: 603\n",
      "Done with the batch: 604\n",
      "Done with the batch: 605\n",
      "Done with the batch: 606\n",
      "Done with the batch: 607\n",
      "Done with the batch: 608\n",
      "Done with the batch: 609\n",
      "Done with the batch: 610\n",
      "Done with the batch: 611\n",
      "Done with the batch: 612\n",
      "Done with the batch: 613\n",
      "Done with the batch: 614\n",
      "Done with the batch: 615\n",
      "Done with the batch: 616\n",
      "Done with the batch: 617\n",
      "Done with the batch: 618\n",
      "Done with the batch: 619\n",
      "Done with the batch: 620\n",
      "Done with the batch: 621\n",
      "Done with the batch: 622\n",
      "Done with the batch: 623\n",
      "Done with the batch: 624\n",
      "Done with the batch: 625\n",
      "Done with the batch: 626\n",
      "Done with the batch: 627\n",
      "Done with the batch: 628\n",
      "Done with the batch: 629\n",
      "Done with the batch: 630\n",
      "Done with the batch: 631\n",
      "Done with the batch: 632\n",
      "Done with the batch: 633\n",
      "Done with the batch: 634\n",
      "Done with the batch: 635\n",
      "Done with the batch: 636\n",
      "Done with the batch: 637\n",
      "Done with the batch: 638\n",
      "Done with the batch: 639\n",
      "Done with the batch: 640\n",
      "Done with the batch: 641\n",
      "Done with the batch: 642\n",
      "Done with the batch: 643\n",
      "Done with the batch: 644\n",
      "Done with the batch: 645\n",
      "Done with the batch: 646\n",
      "Done with the batch: 647\n",
      "Done with the batch: 648\n",
      "Done with the batch: 649\n",
      "Done with the batch: 650\n",
      "Done with the batch: 651\n",
      "Done with the batch: 652\n",
      "Done with the batch: 653\n",
      "Done with the batch: 654\n",
      "Done with the batch: 655\n",
      "Done with the batch: 656\n",
      "Done with the batch: 657\n",
      "Done with the batch: 658\n",
      "Done with the batch: 659\n",
      "Done with the batch: 660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 661\n",
      "Done with the batch: 662\n",
      "Done with the batch: 663\n",
      "Done with the batch: 664\n",
      "Done with the batch: 665\n",
      "Done with the batch: 666\n",
      "Done with the batch: 667\n",
      "Done with the batch: 668\n",
      "Done with the batch: 669\n",
      "Done with the batch: 670\n",
      "Done with the batch: 671\n",
      "Done with the batch: 672\n",
      "Done with the batch: 673\n",
      "Done with the batch: 674\n",
      "Done with the batch: 675\n",
      "Done with the batch: 676\n",
      "Done with the batch: 677\n",
      "Done with the batch: 678\n",
      "Done with the batch: 679\n",
      "Done with the batch: 680\n",
      "Done with the batch: 681\n",
      "Done with the batch: 682\n",
      "Done with the batch: 683\n",
      "Done with the batch: 684\n",
      "Done with the batch: 685\n",
      "Done with the batch: 686\n",
      "Done with the batch: 687\n",
      "Done with the batch: 688\n",
      "Done with the batch: 689\n",
      "Done with the batch: 690\n",
      "Done with the batch: 691\n",
      "Done with the batch: 692\n",
      "Done with the batch: 693\n",
      "Done with the batch: 694\n",
      "Done with the batch: 695\n",
      "Done with the batch: 696\n",
      "Done with the batch: 697\n",
      "Done with the batch: 698\n",
      "Done with the batch: 699\n",
      "Done with the batch: 700\n",
      "Done with the batch: 701\n",
      "Done with the batch: 702\n",
      "Done with the batch: 703\n",
      "Done with the batch: 704\n",
      "Done with the batch: 705\n",
      "Done with the batch: 706\n",
      "Done with the batch: 707\n",
      "Done with the batch: 708\n",
      "Done with the batch: 709\n",
      "Done with the batch: 710\n",
      "Done with the batch: 711\n",
      "Done with the batch: 712\n",
      "Done with the batch: 713\n",
      "Done with the batch: 714\n",
      "Done with the batch: 715\n",
      "Done with the batch: 716\n",
      "Done with the batch: 717\n",
      "Done with the batch: 718\n",
      "Done with the batch: 719\n",
      "Done with the batch: 720\n",
      "Done with the batch: 721\n",
      "Done with the batch: 722\n",
      "Done with the batch: 723\n",
      "Done with the batch: 724\n",
      "Done with the batch: 725\n",
      "Done with the batch: 726\n",
      "Done with the batch: 727\n",
      "Done with the batch: 728\n",
      "Done with the batch: 729\n",
      "Done with the batch: 730\n",
      "Done with the batch: 731\n",
      "Done with the batch: 732\n",
      "Done with the batch: 733\n",
      "Done with the batch: 734\n",
      "Done with the batch: 735\n",
      "Done with the batch: 736\n",
      "Done with the batch: 737\n",
      "Done with the batch: 738\n",
      "Done with the batch: 739\n",
      "Done with the batch: 740\n",
      "Done with the batch: 741\n",
      "Done with the batch: 742\n",
      "Done with the batch: 743\n",
      "Done with the batch: 744\n",
      "Done with the batch: 745\n",
      "Done with the batch: 746\n",
      "Done with the batch: 747\n",
      "Done with the batch: 748\n",
      "Done with the batch: 749\n",
      "Done with the batch: 750\n",
      "Done with the batch: 751\n",
      "Done with the batch: 752\n",
      "Done with the batch: 753\n",
      "Done with the batch: 754\n",
      "Done with the batch: 755\n",
      "Done with the batch: 756\n",
      "Done with the batch: 757\n",
      "Done with the batch: 758\n",
      "Done with the batch: 759\n",
      "Done with the batch: 760\n",
      "Done with the batch: 761\n",
      "Done with the batch: 762\n",
      "Done with the batch: 763\n",
      "Done with the batch: 764\n",
      "Done with the batch: 765\n",
      "Done with the batch: 766\n",
      "Done with the batch: 767\n",
      "Done with the batch: 768\n",
      "Done with the batch: 769\n",
      "Done with the batch: 770\n",
      "Done with the batch: 771\n",
      "Done with the batch: 772\n",
      "Done with the batch: 773\n",
      "Done with the batch: 774\n",
      "Done with the batch: 775\n",
      "Done with the batch: 776\n",
      "Done with the batch: 777\n",
      "Done with the batch: 778\n",
      "Done with the batch: 779\n",
      "Done with the batch: 780\n",
      "Done with the batch: 781\n",
      "Done with the batch: 782\n",
      "Done with the batch: 783\n",
      "Done with the batch: 784\n",
      "Done with the batch: 785\n",
      "Done with the batch: 786\n",
      "Done with the batch: 787\n",
      "Done with the batch: 788\n",
      "Done with the batch: 789\n",
      "Done with the batch: 790\n",
      "Done with the batch: 791\n",
      "Done with the batch: 792\n",
      "Done with the batch: 793\n",
      "Done with the batch: 794\n",
      "Done with the batch: 795\n",
      "Done with the batch: 796\n",
      "Done with the batch: 797\n",
      "Done with the batch: 798\n",
      "Done with the batch: 799\n",
      "Done with the batch: 800\n",
      "Done with the batch: 801\n",
      "Done with the batch: 802\n",
      "Done with the batch: 803\n",
      "Done with the batch: 804\n",
      "Done with the batch: 805\n",
      "Done with the batch: 806\n",
      "Done with the batch: 807\n",
      "Done with the batch: 808\n",
      "Done with the batch: 809\n",
      "Done with the batch: 810\n",
      "Done with the batch: 811\n",
      "Done with the batch: 812\n",
      "Done with the batch: 813\n",
      "Done with the batch: 814\n",
      "Done with the batch: 815\n",
      "Done with the batch: 816\n",
      "Done with the batch: 817\n",
      "Done with the batch: 818\n",
      "Done with the batch: 819\n",
      "Done with the batch: 820\n",
      "Done with the batch: 821\n",
      "Done with the batch: 822\n",
      "Done with the batch: 823\n",
      "Done with the batch: 824\n",
      "Done with the batch: 825\n",
      "Done with the batch: 826\n",
      "Done with the batch: 827\n",
      "Done with the batch: 828\n",
      "Done with the batch: 829\n",
      "Done with the batch: 830\n",
      "Done with the batch: 831\n",
      "Done with the batch: 832\n",
      "Done with the batch: 833\n",
      "Done with the batch: 834\n",
      "Done with the batch: 835\n",
      "Done with the batch: 836\n",
      "Done with the batch: 837\n",
      "Done with the batch: 838\n",
      "Done with the batch: 839\n",
      "Done with the batch: 840\n",
      "Done with the batch: 841\n",
      "Done with the batch: 842\n",
      "Done with the batch: 843\n",
      "Done with the batch: 844\n",
      "Done with the batch: 845\n",
      "Done with the batch: 846\n",
      "Done with the batch: 847\n",
      "Done with the batch: 848\n",
      "Done with the batch: 849\n",
      "Done with the batch: 850\n",
      "Done with the batch: 851\n",
      "Done with the batch: 852\n",
      "Done with the batch: 853\n",
      "Done with the batch: 854\n",
      "Done with the batch: 855\n",
      "Done with the batch: 856\n",
      "Done with the batch: 857\n",
      "Done with the batch: 858\n",
      "Done with the batch: 859\n",
      "Done with the batch: 860\n",
      "Done with the batch: 861\n",
      "Done with the batch: 862\n",
      "Done with the batch: 863\n",
      "Done with the batch: 864\n",
      "Done with the batch: 865\n",
      "Done with the batch: 866\n",
      "Done with the batch: 867\n",
      "Done with the batch: 868\n",
      "Done with the batch: 869\n",
      "Done with the batch: 870\n",
      "Done with the batch: 871\n",
      "Done with the batch: 872\n",
      "Done with the batch: 873\n",
      "Done with the batch: 874\n",
      "Done with the batch: 875\n",
      "Done with the batch: 876\n",
      "Done with the batch: 877\n",
      "Done with the batch: 878\n",
      "Done with the batch: 879\n",
      "Done with the batch: 880\n",
      "Done with the batch: 881\n",
      "Done with the batch: 882\n",
      "Done with the batch: 883\n",
      "Done with the batch: 884\n",
      "Done with the batch: 885\n",
      "Done with the batch: 886\n",
      "Done with the batch: 887\n",
      "Done with the batch: 888\n",
      "Done with the batch: 889\n",
      "Done with the batch: 890\n",
      "Done with the batch: 891\n",
      "Done with the batch: 892\n",
      "Done with the batch: 893\n",
      "Done with the batch: 894\n",
      "Done with the batch: 895\n",
      "Done with the batch: 896\n",
      "Done with the batch: 897\n",
      "Done with the batch: 898\n",
      "Done with the batch: 899\n",
      "Done with the batch: 900\n",
      "Done with the batch: 901\n",
      "Done with the batch: 902\n",
      "Done with the batch: 903\n",
      "Done with the batch: 904\n",
      "Done with the batch: 905\n",
      "Done with the batch: 906\n",
      "Done with the batch: 907\n",
      "Done with the batch: 908\n",
      "Done with the batch: 909\n",
      "Done with the batch: 910\n",
      "Done with the batch: 911\n",
      "Done with the batch: 912\n",
      "Done with the batch: 913\n",
      "Done with the batch: 914\n",
      "Done with the batch: 915\n",
      "Done with the batch: 916\n",
      "Done with the batch: 917\n",
      "Done with the batch: 918\n",
      "Done with the batch: 919\n",
      "Done with the batch: 920\n",
      "Done with the batch: 921\n",
      "Done with the batch: 922\n",
      "Done with the batch: 923\n",
      "Done with the batch: 924\n",
      "Done with the batch: 925\n",
      "Done with the batch: 926\n",
      "Done with the batch: 927\n",
      "Done with the batch: 928\n",
      "Done with the batch: 929\n",
      "Done with the batch: 930\n",
      "Done with the batch: 931\n",
      "Done with the batch: 932\n",
      "Done with the batch: 933\n",
      "Done with the batch: 934\n",
      "Done with the batch: 935\n",
      "Done with the batch: 936\n",
      "Done with the batch: 937\n",
      "Done with the batch: 938\n",
      "Done with the batch: 939\n",
      "Done with the batch: 940\n",
      "Done with the batch: 941\n",
      "Done with the batch: 942\n",
      "Done with the batch: 943\n",
      "Done with the batch: 944\n",
      "Done with the batch: 945\n",
      "Done with the batch: 946\n",
      "Done with the batch: 947\n",
      "Done with the batch: 948\n",
      "Done with the batch: 949\n",
      "Done with the batch: 950\n",
      "Done with the batch: 951\n",
      "Done with the batch: 952\n",
      "Done with the batch: 953\n",
      "Done with the batch: 954\n",
      "Done with the batch: 955\n",
      "Done with the batch: 956\n",
      "Done with the batch: 957\n",
      "Done with the batch: 958\n",
      "Done with the batch: 959\n",
      "Done with the batch: 960\n",
      "Done with the batch: 961\n",
      "Done with the batch: 962\n",
      "Done with the batch: 963\n",
      "Done with the batch: 964\n",
      "Done with the batch: 965\n",
      "Done with the batch: 966\n",
      "Done with the batch: 967\n",
      "Done with the batch: 968\n",
      "Done with the batch: 969\n",
      "Done with the batch: 970\n",
      "Done with the batch: 971\n",
      "Done with the batch: 972\n",
      "Done with the batch: 973\n",
      "Done with the batch: 974\n",
      "Done with the batch: 975\n",
      "Done with the batch: 976\n",
      "Done with the batch: 977\n",
      "Done with the batch: 978\n",
      "Done with the batch: 979\n",
      "Done with the batch: 980\n",
      "Done with the batch: 981\n",
      "Done with the batch: 982\n",
      "Done with the batch: 983\n",
      "Done with the batch: 984\n",
      "Done with the batch: 985\n",
      "Done with the batch: 986\n",
      "Done with the batch: 987\n",
      "Done with the batch: 988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 989\n",
      "Done with the batch: 990\n",
      "Done with the batch: 991\n",
      "Done with the batch: 992\n",
      "Done with the batch: 993\n",
      "Done with the batch: 994\n",
      "Done with the batch: 995\n",
      "Done with the batch: 996\n",
      "Done with the batch: 997\n",
      "Done with the batch: 998\n",
      "Done with the batch: 999\n",
      "Done with the batch: 1000\n",
      "Done with the batch: 1001\n",
      "Done with the batch: 1002\n",
      "Done with the batch: 1003\n",
      "Done with the batch: 1004\n",
      "Done with the batch: 1005\n",
      "Done with the batch: 1006\n",
      "Done with the batch: 1007\n",
      "Done with the batch: 1008\n",
      "Done with the batch: 1009\n",
      "Done with the batch: 1010\n",
      "Done with the batch: 1011\n",
      "Done with the batch: 1012\n",
      "Done with the batch: 1013\n",
      "Done with the batch: 1014\n",
      "Done with the batch: 1015\n",
      "Done with the batch: 1016\n",
      "Done with the batch: 1017\n",
      "Done with the batch: 1018\n",
      "Done with the batch: 1019\n",
      "Done with the batch: 1020\n",
      "Done with the batch: 1021\n",
      "Done with the batch: 1022\n",
      "Done with the batch: 1023\n",
      "Done with the batch: 1024\n",
      "Done with the batch: 1025\n",
      "Done with the batch: 1026\n",
      "Done with the batch: 1027\n",
      "Done with the batch: 1028\n",
      "Done with the batch: 1029\n",
      "Done with the batch: 1030\n",
      "Done with the batch: 1031\n",
      "Done with the batch: 1032\n",
      "Done with the batch: 1033\n",
      "Done with the batch: 1034\n",
      "Done with the batch: 1035\n",
      "Done with the batch: 1036\n",
      "Done with the batch: 1037\n",
      "Done with the batch: 1038\n",
      "Done with the batch: 1039\n",
      "Done with the batch: 1040\n",
      "Done with the batch: 1041\n",
      "Done with the batch: 1042\n",
      "Done with the batch: 1043\n",
      "Done with the batch: 1044\n",
      "Done with the batch: 1045\n",
      "Done with the batch: 1046\n",
      "Done with the batch: 1047\n",
      "Done with the batch: 1048\n",
      "Done with the batch: 1049\n",
      "Done with the batch: 1050\n",
      "Done with the batch: 1051\n",
      "Done with the batch: 1052\n",
      "Done with the batch: 1053\n",
      "Done with the batch: 1054\n",
      "Done with the batch: 1055\n",
      "Done with the batch: 1056\n",
      "Done with the batch: 1057\n",
      "Done with the batch: 1058\n",
      "Done with the batch: 1059\n",
      "Done with the batch: 1060\n",
      "Done with the batch: 1061\n",
      "Done with the batch: 1062\n",
      "Done with the batch: 1063\n",
      "Done with the batch: 1064\n",
      "Done with the batch: 1065\n",
      "Done with the batch: 1066\n",
      "Done with the batch: 1067\n",
      "Done with the batch: 1068\n",
      "Done with the batch: 1069\n",
      "Done with the batch: 1070\n",
      "Done with the batch: 1071\n",
      "Done with the batch: 1072\n",
      "Done with the batch: 1073\n",
      "Done with the batch: 1074\n",
      "Done with the batch: 1075\n",
      "Done with the batch: 1076\n",
      "Done with the batch: 1077\n",
      "Done with the batch: 1078\n",
      "Done with the batch: 1079\n",
      "Done with the batch: 1080\n",
      "Done with the batch: 1081\n",
      "Done with the batch: 1082\n",
      "Done with the batch: 1083\n",
      "Done with the batch: 1084\n",
      "Done with the batch: 1085\n",
      "Done with the batch: 1086\n",
      "Done with the batch: 1087\n",
      "Done with the batch: 1088\n",
      "Done with the batch: 1089\n",
      "Done with the batch: 1090\n",
      "Done with the batch: 1091\n",
      "Done with the batch: 1092\n",
      "Done with the batch: 1093\n",
      "Done with the batch: 1094\n",
      "Done with the batch: 1095\n",
      "Done with the batch: 1096\n",
      "Done with the batch: 1097\n",
      "Done with the batch: 1098\n",
      "Done with the batch: 1099\n",
      "Done with the batch: 1100\n",
      "Done with the batch: 1101\n",
      "Done with the batch: 1102\n",
      "Done with the batch: 1103\n",
      "Done with the batch: 1104\n",
      "Done with the batch: 1105\n",
      "Done with the batch: 1106\n",
      "Done with the batch: 1107\n",
      "Done with the batch: 1108\n",
      "Done with the batch: 1109\n",
      "Done with the batch: 1110\n",
      "Done with the batch: 1111\n",
      "Done with the batch: 1112\n",
      "Done with the batch: 1113\n",
      "Done with the batch: 1114\n",
      "Done with the batch: 1115\n",
      "Done with the batch: 1116\n",
      "Done with the batch: 1117\n",
      "Done with the batch: 1118\n",
      "Done with the batch: 1119\n",
      "Done with the batch: 1120\n",
      "Done with the batch: 1121\n",
      "Done with the batch: 1122\n",
      "Done with the batch: 1123\n",
      "Done with the batch: 1124\n",
      "Done with the batch: 1125\n",
      "Done with the batch: 1126\n",
      "Done with the batch: 1127\n",
      "Done with the batch: 1128\n",
      "Done with the batch: 1129\n",
      "Done with the batch: 1130\n",
      "Done with the batch: 1131\n",
      "Done with the batch: 1132\n",
      "Done with the batch: 1133\n",
      "Done with the batch: 1134\n",
      "Done with the batch: 1135\n",
      "Done with the batch: 1136\n",
      "Done with the batch: 1137\n",
      "Done with the batch: 1138\n",
      "Done with the batch: 1139\n",
      "Done with the batch: 1140\n",
      "Done with the batch: 1141\n",
      "Done with the batch: 1142\n",
      "Done with the batch: 1143\n",
      "Done with the batch: 1144\n",
      "Done with the batch: 1145\n",
      "Done with the batch: 1146\n",
      "Done with the batch: 1147\n",
      "Done with the batch: 1148\n",
      "Done with the batch: 1149\n",
      "Done with the batch: 1150\n",
      "Done with the batch: 1151\n",
      "Done with the batch: 1152\n",
      "Done with the batch: 1153\n",
      "Done with the batch: 1154\n",
      "Done with the batch: 1155\n",
      "Done with the batch: 1156\n",
      "Done with the batch: 1157\n",
      "Done with the batch: 1158\n",
      "Done with the batch: 1159\n",
      "Done with the batch: 1160\n",
      "Done with the batch: 1161\n",
      "Done with the batch: 1162\n",
      "Done with the batch: 1163\n",
      "Done with the batch: 1164\n",
      "Done with the batch: 1165\n",
      "Done with the batch: 1166\n",
      "Done with the batch: 1167\n",
      "Done with the batch: 1168\n",
      "Done with the batch: 1169\n",
      "Done with the batch: 1170\n",
      "Done with the batch: 1171\n",
      "Done with the batch: 1172\n",
      "Done with the batch: 1173\n",
      "Done with the batch: 1174\n",
      "Done with the batch: 1175\n",
      "Done with the batch: 1176\n",
      "Done with the batch: 1177\n",
      "Done with the batch: 1178\n",
      "Done with the batch: 1179\n",
      "Done with the batch: 1180\n",
      "Done with the batch: 1181\n",
      "Done with the batch: 1182\n",
      "Done with the batch: 1183\n",
      "Done with the batch: 1184\n",
      "Done with the batch: 1185\n",
      "Done with the batch: 1186\n",
      "Done with the batch: 1187\n",
      "Done with the batch: 1188\n",
      "Done with the batch: 1189\n",
      "Done with the batch: 1190\n",
      "Done with the batch: 1191\n",
      "Done with the batch: 1192\n",
      "Done with the batch: 1193\n",
      "Done with the batch: 1194\n",
      "Done with the batch: 1195\n",
      "Done with the batch: 1196\n",
      "Done with the batch: 1197\n",
      "Done with the batch: 1198\n",
      "Done with the batch: 1199\n",
      "Done with the batch: 1200\n",
      "Done with the batch: 1201\n",
      "Done with the batch: 1202\n",
      "Done with the batch: 1203\n",
      "Done with the batch: 1204\n",
      "Done with the batch: 1205\n",
      "Done with the batch: 1206\n",
      "Done with the batch: 1207\n",
      "Done with the batch: 1208\n",
      "Done with the batch: 1209\n",
      "Done with the batch: 1210\n",
      "Done with the batch: 1211\n",
      "Done with the batch: 1212\n",
      "Done with the batch: 1213\n",
      "Done with the batch: 1214\n",
      "Done with the batch: 1215\n",
      "Done with the batch: 1216\n",
      "Done with the batch: 1217\n",
      "Done with the batch: 1218\n",
      "Done with the batch: 1219\n",
      "Done with the batch: 1220\n",
      "Done with the batch: 1221\n",
      "Done with the batch: 1222\n",
      "Done with the batch: 1223\n",
      "Done with the batch: 1224\n",
      "Done with the batch: 1225\n",
      "Done with the batch: 1226\n",
      "Done with the batch: 1227\n",
      "Done with the batch: 1228\n",
      "Done with the batch: 1229\n",
      "Done with the batch: 1230\n",
      "Done with the batch: 1231\n",
      "Done with the batch: 1232\n",
      "Done with the batch: 1233\n",
      "Done with the batch: 1234\n",
      "Done with the batch: 1235\n",
      "Done with the batch: 1236\n",
      "Done with the batch: 1237\n",
      "Done with the batch: 1238\n",
      "Done with the batch: 1239\n",
      "Done with the batch: 1240\n",
      "Done with the batch: 1241\n",
      "Done with the batch: 1242\n",
      "Done with the batch: 1243\n",
      "Done with the batch: 1244\n",
      "Done with the batch: 1245\n",
      "Done with the batch: 1246\n",
      "Done with the batch: 1247\n",
      "Done with the batch: 1248\n",
      "Done with the batch: 1249\n",
      "Done with the batch: 1250\n",
      "Done with the batch: 1251\n",
      "Done with the batch: 1252\n",
      "Done with the batch: 1253\n",
      "Done with the batch: 1254\n",
      "Done with the batch: 1255\n",
      "Done with the batch: 1256\n",
      "Done with the batch: 1257\n",
      "Done with the batch: 1258\n",
      "Done with the batch: 1259\n",
      "Done with the batch: 1260\n",
      "Done with the batch: 1261\n",
      "Done with the batch: 1262\n",
      "Done with the batch: 1263\n",
      "Done with the batch: 1264\n",
      "Done with the batch: 1265\n",
      "Done with the batch: 1266\n",
      "Done with the batch: 1267\n",
      "Done with the batch: 1268\n",
      "Done with the batch: 1269\n",
      "Done with the batch: 1270\n",
      "Done with the batch: 1271\n",
      "Done with the batch: 1272\n",
      "Done with the batch: 1273\n",
      "Done with the batch: 1274\n",
      "Done with the batch: 1275\n",
      "Done with the batch: 1276\n",
      "Done with the batch: 1277\n",
      "Done with the batch: 1278\n",
      "Done with the batch: 1279\n",
      "Done with the batch: 1280\n",
      "Done with the batch: 1281\n",
      "Done with the batch: 1282\n",
      "Done with the batch: 1283\n",
      "Done with the batch: 1284\n",
      "Done with the batch: 1285\n",
      "Done with the batch: 1286\n",
      "Done with the batch: 1287\n",
      "Done with the batch: 1288\n",
      "Done with the batch: 1289\n",
      "Done with the batch: 1290\n",
      "Done with the batch: 1291\n",
      "Done with the batch: 1292\n",
      "Done with the batch: 1293\n",
      "Done with the batch: 1294\n",
      "Done with the batch: 1295\n",
      "Done with the batch: 1296\n",
      "Done with the batch: 1297\n",
      "Done with the batch: 1298\n",
      "Done with the batch: 1299\n",
      "Done with the batch: 1300\n",
      "Done with the batch: 1301\n",
      "Done with the batch: 1302\n",
      "Done with the batch: 1303\n",
      "Done with the batch: 1304\n",
      "Done with the batch: 1305\n",
      "Done with the batch: 1306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 1307\n",
      "Done with the batch: 1308\n",
      "Done with the batch: 1309\n",
      "Done with the batch: 1310\n",
      "Done with the batch: 1311\n",
      "Done with the batch: 1312\n",
      "Done with the batch: 1313\n",
      "Done with the batch: 1314\n",
      "Done with the batch: 1315\n",
      "Done with the batch: 1316\n",
      "Done with the batch: 1317\n",
      "Done with the batch: 1318\n",
      "Done with the batch: 1319\n",
      "Done with the batch: 1320\n",
      "Done with the batch: 1321\n",
      "Done with the batch: 1322\n",
      "Done with the batch: 1323\n",
      "Done with the batch: 1324\n",
      "Done with the batch: 1325\n",
      "Done with the batch: 1326\n",
      "Done with the batch: 1327\n",
      "Done with the batch: 1328\n",
      "Done with the batch: 1329\n",
      "Done with the batch: 1330\n",
      "Done with the batch: 1331\n",
      "Done with the batch: 1332\n",
      "Done with the batch: 1333\n",
      "Done with the batch: 1334\n",
      "Done with the batch: 1335\n",
      "Done with the batch: 1336\n",
      "Done with the batch: 1337\n",
      "Done with the batch: 1338\n",
      "Done with the batch: 1339\n",
      "Done with the batch: 1340\n",
      "Done with the batch: 1341\n",
      "Done with the batch: 1342\n",
      "Done with the batch: 1343\n",
      "Done with the batch: 1344\n",
      "Done with the batch: 1345\n",
      "Done with the batch: 1346\n",
      "Done with the batch: 1347\n",
      "Done with the batch: 1348\n",
      "Done with the batch: 1349\n",
      "Done with the batch: 1350\n",
      "Done with the batch: 1351\n",
      "Done with the batch: 1352\n",
      "Done with the batch: 1353\n",
      "Done with the batch: 1354\n",
      "Done with the batch: 1355\n",
      "Done with the batch: 1356\n",
      "Done with the batch: 1357\n",
      "Done with the batch: 1358\n",
      "Done with the batch: 1359\n",
      "Done with the batch: 1360\n",
      "Done with the batch: 1361\n",
      "Done with the batch: 1362\n",
      "Done with the batch: 1363\n",
      "Done with the batch: 1364\n",
      "Done with the batch: 1365\n",
      "Done with the batch: 1366\n",
      "Done with the batch: 1367\n",
      "Done with the batch: 1368\n",
      "Done with the batch: 1369\n",
      "Done with the batch: 1370\n",
      "Done with the batch: 1371\n",
      "Done with the batch: 1372\n",
      "Done with the batch: 1373\n",
      "Done with the batch: 1374\n",
      "Done with the batch: 1375\n",
      "Done with the batch: 1376\n",
      "Done with the batch: 1377\n",
      "Done with the batch: 1378\n",
      "Done with the batch: 1379\n",
      "Done with the batch: 1380\n",
      "Done with the batch: 1381\n",
      "Done with the batch: 1382\n",
      "Done with the batch: 1383\n",
      "Done with the batch: 1384\n",
      "Done with the batch: 1385\n",
      "Done with the batch: 1386\n",
      "Done with the batch: 1387\n",
      "Done with the batch: 1388\n",
      "Done with the batch: 1389\n",
      "Done with the batch: 1390\n",
      "Done with the batch: 1391\n",
      "Done with the batch: 1392\n",
      "Done with the batch: 1393\n",
      "Done with the batch: 1394\n",
      "Done with the batch: 1395\n",
      "Done with the batch: 1396\n",
      "Done with the batch: 1397\n",
      "Done with the batch: 1398\n",
      "Done with the batch: 1399\n",
      "Done with the batch: 1400\n",
      "Done with the batch: 1401\n",
      "Done with the batch: 1402\n",
      "Done with the batch: 1403\n",
      "Done with the batch: 1404\n",
      "Done with the batch: 1405\n",
      "Done with the batch: 1406\n",
      "Done with the batch: 1407\n",
      "Done with the batch: 1408\n",
      "Done with the batch: 1409\n",
      "Done with the batch: 1410\n",
      "Done with the batch: 1411\n",
      "Done with the batch: 1412\n",
      "Done with the batch: 1413\n",
      "Done with the batch: 1414\n",
      "Done with the batch: 1415\n",
      "Done with the batch: 1416\n",
      "Done with the batch: 1417\n",
      "Done with the batch: 1418\n",
      "Done with the batch: 1419\n",
      "Done with the batch: 1420\n",
      "Done with the batch: 1421\n",
      "Done with the batch: 1422\n",
      "Done with the batch: 1423\n",
      "Done with the batch: 1424\n",
      "Done with the batch: 1425\n",
      "Done with the batch: 1426\n",
      "Done with the batch: 1427\n",
      "Done with the batch: 1428\n",
      "Done with the batch: 1429\n",
      "Done with the batch: 1430\n",
      "Done with the batch: 1431\n",
      "Done with the batch: 1432\n",
      "Done with the batch: 1433\n",
      "Done with the batch: 1434\n",
      "Done with the batch: 1435\n",
      "Done with the batch: 1436\n",
      "Done with the batch: 1437\n",
      "Done with the batch: 1438\n",
      "Done with the batch: 1439\n",
      "Done with the batch: 1440\n",
      "Done with the batch: 1441\n",
      "Done with the batch: 1442\n",
      "Done with the batch: 1443\n",
      "Done with the batch: 1444\n",
      "Done with the batch: 1445\n",
      "Done with the batch: 1446\n",
      "Done with the batch: 1447\n",
      "Done with the batch: 1448\n",
      "Done with the batch: 1449\n",
      "Done with the batch: 1450\n",
      "Done with the batch: 1451\n",
      "Done with the batch: 1452\n",
      "Done with the batch: 1453\n",
      "Done with the batch: 1454\n",
      "Done with the batch: 1455\n",
      "Done with the batch: 1456\n",
      "Done with the batch: 1457\n",
      "Done with the batch: 1458\n",
      "Done with the batch: 1459\n",
      "Done with the batch: 1460\n",
      "Done with the batch: 1461\n",
      "Done with the batch: 1462\n",
      "Done with the batch: 1463\n",
      "Done with the batch: 1464\n",
      "Done with the batch: 1465\n",
      "Done with the batch: 1466\n",
      "Done with the batch: 1467\n",
      "Done with the batch: 1468\n",
      "Done with the batch: 1469\n",
      "Done with the batch: 1470\n",
      "Done with the batch: 1471\n",
      "Done with the batch: 1472\n",
      "Done with the batch: 1473\n",
      "Done with the batch: 1474\n",
      "Done with the batch: 1475\n",
      "Done with the batch: 1476\n",
      "Done with the batch: 1477\n",
      "Done with the batch: 1478\n",
      "Done with the batch: 1479\n",
      "Done with the batch: 1480\n",
      "Done with the batch: 1481\n",
      "Done with the batch: 1482\n",
      "Done with the batch: 1483\n",
      "Done with the batch: 1484\n",
      "Done with the batch: 1485\n",
      "Done with the batch: 1486\n",
      "Done with the batch: 1487\n",
      "Done with the batch: 1488\n",
      "Done with the batch: 1489\n",
      "Done with the batch: 1490\n",
      "Done with the batch: 1491\n",
      "Done with the batch: 1492\n",
      "Done with the batch: 1493\n",
      "Done with the batch: 1494\n",
      "Done with the batch: 1495\n",
      "Done with the batch: 1496\n",
      "Done with the batch: 1497\n",
      "Done with the batch: 1498\n",
      "Done with the batch: 1499\n",
      "Done with the batch: 1500\n",
      "Done with the batch: 1501\n",
      "Done with the batch: 1502\n",
      "Done with the batch: 1503\n",
      "Done with the batch: 1504\n",
      "Done with the batch: 1505\n",
      "Done with the batch: 1506\n",
      "Done with the batch: 1507\n",
      "Done with the batch: 1508\n",
      "Done with the batch: 1509\n",
      "Done with the batch: 1510\n",
      "Done with the batch: 1511\n",
      "Done with the batch: 1512\n",
      "Done with the batch: 1513\n",
      "Done with the batch: 1514\n",
      "Done with the batch: 1515\n",
      "Done with the batch: 1516\n",
      "Done with the batch: 1517\n",
      "Done with the batch: 1518\n",
      "Done with the batch: 1519\n",
      "Done with the batch: 1520\n",
      "Done with the batch: 1521\n",
      "Done with the batch: 1522\n",
      "Done with the batch: 1523\n",
      "Done with the batch: 1524\n",
      "Done with the batch: 1525\n",
      "Done with the batch: 1526\n",
      "Done with the batch: 1527\n",
      "Done with the batch: 1528\n",
      "Done with the batch: 1529\n",
      "Done with the batch: 1530\n",
      "Done with the batch: 1531\n",
      "Done with the batch: 1532\n",
      "Done with the batch: 1533\n",
      "Done with the batch: 1534\n",
      "Done with the batch: 1535\n",
      "Done with the batch: 1536\n",
      "Done with the batch: 1537\n",
      "Done with the batch: 1538\n",
      "Done with the batch: 1539\n",
      "Done with the batch: 1540\n",
      "Done with the batch: 1541\n",
      "Done with the batch: 1542\n",
      "Done with the batch: 1543\n",
      "Done with the batch: 1544\n",
      "Done with the batch: 1545\n",
      "Done with the batch: 1546\n",
      "Done with the batch: 1547\n",
      "Done with the batch: 1548\n",
      "Done with the batch: 1549\n",
      "Done with the batch: 1550\n",
      "Done with the batch: 1551\n",
      "Done with the batch: 1552\n",
      "Done with the batch: 1553\n",
      "Done with the batch: 1554\n",
      "Done with the batch: 1555\n",
      "Done with the batch: 1556\n",
      "Done with the batch: 1557\n",
      "Done with the batch: 1558\n",
      "Done with the batch: 1559\n",
      "Done with the batch: 1560\n",
      "Done with the batch: 1561\n",
      "Done with the batch: 1562\n",
      "Done with the batch: 1563\n",
      "Done with the batch: 1564\n",
      "Done with the batch: 1565\n",
      "Done with the batch: 1566\n",
      "Done with the batch: 1567\n",
      "Done with the batch: 1568\n",
      "Done with the batch: 1569\n",
      "Done with the batch: 1570\n",
      "Done with the batch: 1571\n",
      "Done with the batch: 1572\n",
      "Done with the batch: 1573\n",
      "Done with the batch: 1574\n",
      "Done with the batch: 1575\n",
      "Done with the batch: 1576\n",
      "Done with the batch: 1577\n",
      "Done with the batch: 1578\n",
      "Done with the batch: 1579\n",
      "Done with the batch: 1580\n",
      "Done with the batch: 1581\n",
      "Done with the batch: 1582\n",
      "Done with the batch: 1583\n",
      "Done with the batch: 1584\n",
      "Done with the batch: 1585\n",
      "Done with the batch: 1586\n",
      "Done with the batch: 1587\n",
      "Done with the batch: 1588\n",
      "Done with the batch: 1589\n",
      "Done with the batch: 1590\n",
      "Done with the batch: 1591\n",
      "Done with the batch: 1592\n",
      "Done with the batch: 1593\n",
      "Done with the batch: 1594\n",
      "Done with the batch: 1595\n",
      "Done with the batch: 1596\n",
      "Done with the batch: 1597\n",
      "Done with the batch: 1598\n",
      "Done with the batch: 1599\n",
      "Done with the batch: 1600\n",
      "Done with the batch: 1601\n",
      "Done with the batch: 1602\n",
      "Done with the batch: 1603\n",
      "Done with the batch: 1604\n",
      "Done with the batch: 1605\n",
      "Done with the batch: 1606\n",
      "Done with the batch: 1607\n",
      "Done with the batch: 1608\n",
      "Done with the batch: 1609\n",
      "Done with the batch: 1610\n",
      "Done with the batch: 1611\n",
      "Done with the batch: 1612\n",
      "Done with the batch: 1613\n",
      "Done with the batch: 1614\n",
      "Done with the batch: 1615\n",
      "Done with the batch: 1616\n",
      "Done with the batch: 1617\n",
      "(6470, 512) (6470,)\n"
     ]
    }
   ],
   "source": [
    "X_Train=np.empty((0,512))\n",
    "Y_Train=np.empty((0,batch_size))\n",
    "print(X_Train.shape)\n",
    "for i,data in enumerate(trainloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    FCLayer=net.get_first_FC_Layer(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Train=np.append(X_Train,FCLayer,axis=0)\n",
    "    Y_Train=np.append(Y_Train,labels.numpy())\n",
    "print(X_Train.shape,Y_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0853a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 512)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "(1618, 512) (1618,)\n"
     ]
    }
   ],
   "source": [
    "X_Test=np.empty((0,512))\n",
    "Y_Test=np.empty((0,batch_size))\n",
    "print(X_Test.shape)\n",
    "for i,data in enumerate(testloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    FCLayer=net.get_first_FC_Layer(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Test=np.append(X_Test,FCLayer,axis=0)\n",
    "    Y_Test=np.append(Y_Test,labels.numpy())\n",
    "print(X_Test.shape,Y_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55025900",
   "metadata": {},
   "source": [
    "## Getting the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f4b08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 18432)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "Done with the batch: 405\n",
      "Done with the batch: 406\n",
      "Done with the batch: 407\n",
      "Done with the batch: 408\n",
      "Done with the batch: 409\n",
      "Done with the batch: 410\n",
      "Done with the batch: 411\n",
      "Done with the batch: 412\n",
      "Done with the batch: 413\n",
      "Done with the batch: 414\n",
      "Done with the batch: 415\n",
      "Done with the batch: 416\n",
      "Done with the batch: 417\n",
      "Done with the batch: 418\n",
      "Done with the batch: 419\n",
      "Done with the batch: 420\n",
      "Done with the batch: 421\n",
      "Done with the batch: 422\n",
      "Done with the batch: 423\n",
      "Done with the batch: 424\n",
      "Done with the batch: 425\n",
      "Done with the batch: 426\n",
      "Done with the batch: 427\n",
      "Done with the batch: 428\n",
      "Done with the batch: 429\n",
      "Done with the batch: 430\n",
      "Done with the batch: 431\n",
      "Done with the batch: 432\n",
      "Done with the batch: 433\n",
      "Done with the batch: 434\n",
      "Done with the batch: 435\n",
      "Done with the batch: 436\n",
      "Done with the batch: 437\n",
      "Done with the batch: 438\n",
      "Done with the batch: 439\n",
      "Done with the batch: 440\n",
      "Done with the batch: 441\n",
      "Done with the batch: 442\n",
      "Done with the batch: 443\n",
      "Done with the batch: 444\n",
      "Done with the batch: 445\n",
      "Done with the batch: 446\n",
      "Done with the batch: 447\n",
      "Done with the batch: 448\n",
      "Done with the batch: 449\n",
      "Done with the batch: 450\n",
      "Done with the batch: 451\n",
      "Done with the batch: 452\n",
      "Done with the batch: 453\n",
      "Done with the batch: 454\n",
      "Done with the batch: 455\n",
      "Done with the batch: 456\n",
      "Done with the batch: 457\n",
      "Done with the batch: 458\n",
      "Done with the batch: 459\n",
      "Done with the batch: 460\n",
      "Done with the batch: 461\n",
      "Done with the batch: 462\n",
      "Done with the batch: 463\n",
      "Done with the batch: 464\n",
      "Done with the batch: 465\n",
      "Done with the batch: 466\n",
      "Done with the batch: 467\n",
      "Done with the batch: 468\n",
      "Done with the batch: 469\n",
      "Done with the batch: 470\n",
      "Done with the batch: 471\n",
      "Done with the batch: 472\n",
      "Done with the batch: 473\n",
      "Done with the batch: 474\n",
      "Done with the batch: 475\n",
      "Done with the batch: 476\n",
      "Done with the batch: 477\n",
      "Done with the batch: 478\n",
      "Done with the batch: 479\n",
      "Done with the batch: 480\n",
      "Done with the batch: 481\n",
      "Done with the batch: 482\n",
      "Done with the batch: 483\n",
      "Done with the batch: 484\n",
      "Done with the batch: 485\n",
      "Done with the batch: 486\n",
      "Done with the batch: 487\n",
      "Done with the batch: 488\n",
      "Done with the batch: 489\n",
      "Done with the batch: 490\n",
      "Done with the batch: 491\n",
      "Done with the batch: 492\n",
      "Done with the batch: 493\n",
      "Done with the batch: 494\n",
      "Done with the batch: 495\n",
      "Done with the batch: 496\n",
      "Done with the batch: 497\n",
      "Done with the batch: 498\n",
      "Done with the batch: 499\n",
      "Done with the batch: 500\n",
      "Done with the batch: 501\n",
      "Done with the batch: 502\n",
      "Done with the batch: 503\n",
      "Done with the batch: 504\n",
      "Done with the batch: 505\n",
      "Done with the batch: 506\n",
      "Done with the batch: 507\n",
      "Done with the batch: 508\n",
      "Done with the batch: 509\n",
      "Done with the batch: 510\n",
      "Done with the batch: 511\n",
      "Done with the batch: 512\n",
      "Done with the batch: 513\n",
      "Done with the batch: 514\n",
      "Done with the batch: 515\n",
      "Done with the batch: 516\n",
      "Done with the batch: 517\n",
      "Done with the batch: 518\n",
      "Done with the batch: 519\n",
      "Done with the batch: 520\n",
      "Done with the batch: 521\n",
      "Done with the batch: 522\n",
      "Done with the batch: 523\n",
      "Done with the batch: 524\n",
      "Done with the batch: 525\n",
      "Done with the batch: 526\n",
      "Done with the batch: 527\n",
      "Done with the batch: 528\n",
      "Done with the batch: 529\n",
      "Done with the batch: 530\n",
      "Done with the batch: 531\n",
      "Done with the batch: 532\n",
      "Done with the batch: 533\n",
      "Done with the batch: 534\n",
      "Done with the batch: 535\n",
      "Done with the batch: 536\n",
      "Done with the batch: 537\n",
      "Done with the batch: 538\n",
      "Done with the batch: 539\n",
      "Done with the batch: 540\n",
      "Done with the batch: 541\n",
      "Done with the batch: 542\n",
      "Done with the batch: 543\n",
      "Done with the batch: 544\n",
      "Done with the batch: 545\n",
      "Done with the batch: 546\n",
      "Done with the batch: 547\n",
      "Done with the batch: 548\n",
      "Done with the batch: 549\n",
      "Done with the batch: 550\n",
      "Done with the batch: 551\n",
      "Done with the batch: 552\n",
      "Done with the batch: 553\n",
      "Done with the batch: 554\n",
      "Done with the batch: 555\n",
      "Done with the batch: 556\n",
      "Done with the batch: 557\n",
      "Done with the batch: 558\n",
      "Done with the batch: 559\n",
      "Done with the batch: 560\n",
      "Done with the batch: 561\n",
      "Done with the batch: 562\n",
      "Done with the batch: 563\n",
      "Done with the batch: 564\n",
      "Done with the batch: 565\n",
      "Done with the batch: 566\n",
      "Done with the batch: 567\n",
      "Done with the batch: 568\n",
      "Done with the batch: 569\n",
      "Done with the batch: 570\n",
      "Done with the batch: 571\n",
      "Done with the batch: 572\n",
      "Done with the batch: 573\n",
      "Done with the batch: 574\n",
      "Done with the batch: 575\n",
      "Done with the batch: 576\n",
      "Done with the batch: 577\n",
      "Done with the batch: 578\n",
      "Done with the batch: 579\n",
      "Done with the batch: 580\n",
      "Done with the batch: 581\n",
      "Done with the batch: 582\n",
      "Done with the batch: 583\n",
      "Done with the batch: 584\n",
      "Done with the batch: 585\n",
      "Done with the batch: 586\n",
      "Done with the batch: 587\n",
      "Done with the batch: 588\n",
      "Done with the batch: 589\n",
      "Done with the batch: 590\n",
      "Done with the batch: 591\n",
      "Done with the batch: 592\n",
      "Done with the batch: 593\n",
      "Done with the batch: 594\n",
      "Done with the batch: 595\n",
      "Done with the batch: 596\n",
      "Done with the batch: 597\n",
      "Done with the batch: 598\n",
      "Done with the batch: 599\n",
      "Done with the batch: 600\n",
      "Done with the batch: 601\n",
      "Done with the batch: 602\n",
      "Done with the batch: 603\n",
      "Done with the batch: 604\n",
      "Done with the batch: 605\n",
      "Done with the batch: 606\n",
      "Done with the batch: 607\n",
      "Done with the batch: 608\n",
      "Done with the batch: 609\n",
      "Done with the batch: 610\n",
      "Done with the batch: 611\n",
      "Done with the batch: 612\n",
      "Done with the batch: 613\n",
      "Done with the batch: 614\n",
      "Done with the batch: 615\n",
      "Done with the batch: 616\n",
      "Done with the batch: 617\n",
      "Done with the batch: 618\n",
      "Done with the batch: 619\n",
      "Done with the batch: 620\n",
      "Done with the batch: 621\n",
      "Done with the batch: 622\n",
      "Done with the batch: 623\n",
      "Done with the batch: 624\n",
      "Done with the batch: 625\n",
      "Done with the batch: 626\n",
      "Done with the batch: 627\n",
      "Done with the batch: 628\n",
      "Done with the batch: 629\n",
      "Done with the batch: 630\n",
      "Done with the batch: 631\n",
      "Done with the batch: 632\n",
      "Done with the batch: 633\n",
      "Done with the batch: 634\n",
      "Done with the batch: 635\n",
      "Done with the batch: 636\n",
      "Done with the batch: 637\n",
      "Done with the batch: 638\n",
      "Done with the batch: 639\n",
      "Done with the batch: 640\n",
      "Done with the batch: 641\n",
      "Done with the batch: 642\n",
      "Done with the batch: 643\n",
      "Done with the batch: 644\n",
      "Done with the batch: 645\n",
      "Done with the batch: 646\n",
      "Done with the batch: 647\n",
      "Done with the batch: 648\n",
      "Done with the batch: 649\n",
      "Done with the batch: 650\n",
      "Done with the batch: 651\n",
      "Done with the batch: 652\n",
      "Done with the batch: 653\n",
      "Done with the batch: 654\n",
      "Done with the batch: 655\n",
      "Done with the batch: 656\n",
      "Done with the batch: 657\n",
      "Done with the batch: 658\n",
      "Done with the batch: 659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 660\n",
      "Done with the batch: 661\n",
      "Done with the batch: 662\n",
      "Done with the batch: 663\n",
      "Done with the batch: 664\n",
      "Done with the batch: 665\n",
      "Done with the batch: 666\n",
      "Done with the batch: 667\n",
      "Done with the batch: 668\n",
      "Done with the batch: 669\n",
      "Done with the batch: 670\n",
      "Done with the batch: 671\n",
      "Done with the batch: 672\n",
      "Done with the batch: 673\n",
      "Done with the batch: 674\n",
      "Done with the batch: 675\n",
      "Done with the batch: 676\n",
      "Done with the batch: 677\n",
      "Done with the batch: 678\n",
      "Done with the batch: 679\n",
      "Done with the batch: 680\n",
      "Done with the batch: 681\n",
      "Done with the batch: 682\n",
      "Done with the batch: 683\n",
      "Done with the batch: 684\n",
      "Done with the batch: 685\n",
      "Done with the batch: 686\n",
      "Done with the batch: 687\n",
      "Done with the batch: 688\n",
      "Done with the batch: 689\n",
      "Done with the batch: 690\n",
      "Done with the batch: 691\n",
      "Done with the batch: 692\n",
      "Done with the batch: 693\n",
      "Done with the batch: 694\n",
      "Done with the batch: 695\n",
      "Done with the batch: 696\n",
      "Done with the batch: 697\n",
      "Done with the batch: 698\n",
      "Done with the batch: 699\n",
      "Done with the batch: 700\n",
      "Done with the batch: 701\n",
      "Done with the batch: 702\n",
      "Done with the batch: 703\n",
      "Done with the batch: 704\n",
      "Done with the batch: 705\n",
      "Done with the batch: 706\n",
      "Done with the batch: 707\n",
      "Done with the batch: 708\n",
      "Done with the batch: 709\n",
      "Done with the batch: 710\n",
      "Done with the batch: 711\n",
      "Done with the batch: 712\n",
      "Done with the batch: 713\n",
      "Done with the batch: 714\n",
      "Done with the batch: 715\n",
      "Done with the batch: 716\n",
      "Done with the batch: 717\n",
      "Done with the batch: 718\n",
      "Done with the batch: 719\n",
      "Done with the batch: 720\n",
      "Done with the batch: 721\n",
      "Done with the batch: 722\n",
      "Done with the batch: 723\n",
      "Done with the batch: 724\n",
      "Done with the batch: 725\n",
      "Done with the batch: 726\n",
      "Done with the batch: 727\n",
      "Done with the batch: 728\n",
      "Done with the batch: 729\n",
      "Done with the batch: 730\n",
      "Done with the batch: 731\n",
      "Done with the batch: 732\n",
      "Done with the batch: 733\n",
      "Done with the batch: 734\n",
      "Done with the batch: 735\n",
      "Done with the batch: 736\n",
      "Done with the batch: 737\n",
      "Done with the batch: 738\n",
      "Done with the batch: 739\n",
      "Done with the batch: 740\n",
      "Done with the batch: 741\n",
      "Done with the batch: 742\n",
      "Done with the batch: 743\n",
      "Done with the batch: 744\n",
      "Done with the batch: 745\n",
      "Done with the batch: 746\n",
      "Done with the batch: 747\n",
      "Done with the batch: 748\n",
      "Done with the batch: 749\n",
      "Done with the batch: 750\n",
      "Done with the batch: 751\n",
      "Done with the batch: 752\n",
      "Done with the batch: 753\n",
      "Done with the batch: 754\n",
      "Done with the batch: 755\n",
      "Done with the batch: 756\n",
      "Done with the batch: 757\n",
      "Done with the batch: 758\n",
      "Done with the batch: 759\n",
      "Done with the batch: 760\n",
      "Done with the batch: 761\n",
      "Done with the batch: 762\n",
      "Done with the batch: 763\n",
      "Done with the batch: 764\n",
      "Done with the batch: 765\n",
      "Done with the batch: 766\n",
      "Done with the batch: 767\n",
      "Done with the batch: 768\n",
      "Done with the batch: 769\n",
      "Done with the batch: 770\n",
      "Done with the batch: 771\n",
      "Done with the batch: 772\n",
      "Done with the batch: 773\n",
      "Done with the batch: 774\n",
      "Done with the batch: 775\n",
      "Done with the batch: 776\n",
      "Done with the batch: 777\n",
      "Done with the batch: 778\n",
      "Done with the batch: 779\n",
      "Done with the batch: 780\n",
      "Done with the batch: 781\n",
      "Done with the batch: 782\n",
      "Done with the batch: 783\n",
      "Done with the batch: 784\n",
      "Done with the batch: 785\n",
      "Done with the batch: 786\n",
      "Done with the batch: 787\n",
      "Done with the batch: 788\n",
      "Done with the batch: 789\n",
      "Done with the batch: 790\n",
      "Done with the batch: 791\n",
      "Done with the batch: 792\n",
      "Done with the batch: 793\n",
      "Done with the batch: 794\n",
      "Done with the batch: 795\n",
      "Done with the batch: 796\n",
      "Done with the batch: 797\n",
      "Done with the batch: 798\n",
      "Done with the batch: 799\n",
      "Done with the batch: 800\n",
      "Done with the batch: 801\n",
      "Done with the batch: 802\n",
      "Done with the batch: 803\n",
      "Done with the batch: 804\n",
      "Done with the batch: 805\n",
      "Done with the batch: 806\n",
      "Done with the batch: 807\n",
      "Done with the batch: 808\n",
      "Done with the batch: 809\n",
      "Done with the batch: 810\n",
      "Done with the batch: 811\n",
      "Done with the batch: 812\n",
      "Done with the batch: 813\n",
      "Done with the batch: 814\n",
      "Done with the batch: 815\n",
      "Done with the batch: 816\n",
      "Done with the batch: 817\n",
      "Done with the batch: 818\n",
      "Done with the batch: 819\n",
      "Done with the batch: 820\n",
      "Done with the batch: 821\n",
      "Done with the batch: 822\n",
      "Done with the batch: 823\n",
      "Done with the batch: 824\n",
      "Done with the batch: 825\n",
      "Done with the batch: 826\n",
      "Done with the batch: 827\n",
      "Done with the batch: 828\n",
      "Done with the batch: 829\n",
      "Done with the batch: 830\n",
      "Done with the batch: 831\n",
      "Done with the batch: 832\n",
      "Done with the batch: 833\n",
      "Done with the batch: 834\n",
      "Done with the batch: 835\n",
      "Done with the batch: 836\n",
      "Done with the batch: 837\n",
      "Done with the batch: 838\n",
      "Done with the batch: 839\n",
      "Done with the batch: 840\n",
      "Done with the batch: 841\n",
      "Done with the batch: 842\n",
      "Done with the batch: 843\n",
      "Done with the batch: 844\n",
      "Done with the batch: 845\n",
      "Done with the batch: 846\n",
      "Done with the batch: 847\n",
      "Done with the batch: 848\n",
      "Done with the batch: 849\n",
      "Done with the batch: 850\n",
      "Done with the batch: 851\n",
      "Done with the batch: 852\n",
      "Done with the batch: 853\n",
      "Done with the batch: 854\n",
      "Done with the batch: 855\n",
      "Done with the batch: 856\n",
      "Done with the batch: 857\n",
      "Done with the batch: 858\n",
      "Done with the batch: 859\n",
      "Done with the batch: 860\n",
      "Done with the batch: 861\n",
      "Done with the batch: 862\n",
      "Done with the batch: 863\n",
      "Done with the batch: 864\n",
      "Done with the batch: 865\n",
      "Done with the batch: 866\n",
      "Done with the batch: 867\n",
      "Done with the batch: 868\n",
      "Done with the batch: 869\n",
      "Done with the batch: 870\n",
      "Done with the batch: 871\n",
      "Done with the batch: 872\n",
      "Done with the batch: 873\n",
      "Done with the batch: 874\n",
      "Done with the batch: 875\n",
      "Done with the batch: 876\n",
      "Done with the batch: 877\n",
      "Done with the batch: 878\n",
      "Done with the batch: 879\n",
      "Done with the batch: 880\n",
      "Done with the batch: 881\n",
      "Done with the batch: 882\n",
      "Done with the batch: 883\n",
      "Done with the batch: 884\n",
      "Done with the batch: 885\n",
      "Done with the batch: 886\n",
      "Done with the batch: 887\n",
      "Done with the batch: 888\n",
      "Done with the batch: 889\n",
      "Done with the batch: 890\n",
      "Done with the batch: 891\n",
      "Done with the batch: 892\n",
      "Done with the batch: 893\n",
      "Done with the batch: 894\n",
      "Done with the batch: 895\n",
      "Done with the batch: 896\n",
      "Done with the batch: 897\n",
      "Done with the batch: 898\n",
      "Done with the batch: 899\n",
      "Done with the batch: 900\n",
      "Done with the batch: 901\n",
      "Done with the batch: 902\n",
      "Done with the batch: 903\n",
      "Done with the batch: 904\n",
      "Done with the batch: 905\n",
      "Done with the batch: 906\n",
      "Done with the batch: 907\n",
      "Done with the batch: 908\n",
      "Done with the batch: 909\n",
      "Done with the batch: 910\n",
      "Done with the batch: 911\n",
      "Done with the batch: 912\n",
      "Done with the batch: 913\n",
      "Done with the batch: 914\n",
      "Done with the batch: 915\n",
      "Done with the batch: 916\n",
      "Done with the batch: 917\n",
      "Done with the batch: 918\n",
      "Done with the batch: 919\n",
      "Done with the batch: 920\n",
      "Done with the batch: 921\n",
      "Done with the batch: 922\n",
      "Done with the batch: 923\n",
      "Done with the batch: 924\n",
      "Done with the batch: 925\n",
      "Done with the batch: 926\n",
      "Done with the batch: 927\n",
      "Done with the batch: 928\n",
      "Done with the batch: 929\n",
      "Done with the batch: 930\n",
      "Done with the batch: 931\n",
      "Done with the batch: 932\n",
      "Done with the batch: 933\n",
      "Done with the batch: 934\n",
      "Done with the batch: 935\n",
      "Done with the batch: 936\n",
      "Done with the batch: 937\n",
      "Done with the batch: 938\n",
      "Done with the batch: 939\n",
      "Done with the batch: 940\n",
      "Done with the batch: 941\n",
      "Done with the batch: 942\n",
      "Done with the batch: 943\n",
      "Done with the batch: 944\n",
      "Done with the batch: 945\n",
      "Done with the batch: 946\n",
      "Done with the batch: 947\n",
      "Done with the batch: 948\n",
      "Done with the batch: 949\n",
      "Done with the batch: 950\n",
      "Done with the batch: 951\n",
      "Done with the batch: 952\n",
      "Done with the batch: 953\n",
      "Done with the batch: 954\n",
      "Done with the batch: 955\n",
      "Done with the batch: 956\n",
      "Done with the batch: 957\n",
      "Done with the batch: 958\n",
      "Done with the batch: 959\n",
      "Done with the batch: 960\n",
      "Done with the batch: 961\n",
      "Done with the batch: 962\n",
      "Done with the batch: 963\n",
      "Done with the batch: 964\n",
      "Done with the batch: 965\n",
      "Done with the batch: 966\n",
      "Done with the batch: 967\n",
      "Done with the batch: 968\n",
      "Done with the batch: 969\n",
      "Done with the batch: 970\n",
      "Done with the batch: 971\n",
      "Done with the batch: 972\n",
      "Done with the batch: 973\n",
      "Done with the batch: 974\n",
      "Done with the batch: 975\n",
      "Done with the batch: 976\n",
      "Done with the batch: 977\n",
      "Done with the batch: 978\n",
      "Done with the batch: 979\n",
      "Done with the batch: 980\n",
      "Done with the batch: 981\n",
      "Done with the batch: 982\n",
      "Done with the batch: 983\n",
      "Done with the batch: 984\n",
      "Done with the batch: 985\n",
      "Done with the batch: 986\n",
      "Done with the batch: 987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 988\n",
      "Done with the batch: 989\n",
      "Done with the batch: 990\n",
      "Done with the batch: 991\n",
      "Done with the batch: 992\n",
      "Done with the batch: 993\n",
      "Done with the batch: 994\n",
      "Done with the batch: 995\n",
      "Done with the batch: 996\n",
      "Done with the batch: 997\n",
      "Done with the batch: 998\n",
      "Done with the batch: 999\n",
      "Done with the batch: 1000\n",
      "Done with the batch: 1001\n",
      "Done with the batch: 1002\n",
      "Done with the batch: 1003\n",
      "Done with the batch: 1004\n",
      "Done with the batch: 1005\n",
      "Done with the batch: 1006\n",
      "Done with the batch: 1007\n",
      "Done with the batch: 1008\n",
      "Done with the batch: 1009\n",
      "Done with the batch: 1010\n",
      "Done with the batch: 1011\n",
      "Done with the batch: 1012\n",
      "Done with the batch: 1013\n",
      "Done with the batch: 1014\n",
      "Done with the batch: 1015\n",
      "Done with the batch: 1016\n",
      "Done with the batch: 1017\n",
      "Done with the batch: 1018\n",
      "Done with the batch: 1019\n",
      "Done with the batch: 1020\n",
      "Done with the batch: 1021\n",
      "Done with the batch: 1022\n",
      "Done with the batch: 1023\n",
      "Done with the batch: 1024\n",
      "Done with the batch: 1025\n",
      "Done with the batch: 1026\n",
      "Done with the batch: 1027\n",
      "Done with the batch: 1028\n",
      "Done with the batch: 1029\n",
      "Done with the batch: 1030\n",
      "Done with the batch: 1031\n",
      "Done with the batch: 1032\n",
      "Done with the batch: 1033\n",
      "Done with the batch: 1034\n",
      "Done with the batch: 1035\n",
      "Done with the batch: 1036\n",
      "Done with the batch: 1037\n",
      "Done with the batch: 1038\n",
      "Done with the batch: 1039\n",
      "Done with the batch: 1040\n",
      "Done with the batch: 1041\n",
      "Done with the batch: 1042\n",
      "Done with the batch: 1043\n",
      "Done with the batch: 1044\n",
      "Done with the batch: 1045\n",
      "Done with the batch: 1046\n",
      "Done with the batch: 1047\n",
      "Done with the batch: 1048\n",
      "Done with the batch: 1049\n",
      "Done with the batch: 1050\n",
      "Done with the batch: 1051\n",
      "Done with the batch: 1052\n",
      "Done with the batch: 1053\n",
      "Done with the batch: 1054\n",
      "Done with the batch: 1055\n",
      "Done with the batch: 1056\n",
      "Done with the batch: 1057\n",
      "Done with the batch: 1058\n",
      "Done with the batch: 1059\n",
      "Done with the batch: 1060\n",
      "Done with the batch: 1061\n",
      "Done with the batch: 1062\n",
      "Done with the batch: 1063\n",
      "Done with the batch: 1064\n",
      "Done with the batch: 1065\n",
      "Done with the batch: 1066\n",
      "Done with the batch: 1067\n",
      "Done with the batch: 1068\n",
      "Done with the batch: 1069\n",
      "Done with the batch: 1070\n",
      "Done with the batch: 1071\n",
      "Done with the batch: 1072\n",
      "Done with the batch: 1073\n",
      "Done with the batch: 1074\n",
      "Done with the batch: 1075\n",
      "Done with the batch: 1076\n",
      "Done with the batch: 1077\n",
      "Done with the batch: 1078\n",
      "Done with the batch: 1079\n",
      "Done with the batch: 1080\n",
      "Done with the batch: 1081\n",
      "Done with the batch: 1082\n",
      "Done with the batch: 1083\n",
      "Done with the batch: 1084\n",
      "Done with the batch: 1085\n",
      "Done with the batch: 1086\n",
      "Done with the batch: 1087\n",
      "Done with the batch: 1088\n",
      "Done with the batch: 1089\n",
      "Done with the batch: 1090\n",
      "Done with the batch: 1091\n",
      "Done with the batch: 1092\n",
      "Done with the batch: 1093\n",
      "Done with the batch: 1094\n",
      "Done with the batch: 1095\n",
      "Done with the batch: 1096\n",
      "Done with the batch: 1097\n",
      "Done with the batch: 1098\n",
      "Done with the batch: 1099\n",
      "Done with the batch: 1100\n",
      "Done with the batch: 1101\n",
      "Done with the batch: 1102\n",
      "Done with the batch: 1103\n",
      "Done with the batch: 1104\n",
      "Done with the batch: 1105\n",
      "Done with the batch: 1106\n",
      "Done with the batch: 1107\n",
      "Done with the batch: 1108\n",
      "Done with the batch: 1109\n",
      "Done with the batch: 1110\n",
      "Done with the batch: 1111\n",
      "Done with the batch: 1112\n",
      "Done with the batch: 1113\n",
      "Done with the batch: 1114\n",
      "Done with the batch: 1115\n",
      "Done with the batch: 1116\n",
      "Done with the batch: 1117\n",
      "Done with the batch: 1118\n",
      "Done with the batch: 1119\n",
      "Done with the batch: 1120\n",
      "Done with the batch: 1121\n",
      "Done with the batch: 1122\n",
      "Done with the batch: 1123\n",
      "Done with the batch: 1124\n",
      "Done with the batch: 1125\n",
      "Done with the batch: 1126\n",
      "Done with the batch: 1127\n",
      "Done with the batch: 1128\n",
      "Done with the batch: 1129\n",
      "Done with the batch: 1130\n",
      "Done with the batch: 1131\n",
      "Done with the batch: 1132\n",
      "Done with the batch: 1133\n",
      "Done with the batch: 1134\n",
      "Done with the batch: 1135\n",
      "Done with the batch: 1136\n",
      "Done with the batch: 1137\n",
      "Done with the batch: 1138\n",
      "Done with the batch: 1139\n",
      "Done with the batch: 1140\n",
      "Done with the batch: 1141\n",
      "Done with the batch: 1142\n",
      "Done with the batch: 1143\n",
      "Done with the batch: 1144\n",
      "Done with the batch: 1145\n",
      "Done with the batch: 1146\n",
      "Done with the batch: 1147\n",
      "Done with the batch: 1148\n",
      "Done with the batch: 1149\n",
      "Done with the batch: 1150\n",
      "Done with the batch: 1151\n",
      "Done with the batch: 1152\n",
      "Done with the batch: 1153\n",
      "Done with the batch: 1154\n",
      "Done with the batch: 1155\n",
      "Done with the batch: 1156\n",
      "Done with the batch: 1157\n",
      "Done with the batch: 1158\n",
      "Done with the batch: 1159\n",
      "Done with the batch: 1160\n",
      "Done with the batch: 1161\n",
      "Done with the batch: 1162\n",
      "Done with the batch: 1163\n",
      "Done with the batch: 1164\n",
      "Done with the batch: 1165\n",
      "Done with the batch: 1166\n",
      "Done with the batch: 1167\n",
      "Done with the batch: 1168\n",
      "Done with the batch: 1169\n",
      "Done with the batch: 1170\n",
      "Done with the batch: 1171\n",
      "Done with the batch: 1172\n",
      "Done with the batch: 1173\n",
      "Done with the batch: 1174\n",
      "Done with the batch: 1175\n",
      "Done with the batch: 1176\n",
      "Done with the batch: 1177\n",
      "Done with the batch: 1178\n",
      "Done with the batch: 1179\n",
      "Done with the batch: 1180\n",
      "Done with the batch: 1181\n",
      "Done with the batch: 1182\n",
      "Done with the batch: 1183\n",
      "Done with the batch: 1184\n",
      "Done with the batch: 1185\n",
      "Done with the batch: 1186\n",
      "Done with the batch: 1187\n",
      "Done with the batch: 1188\n",
      "Done with the batch: 1189\n",
      "Done with the batch: 1190\n",
      "Done with the batch: 1191\n",
      "Done with the batch: 1192\n",
      "Done with the batch: 1193\n",
      "Done with the batch: 1194\n",
      "Done with the batch: 1195\n",
      "Done with the batch: 1196\n",
      "Done with the batch: 1197\n",
      "Done with the batch: 1198\n",
      "Done with the batch: 1199\n",
      "Done with the batch: 1200\n",
      "Done with the batch: 1201\n",
      "Done with the batch: 1202\n",
      "Done with the batch: 1203\n",
      "Done with the batch: 1204\n",
      "Done with the batch: 1205\n",
      "Done with the batch: 1206\n",
      "Done with the batch: 1207\n",
      "Done with the batch: 1208\n",
      "Done with the batch: 1209\n",
      "Done with the batch: 1210\n",
      "Done with the batch: 1211\n",
      "Done with the batch: 1212\n",
      "Done with the batch: 1213\n",
      "Done with the batch: 1214\n",
      "Done with the batch: 1215\n",
      "Done with the batch: 1216\n",
      "Done with the batch: 1217\n",
      "Done with the batch: 1218\n",
      "Done with the batch: 1219\n",
      "Done with the batch: 1220\n",
      "Done with the batch: 1221\n",
      "Done with the batch: 1222\n",
      "Done with the batch: 1223\n",
      "Done with the batch: 1224\n",
      "Done with the batch: 1225\n",
      "Done with the batch: 1226\n",
      "Done with the batch: 1227\n",
      "Done with the batch: 1228\n",
      "Done with the batch: 1229\n",
      "Done with the batch: 1230\n",
      "Done with the batch: 1231\n",
      "Done with the batch: 1232\n",
      "Done with the batch: 1233\n",
      "Done with the batch: 1234\n",
      "Done with the batch: 1235\n",
      "Done with the batch: 1236\n",
      "Done with the batch: 1237\n",
      "Done with the batch: 1238\n",
      "Done with the batch: 1239\n",
      "Done with the batch: 1240\n",
      "Done with the batch: 1241\n",
      "Done with the batch: 1242\n",
      "Done with the batch: 1243\n",
      "Done with the batch: 1244\n",
      "Done with the batch: 1245\n",
      "Done with the batch: 1246\n",
      "Done with the batch: 1247\n",
      "Done with the batch: 1248\n",
      "Done with the batch: 1249\n",
      "Done with the batch: 1250\n",
      "Done with the batch: 1251\n",
      "Done with the batch: 1252\n",
      "Done with the batch: 1253\n",
      "Done with the batch: 1254\n",
      "Done with the batch: 1255\n",
      "Done with the batch: 1256\n",
      "Done with the batch: 1257\n",
      "Done with the batch: 1258\n",
      "Done with the batch: 1259\n",
      "Done with the batch: 1260\n",
      "Done with the batch: 1261\n",
      "Done with the batch: 1262\n",
      "Done with the batch: 1263\n",
      "Done with the batch: 1264\n",
      "Done with the batch: 1265\n",
      "Done with the batch: 1266\n",
      "Done with the batch: 1267\n",
      "Done with the batch: 1268\n",
      "Done with the batch: 1269\n",
      "Done with the batch: 1270\n",
      "Done with the batch: 1271\n",
      "Done with the batch: 1272\n",
      "Done with the batch: 1273\n",
      "Done with the batch: 1274\n",
      "Done with the batch: 1275\n",
      "Done with the batch: 1276\n",
      "Done with the batch: 1277\n",
      "Done with the batch: 1278\n",
      "Done with the batch: 1279\n",
      "Done with the batch: 1280\n",
      "Done with the batch: 1281\n",
      "Done with the batch: 1282\n",
      "Done with the batch: 1283\n",
      "Done with the batch: 1284\n",
      "Done with the batch: 1285\n",
      "Done with the batch: 1286\n",
      "Done with the batch: 1287\n",
      "Done with the batch: 1288\n",
      "Done with the batch: 1289\n",
      "Done with the batch: 1290\n",
      "Done with the batch: 1291\n",
      "Done with the batch: 1292\n",
      "Done with the batch: 1293\n",
      "Done with the batch: 1294\n",
      "Done with the batch: 1295\n",
      "Done with the batch: 1296\n",
      "Done with the batch: 1297\n",
      "Done with the batch: 1298\n",
      "Done with the batch: 1299\n",
      "Done with the batch: 1300\n",
      "Done with the batch: 1301\n",
      "Done with the batch: 1302\n",
      "Done with the batch: 1303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 1304\n",
      "Done with the batch: 1305\n",
      "Done with the batch: 1306\n",
      "Done with the batch: 1307\n",
      "Done with the batch: 1308\n",
      "Done with the batch: 1309\n",
      "Done with the batch: 1310\n",
      "Done with the batch: 1311\n",
      "Done with the batch: 1312\n",
      "Done with the batch: 1313\n",
      "Done with the batch: 1314\n",
      "Done with the batch: 1315\n",
      "Done with the batch: 1316\n",
      "Done with the batch: 1317\n",
      "Done with the batch: 1318\n",
      "Done with the batch: 1319\n",
      "Done with the batch: 1320\n",
      "Done with the batch: 1321\n",
      "Done with the batch: 1322\n",
      "Done with the batch: 1323\n",
      "Done with the batch: 1324\n",
      "Done with the batch: 1325\n",
      "Done with the batch: 1326\n",
      "Done with the batch: 1327\n",
      "Done with the batch: 1328\n",
      "Done with the batch: 1329\n",
      "Done with the batch: 1330\n",
      "Done with the batch: 1331\n",
      "Done with the batch: 1332\n",
      "Done with the batch: 1333\n",
      "Done with the batch: 1334\n",
      "Done with the batch: 1335\n",
      "Done with the batch: 1336\n",
      "Done with the batch: 1337\n",
      "Done with the batch: 1338\n",
      "Done with the batch: 1339\n",
      "Done with the batch: 1340\n",
      "Done with the batch: 1341\n",
      "Done with the batch: 1342\n",
      "Done with the batch: 1343\n",
      "Done with the batch: 1344\n",
      "Done with the batch: 1345\n",
      "Done with the batch: 1346\n",
      "Done with the batch: 1347\n",
      "Done with the batch: 1348\n",
      "Done with the batch: 1349\n",
      "Done with the batch: 1350\n",
      "Done with the batch: 1351\n",
      "Done with the batch: 1352\n",
      "Done with the batch: 1353\n",
      "Done with the batch: 1354\n",
      "Done with the batch: 1355\n",
      "Done with the batch: 1356\n",
      "Done with the batch: 1357\n",
      "Done with the batch: 1358\n",
      "Done with the batch: 1359\n",
      "Done with the batch: 1360\n",
      "Done with the batch: 1361\n",
      "Done with the batch: 1362\n",
      "Done with the batch: 1363\n",
      "Done with the batch: 1364\n",
      "Done with the batch: 1365\n",
      "Done with the batch: 1366\n",
      "Done with the batch: 1367\n",
      "Done with the batch: 1368\n",
      "Done with the batch: 1369\n",
      "Done with the batch: 1370\n",
      "Done with the batch: 1371\n",
      "Done with the batch: 1372\n",
      "Done with the batch: 1373\n",
      "Done with the batch: 1374\n",
      "Done with the batch: 1375\n",
      "Done with the batch: 1376\n",
      "Done with the batch: 1377\n",
      "Done with the batch: 1378\n",
      "Done with the batch: 1379\n",
      "Done with the batch: 1380\n",
      "Done with the batch: 1381\n",
      "Done with the batch: 1382\n",
      "Done with the batch: 1383\n",
      "Done with the batch: 1384\n",
      "Done with the batch: 1385\n",
      "Done with the batch: 1386\n",
      "Done with the batch: 1387\n",
      "Done with the batch: 1388\n",
      "Done with the batch: 1389\n",
      "Done with the batch: 1390\n",
      "Done with the batch: 1391\n",
      "Done with the batch: 1392\n",
      "Done with the batch: 1393\n",
      "Done with the batch: 1394\n",
      "Done with the batch: 1395\n",
      "Done with the batch: 1396\n",
      "Done with the batch: 1397\n",
      "Done with the batch: 1398\n",
      "Done with the batch: 1399\n",
      "Done with the batch: 1400\n",
      "Done with the batch: 1401\n",
      "Done with the batch: 1402\n",
      "Done with the batch: 1403\n",
      "Done with the batch: 1404\n",
      "Done with the batch: 1405\n",
      "Done with the batch: 1406\n",
      "Done with the batch: 1407\n",
      "Done with the batch: 1408\n",
      "Done with the batch: 1409\n",
      "Done with the batch: 1410\n",
      "Done with the batch: 1411\n",
      "Done with the batch: 1412\n",
      "Done with the batch: 1413\n",
      "Done with the batch: 1414\n",
      "Done with the batch: 1415\n",
      "Done with the batch: 1416\n",
      "Done with the batch: 1417\n",
      "Done with the batch: 1418\n",
      "Done with the batch: 1419\n",
      "Done with the batch: 1420\n",
      "Done with the batch: 1421\n",
      "Done with the batch: 1422\n",
      "Done with the batch: 1423\n",
      "Done with the batch: 1424\n",
      "Done with the batch: 1425\n",
      "Done with the batch: 1426\n",
      "Done with the batch: 1427\n",
      "Done with the batch: 1428\n",
      "Done with the batch: 1429\n",
      "Done with the batch: 1430\n",
      "Done with the batch: 1431\n",
      "Done with the batch: 1432\n",
      "Done with the batch: 1433\n",
      "Done with the batch: 1434\n",
      "Done with the batch: 1435\n",
      "Done with the batch: 1436\n",
      "Done with the batch: 1437\n",
      "Done with the batch: 1438\n",
      "Done with the batch: 1439\n",
      "Done with the batch: 1440\n",
      "Done with the batch: 1441\n",
      "Done with the batch: 1442\n",
      "Done with the batch: 1443\n",
      "Done with the batch: 1444\n",
      "Done with the batch: 1445\n",
      "Done with the batch: 1446\n",
      "Done with the batch: 1447\n",
      "Done with the batch: 1448\n",
      "Done with the batch: 1449\n",
      "Done with the batch: 1450\n",
      "Done with the batch: 1451\n",
      "Done with the batch: 1452\n",
      "Done with the batch: 1453\n",
      "Done with the batch: 1454\n",
      "Done with the batch: 1455\n",
      "Done with the batch: 1456\n",
      "Done with the batch: 1457\n",
      "Done with the batch: 1458\n",
      "Done with the batch: 1459\n",
      "Done with the batch: 1460\n",
      "Done with the batch: 1461\n",
      "Done with the batch: 1462\n",
      "Done with the batch: 1463\n",
      "Done with the batch: 1464\n",
      "Done with the batch: 1465\n",
      "Done with the batch: 1466\n",
      "Done with the batch: 1467\n",
      "Done with the batch: 1468\n",
      "Done with the batch: 1469\n",
      "Done with the batch: 1470\n",
      "Done with the batch: 1471\n",
      "Done with the batch: 1472\n",
      "Done with the batch: 1473\n",
      "Done with the batch: 1474\n",
      "Done with the batch: 1475\n",
      "Done with the batch: 1476\n",
      "Done with the batch: 1477\n",
      "Done with the batch: 1478\n",
      "Done with the batch: 1479\n",
      "Done with the batch: 1480\n",
      "Done with the batch: 1481\n",
      "Done with the batch: 1482\n",
      "Done with the batch: 1483\n",
      "Done with the batch: 1484\n",
      "Done with the batch: 1485\n",
      "Done with the batch: 1486\n",
      "Done with the batch: 1487\n",
      "Done with the batch: 1488\n",
      "Done with the batch: 1489\n",
      "Done with the batch: 1490\n",
      "Done with the batch: 1491\n",
      "Done with the batch: 1492\n",
      "Done with the batch: 1493\n",
      "Done with the batch: 1494\n",
      "Done with the batch: 1495\n",
      "Done with the batch: 1496\n",
      "Done with the batch: 1497\n",
      "Done with the batch: 1498\n",
      "Done with the batch: 1499\n",
      "Done with the batch: 1500\n",
      "Done with the batch: 1501\n",
      "Done with the batch: 1502\n",
      "Done with the batch: 1503\n",
      "Done with the batch: 1504\n",
      "Done with the batch: 1505\n",
      "Done with the batch: 1506\n",
      "Done with the batch: 1507\n",
      "Done with the batch: 1508\n",
      "Done with the batch: 1509\n",
      "Done with the batch: 1510\n",
      "Done with the batch: 1511\n",
      "Done with the batch: 1512\n",
      "Done with the batch: 1513\n",
      "Done with the batch: 1514\n",
      "Done with the batch: 1515\n",
      "Done with the batch: 1516\n",
      "Done with the batch: 1517\n",
      "Done with the batch: 1518\n",
      "Done with the batch: 1519\n",
      "Done with the batch: 1520\n",
      "Done with the batch: 1521\n",
      "Done with the batch: 1522\n",
      "Done with the batch: 1523\n",
      "Done with the batch: 1524\n",
      "Done with the batch: 1525\n",
      "Done with the batch: 1526\n",
      "Done with the batch: 1527\n",
      "Done with the batch: 1528\n",
      "Done with the batch: 1529\n",
      "Done with the batch: 1530\n",
      "Done with the batch: 1531\n",
      "Done with the batch: 1532\n",
      "Done with the batch: 1533\n",
      "Done with the batch: 1534\n",
      "Done with the batch: 1535\n",
      "Done with the batch: 1536\n",
      "Done with the batch: 1537\n",
      "Done with the batch: 1538\n",
      "Done with the batch: 1539\n",
      "Done with the batch: 1540\n",
      "Done with the batch: 1541\n",
      "Done with the batch: 1542\n",
      "Done with the batch: 1543\n",
      "Done with the batch: 1544\n",
      "Done with the batch: 1545\n",
      "Done with the batch: 1546\n",
      "Done with the batch: 1547\n",
      "Done with the batch: 1548\n",
      "Done with the batch: 1549\n",
      "Done with the batch: 1550\n",
      "Done with the batch: 1551\n",
      "Done with the batch: 1552\n",
      "Done with the batch: 1553\n",
      "Done with the batch: 1554\n",
      "Done with the batch: 1555\n",
      "Done with the batch: 1556\n",
      "Done with the batch: 1557\n",
      "Done with the batch: 1558\n",
      "Done with the batch: 1559\n",
      "Done with the batch: 1560\n",
      "Done with the batch: 1561\n",
      "Done with the batch: 1562\n",
      "Done with the batch: 1563\n",
      "Done with the batch: 1564\n",
      "Done with the batch: 1565\n",
      "Done with the batch: 1566\n",
      "Done with the batch: 1567\n",
      "Done with the batch: 1568\n",
      "Done with the batch: 1569\n",
      "Done with the batch: 1570\n",
      "Done with the batch: 1571\n",
      "Done with the batch: 1572\n",
      "Done with the batch: 1573\n",
      "Done with the batch: 1574\n",
      "Done with the batch: 1575\n",
      "Done with the batch: 1576\n",
      "Done with the batch: 1577\n",
      "Done with the batch: 1578\n",
      "Done with the batch: 1579\n",
      "Done with the batch: 1580\n",
      "Done with the batch: 1581\n",
      "Done with the batch: 1582\n",
      "Done with the batch: 1583\n",
      "Done with the batch: 1584\n",
      "Done with the batch: 1585\n",
      "Done with the batch: 1586\n",
      "Done with the batch: 1587\n",
      "Done with the batch: 1588\n",
      "Done with the batch: 1589\n",
      "Done with the batch: 1590\n",
      "Done with the batch: 1591\n",
      "Done with the batch: 1592\n",
      "Done with the batch: 1593\n",
      "Done with the batch: 1594\n",
      "Done with the batch: 1595\n",
      "Done with the batch: 1596\n",
      "Done with the batch: 1597\n",
      "Done with the batch: 1598\n",
      "Done with the batch: 1599\n",
      "Done with the batch: 1600\n",
      "Done with the batch: 1601\n",
      "Done with the batch: 1602\n",
      "Done with the batch: 1603\n",
      "Done with the batch: 1604\n",
      "Done with the batch: 1605\n",
      "Done with the batch: 1606\n",
      "Done with the batch: 1607\n",
      "Done with the batch: 1608\n",
      "Done with the batch: 1609\n",
      "Done with the batch: 1610\n",
      "Done with the batch: 1611\n",
      "Done with the batch: 1612\n",
      "Done with the batch: 1613\n",
      "Done with the batch: 1614\n",
      "Done with the batch: 1615\n",
      "Done with the batch: 1616\n",
      "Done with the batch: 1617\n",
      "(6470, 18432) (6470,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"3ba278ca-1854-4afa-b445-f2e27262b7c6\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"3ba278ca-1854-4afa-b445-f2e27262b7c6\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "X_Train_FeatureMap=np.empty((0,18432))\n",
    "Y_Train_FeatureMap=np.empty((0,batch_size))\n",
    "print(X_Train_FeatureMap.shape)\n",
    "for i,data in enumerate(trainloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    featureMap=net.get_Representation_Net(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Train_FeatureMap=np.append(X_Train_FeatureMap,featureMap,axis=0)\n",
    "    Y_Train_FeatureMap=np.append(Y_Train_FeatureMap,labels.numpy())\n",
    "print(X_Train_FeatureMap.shape,Y_Train_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff18344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 18432)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "(1618, 18432) (1618,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"5c37a3c6-0971-40d8-ba27-a9f00f06d9e3\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"5c37a3c6-0971-40d8-ba27-a9f00f06d9e3\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "X_Test_FeatureMap=np.empty((0,18432))\n",
    "Y_Test_FeatureMap=np.empty((0,batch_size))\n",
    "print(X_Test_FeatureMap.shape)\n",
    "for i,data in enumerate(testloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    featuremap=net.get_Representation_Net(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Test_FeatureMap=np.append(X_Test_FeatureMap,featuremap,axis=0)\n",
    "    Y_Test_FeatureMap=np.append(Y_Test_FeatureMap,labels.numpy())\n",
    "print(X_Test_FeatureMap.shape,Y_Test_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6784a",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2bc1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,roc_auc_score,recall_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29f53592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV 1/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 1/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.512 total time=  10.3s\n",
      "[CV 2/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 2/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.517 total time=  10.1s\n",
      "[CV 3/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 3/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.523 total time=  10.0s\n",
      "[CV 4/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 4/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.512 total time=  10.0s\n",
      "[CV 5/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 5/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.521 total time=  10.2s\n",
      "[CV 1/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 1/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.501 total time=  10.1s\n",
      "[CV 2/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 2/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.502 total time=   9.9s\n",
      "[CV 3/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 3/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.502 total time=   9.9s\n",
      "[CV 4/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 4/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.502 total time=  10.0s\n",
      "[CV 5/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 5/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.502 total time=   9.9s\n",
      "[CV 1/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 1/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.501 total time=  10.0s\n",
      "[CV 2/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 2/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.1s\n",
      "[CV 3/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 3/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.1s\n",
      "[CV 4/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 4/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.1s\n",
      "[CV 5/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 5/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.0s\n",
      "[CV 1/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 1/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.501 total time=  10.6s\n",
      "[CV 2/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 2/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.502 total time=  10.7s\n",
      "[CV 3/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 3/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 4/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 4/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 5/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 5/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.502 total time=  10.6s\n",
      "[CV 1/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 1/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.501 total time=  10.8s\n",
      "[CV 2/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 2/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.502 total time=  10.7s\n",
      "[CV 3/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 3/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.502 total time=  10.9s\n",
      "[CV 4/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 4/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.502 total time=  10.7s\n",
      "[CV 5/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 5/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.502 total time=  10.6s\n",
      "[CV 1/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 1/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.941 total time=   3.5s\n",
      "[CV 2/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 2/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.937 total time=   3.5s\n",
      "[CV 3/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 3/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.926 total time=   3.5s\n",
      "[CV 4/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 4/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.943 total time=   3.6s\n",
      "[CV 5/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 5/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.935 total time=   3.5s\n",
      "[CV 1/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 1/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.577 total time=   9.5s\n",
      "[CV 2/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 2/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.589 total time=   9.7s\n",
      "[CV 3/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 3/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.597 total time=   9.6s\n",
      "[CV 4/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 4/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.600 total time=   9.5s\n",
      "[CV 5/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 5/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.591 total time=   9.5s\n",
      "[CV 1/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 1/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.501 total time=  10.1s\n",
      "[CV 2/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 2/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.0s\n",
      "[CV 3/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 3/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=  11.0s\n",
      "[CV 4/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 4/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 5/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 5/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=  11.7s\n",
      "[CV 1/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 1/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.501 total time=  12.9s\n",
      "[CV 2/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 2/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=  12.4s\n",
      "[CV 3/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 3/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=  11.3s\n",
      "[CV 4/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 4/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=  11.5s\n",
      "[CV 5/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 5/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=  12.8s\n",
      "[CV 1/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 1/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.501 total time=  12.6s\n",
      "[CV 2/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 2/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.502 total time=  13.2s\n",
      "[CV 3/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 3/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.502 total time=  12.9s\n",
      "[CV 4/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 4/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.502 total time=  13.4s\n",
      "[CV 5/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 5/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.502 total time=  14.0s\n",
      "[CV 1/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.965 total time=   2.9s\n",
      "[CV 2/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 2/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.959 total time=   9.2s\n",
      "[CV 3/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 3/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.970 total time=   8.7s\n",
      "[CV 4/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 4/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.965 total time=   8.3s\n",
      "[CV 5/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 5/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.967 total time=   8.4s\n",
      "[CV 1/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 1/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.924 total time=  30.7s\n",
      "[CV 2/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 2/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.909 total time=  30.7s\n",
      "[CV 3/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 3/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.901 total time=  31.0s\n",
      "[CV 4/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 4/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.896 total time=  30.6s\n",
      "[CV 5/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 5/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.917 total time=  31.9s\n",
      "[CV 1/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 1/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.606 total time=  44.1s\n",
      "[CV 2/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 2/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.604 total time=  34.8s\n",
      "[CV 3/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 3/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.617 total time=  34.8s\n",
      "[CV 4/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 4/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.630 total time=  34.1s\n",
      "[CV 5/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 5/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.609 total time=  35.6s\n",
      "[CV 1/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 1/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.577 total time=  37.0s\n",
      "[CV 2/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 2/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.577 total time=  38.3s\n",
      "[CV 3/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 3/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.566 total time=  36.4s\n",
      "[CV 4/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 4/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.590 total time=  34.9s\n",
      "[CV 5/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 5/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.578 total time=  36.5s\n",
      "[CV 1/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 1/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.548 total time=  33.8s\n",
      "[CV 2/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 2/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.550 total time=  34.4s\n",
      "[CV 3/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 3/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.547 total time=  34.3s\n",
      "[CV 4/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 4/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.564 total time=  34.3s\n",
      "[CV 5/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 5/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.555 total time=  34.8s\n",
      "[CV 1/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 1/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.964 total time=   7.6s\n",
      "[CV 2/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 2/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.958 total time=   7.5s\n",
      "[CV 3/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 3/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.964 total time=   7.5s\n",
      "[CV 4/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 4/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.968 total time=   7.6s\n",
      "[CV 5/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 5/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.965 total time=   7.7s\n",
      "[CV 1/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 1/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.923 total time=  29.5s\n",
      "[CV 2/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 2/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.907 total time=  29.3s\n",
      "[CV 3/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 3/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.904 total time=  28.6s\n",
      "[CV 4/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 4/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.896 total time=  29.9s\n",
      "[CV 5/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 5/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.920 total time=  30.0s\n",
      "[CV 1/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 1/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.613 total time=  33.7s\n",
      "[CV 2/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 2/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.608 total time=  33.6s\n",
      "[CV 3/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 3/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.619 total time=  33.9s\n",
      "[CV 4/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 4/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.639 total time=  33.0s\n",
      "[CV 5/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 5/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.613 total time=  32.7s\n",
      "[CV 1/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 1/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.581 total time=  33.4s\n",
      "[CV 2/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 2/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.577 total time=  33.1s\n",
      "[CV 3/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 3/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.570 total time=  33.3s\n",
      "[CV 4/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 4/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.594 total time=  33.3s\n",
      "[CV 5/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 5/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.580 total time=  33.3s\n",
      "[CV 1/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 1/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.550 total time=  33.1s\n",
      "[CV 2/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 2/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.551 total time=  33.3s\n",
      "[CV 3/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 3/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.548 total time=  34.0s\n",
      "[CV 4/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 4/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.566 total time=  33.5s\n",
      "[CV 5/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 5/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.557 total time=  34.0s\n",
      "[CV 1/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 1/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.963 total time=   7.7s\n",
      "[CV 2/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.955 total time=   7.2s\n",
      "[CV 3/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 3/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.956 total time=   7.2s\n",
      "[CV 4/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 4/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.963 total time=   7.3s\n",
      "[CV 5/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 5/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.958 total time=   7.2s\n",
      "[CV 1/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 1/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.922 total time=  29.3s\n",
      "[CV 2/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 2/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.905 total time=  29.3s\n",
      "[CV 3/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 3/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.903 total time=  30.6s\n",
      "[CV 4/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 4/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.894 total time=  30.0s\n",
      "[CV 5/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 5/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.917 total time=  31.1s\n",
      "[CV 1/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 1/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.614 total time=  33.9s\n",
      "[CV 2/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 2/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.608 total time=  33.3s\n",
      "[CV 3/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 3/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.619 total time=  33.9s\n",
      "[CV 4/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 4/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.639 total time=  33.6s\n",
      "[CV 5/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 5/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.613 total time=  33.0s\n",
      "[CV 1/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 1/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.581 total time=  34.7s\n",
      "[CV 2/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 2/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.577 total time=  34.6s\n",
      "[CV 3/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 3/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.570 total time=  34.5s\n",
      "[CV 4/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 4/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.594 total time=  33.7s\n",
      "[CV 5/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 5/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.580 total time=  33.5s\n",
      "[CV 1/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 1/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.550 total time=  33.4s\n",
      "[CV 2/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 2/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.551 total time=  33.5s\n",
      "[CV 3/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 3/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.548 total time=  33.4s\n",
      "[CV 4/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 4/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.566 total time=  33.9s\n",
      "[CV 5/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 5/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.557 total time=  35.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.001, 0.1, 1, 10, 100],\n",
       "                         'gamma': [0.001, 0.01, 0.1, 1, 10],\n",
       "                         'kernel': ['rbf']},\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_parameters = {'kernel': ['rbf'], 'gamma': [1e-3, 1e-2,0.1,1,10],\n",
    "                     'C': [0.001,0.1,1, 10, 100],\n",
    "}\n",
    "# tuned_parameters = {'kernel': ['rbf'], 'gamma': [1e-3],\n",
    "#                      'C': [0.001],\n",
    "#                    }\n",
    "clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring= 'accuracy',verbose=10\n",
    "    )\n",
    "clf.fit(X_Train, Y_Train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64a4ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_svm(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "#         clf=SVC(C=10,kernel='rbf',gamma=0.0001)\n",
    "        clf=SVC(C=10,kernel='rbf',gamma=10)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del clf\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43b32274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 96.21329211746522\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 94.97681607418856\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 97.14064914992272\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 96.29057187017001\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 96.83153013910355\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 96.21329211746522 %\n",
      "Fold 1: 94.97681607418856 %\n",
      "Fold 2: 97.14064914992272 %\n",
      "Fold 3: 96.29057187017001 %\n",
      "Fold 4: 96.83153013910355 %\n",
      "Average: 96.29057187017001 %\n",
      "Accuracy:  0.9208899876390606\n",
      "F1-Score:  0.921855921855922\n",
      "Precision:  0.9207317073170732\n",
      "Recall:  0.9229828850855746\n",
      "AUC:  0.9208664425427873\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_svm(X_Train,Y_Train.ravel())\n",
    "clf=SVC(C=10,kernel='rbf',gamma=0.0001)\n",
    "clf.fit(X_Train,Y_Train.ravel())\n",
    "y_pred = clf.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45072389",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "741eaad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "043c0537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_dtree(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        decision_tree = DecisionTreeClassifier(random_state=102)\n",
    "        decision_tree = decision_tree.fit(X_train, y_train.ravel())\n",
    "        y_pred = decision_tree.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del decision_tree\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eaf18c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 93.19938176197836\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 94.5904173106646\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 93.81761978361669\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 94.35857805255023\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 93.50850077279753\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 93.19938176197836 %\n",
      "Fold 1: 94.5904173106646 %\n",
      "Fold 2: 93.81761978361669 %\n",
      "Fold 3: 94.35857805255023 %\n",
      "Fold 4: 93.50850077279753 %\n",
      "Average: 93.89489953632149 %\n",
      "Accuracy:  0.899876390605686\n",
      "F1-Score:  0.9027611044417767\n",
      "Precision:  0.8867924528301887\n",
      "Recall:  0.9193154034229829\n",
      "AUC:  0.8996577017114914\n"
     ]
    }
   ],
   "source": [
    "dtree=k_fold_cv_dtree(X_Train,Y_Train.ravel())\n",
    "dtree = DecisionTreeClassifier(random_state=102)\n",
    "dtree = dtree.fit(X_Train, Y_Train.ravel())\n",
    "y_pred=dtree.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9516e",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ffdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ebbd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_xgb(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        eval_set = [(X_train, y_train.ravel()), (X_test, y_test)]\n",
    "        xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=102,use_label_encoder=False)\n",
    "        xg_cl.fit(X_train,y_train.ravel())\n",
    "        y_pred = xg_cl.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del xg_cl\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return xg_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa35d1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "[14:43:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.59969088098919\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "[14:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.13601236476043\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "[14:44:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.67697063369397\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "[14:44:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.36785162287481\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "[14:44:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.21329211746522\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 96.59969088098919 %\n",
      "Fold 1: 96.13601236476043 %\n",
      "Fold 2: 96.67697063369397 %\n",
      "Fold 3: 96.36785162287481 %\n",
      "Fold 4: 96.21329211746522 %\n",
      "Average: 96.39876352395672 %\n",
      "[14:44:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy:  0.9227441285537701\n",
      "F1-Score:  0.9243799153055051\n",
      "Precision:  0.9149700598802395\n",
      "Recall:  0.9339853300733496\n",
      "AUC:  0.9226176650366748\n"
     ]
    }
   ],
   "source": [
    "xg=k_fold_cv_xgb(X_Train,Y_Train.ravel())\n",
    "xg = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=102,use_label_encoder=False)\n",
    "xg.fit(X_Train,Y_Train.ravel())\n",
    "y_pred=xg.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b25566",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2646f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b9c1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_mlp(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        clf = MLPClassifier(random_state=102, max_iter=3000, verbose=True).fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del clf\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f91d187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.54862838\n",
      "Iteration 2, loss = 0.20145201\n",
      "Iteration 3, loss = 0.17676709\n",
      "Iteration 4, loss = 0.15865270\n",
      "Iteration 5, loss = 0.14569061\n",
      "Iteration 6, loss = 0.13140744\n",
      "Iteration 7, loss = 0.12309088\n",
      "Iteration 8, loss = 0.11730934\n",
      "Iteration 9, loss = 0.11303174\n",
      "Iteration 10, loss = 0.10941145\n",
      "Iteration 11, loss = 0.10460121\n",
      "Iteration 12, loss = 0.10331378\n",
      "Iteration 13, loss = 0.09842188\n",
      "Iteration 14, loss = 0.09644764\n",
      "Iteration 15, loss = 0.09052133\n",
      "Iteration 16, loss = 0.09359614\n",
      "Iteration 17, loss = 0.08805536\n",
      "Iteration 18, loss = 0.08361891\n",
      "Iteration 19, loss = 0.08307174\n",
      "Iteration 20, loss = 0.08233506\n",
      "Iteration 21, loss = 0.08170814\n",
      "Iteration 22, loss = 0.08164868\n",
      "Iteration 23, loss = 0.08355770\n",
      "Iteration 24, loss = 0.07310964\n",
      "Iteration 25, loss = 0.07461062\n",
      "Iteration 26, loss = 0.07308495\n",
      "Iteration 27, loss = 0.07384102\n",
      "Iteration 28, loss = 0.07127505\n",
      "Iteration 29, loss = 0.07095963\n",
      "Iteration 30, loss = 0.06738504\n",
      "Iteration 31, loss = 0.06534472\n",
      "Iteration 32, loss = 0.06426300\n",
      "Iteration 33, loss = 0.06829838\n",
      "Iteration 34, loss = 0.06341072\n",
      "Iteration 35, loss = 0.06115313\n",
      "Iteration 36, loss = 0.06009761\n",
      "Iteration 37, loss = 0.06160835\n",
      "Iteration 38, loss = 0.05882808\n",
      "Iteration 39, loss = 0.06347320\n",
      "Iteration 40, loss = 0.05974294\n",
      "Iteration 41, loss = 0.05924087\n",
      "Iteration 42, loss = 0.06040037\n",
      "Iteration 43, loss = 0.05963537\n",
      "Iteration 44, loss = 0.06270743\n",
      "Iteration 45, loss = 0.05907627\n",
      "Iteration 46, loss = 0.05736986\n",
      "Iteration 47, loss = 0.06153792\n",
      "Iteration 48, loss = 0.05650038\n",
      "Iteration 49, loss = 0.05611936\n",
      "Iteration 50, loss = 0.05664202\n",
      "Iteration 51, loss = 0.05217518\n",
      "Iteration 52, loss = 0.05160993\n",
      "Iteration 53, loss = 0.05471262\n",
      "Iteration 54, loss = 0.05404551\n",
      "Iteration 55, loss = 0.05163052\n",
      "Iteration 56, loss = 0.05085053\n",
      "Iteration 57, loss = 0.05149480\n",
      "Iteration 58, loss = 0.04939833\n",
      "Iteration 59, loss = 0.05220662\n",
      "Iteration 60, loss = 0.05348969\n",
      "Iteration 61, loss = 0.05317690\n",
      "Iteration 62, loss = 0.05680801\n",
      "Iteration 63, loss = 0.04614022\n",
      "Iteration 64, loss = 0.05066533\n",
      "Iteration 65, loss = 0.05569938\n",
      "Iteration 66, loss = 0.05005650\n",
      "Iteration 67, loss = 0.05110487\n",
      "Iteration 68, loss = 0.05284286\n",
      "Iteration 69, loss = 0.04877088\n",
      "Iteration 70, loss = 0.04693070\n",
      "Iteration 71, loss = 0.04275622\n",
      "Iteration 72, loss = 0.04400440\n",
      "Iteration 73, loss = 0.04650691\n",
      "Iteration 74, loss = 0.04711408\n",
      "Iteration 75, loss = 0.04309845\n",
      "Iteration 76, loss = 0.04482133\n",
      "Iteration 77, loss = 0.04197233\n",
      "Iteration 78, loss = 0.04516928\n",
      "Iteration 79, loss = 0.05091477\n",
      "Iteration 80, loss = 0.04361734\n",
      "Iteration 81, loss = 0.04609924\n",
      "Iteration 82, loss = 0.04186176\n",
      "Iteration 83, loss = 0.04528912\n",
      "Iteration 84, loss = 0.04560101\n",
      "Iteration 85, loss = 0.04182491\n",
      "Iteration 86, loss = 0.04488725\n",
      "Iteration 87, loss = 0.04190142\n",
      "Iteration 88, loss = 0.03951242\n",
      "Iteration 89, loss = 0.03934520\n",
      "Iteration 90, loss = 0.04361255\n",
      "Iteration 91, loss = 0.04154204\n",
      "Iteration 92, loss = 0.04278199\n",
      "Iteration 93, loss = 0.04403135\n",
      "Iteration 94, loss = 0.04007217\n",
      "Iteration 95, loss = 0.04232406\n",
      "Iteration 96, loss = 0.03999705\n",
      "Iteration 97, loss = 0.03952847\n",
      "Iteration 98, loss = 0.04116129\n",
      "Iteration 99, loss = 0.03591654\n",
      "Iteration 100, loss = 0.03934157\n",
      "Iteration 101, loss = 0.04630422\n",
      "Iteration 102, loss = 0.04410112\n",
      "Iteration 103, loss = 0.04239131\n",
      "Iteration 104, loss = 0.03969077\n",
      "Iteration 105, loss = 0.03996926\n",
      "Iteration 106, loss = 0.03662287\n",
      "Iteration 107, loss = 0.03598563\n",
      "Iteration 108, loss = 0.03823795\n",
      "Iteration 109, loss = 0.04019637\n",
      "Iteration 110, loss = 0.03839118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 96.52241112828439\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.51165943\n",
      "Iteration 2, loss = 0.20618195\n",
      "Iteration 3, loss = 0.17892902\n",
      "Iteration 4, loss = 0.16045785\n",
      "Iteration 5, loss = 0.15466218\n",
      "Iteration 6, loss = 0.14537274\n",
      "Iteration 7, loss = 0.13457938\n",
      "Iteration 8, loss = 0.13631469\n",
      "Iteration 9, loss = 0.12584218\n",
      "Iteration 10, loss = 0.12062378\n",
      "Iteration 11, loss = 0.11406993\n",
      "Iteration 12, loss = 0.10880819\n",
      "Iteration 13, loss = 0.11197910\n",
      "Iteration 14, loss = 0.10457724\n",
      "Iteration 15, loss = 0.10409191\n",
      "Iteration 16, loss = 0.09909835\n",
      "Iteration 17, loss = 0.09687550\n",
      "Iteration 18, loss = 0.09938229\n",
      "Iteration 19, loss = 0.10277985\n",
      "Iteration 20, loss = 0.09016392\n",
      "Iteration 21, loss = 0.08985631\n",
      "Iteration 22, loss = 0.09206825\n",
      "Iteration 23, loss = 0.08520655\n",
      "Iteration 24, loss = 0.08631962\n",
      "Iteration 25, loss = 0.08506229\n",
      "Iteration 26, loss = 0.08357234\n",
      "Iteration 27, loss = 0.08199183\n",
      "Iteration 28, loss = 0.07909797\n",
      "Iteration 29, loss = 0.07706321\n",
      "Iteration 30, loss = 0.08038532\n",
      "Iteration 31, loss = 0.08333853\n",
      "Iteration 32, loss = 0.08067517\n",
      "Iteration 33, loss = 0.07941506\n",
      "Iteration 34, loss = 0.07394311\n",
      "Iteration 35, loss = 0.07581158\n",
      "Iteration 36, loss = 0.07561425\n",
      "Iteration 37, loss = 0.07022109\n",
      "Iteration 38, loss = 0.07242617\n",
      "Iteration 39, loss = 0.07618597\n",
      "Iteration 40, loss = 0.06788007\n",
      "Iteration 41, loss = 0.06563533\n",
      "Iteration 42, loss = 0.06514360\n",
      "Iteration 43, loss = 0.06636347\n",
      "Iteration 44, loss = 0.06853145\n",
      "Iteration 45, loss = 0.06877236\n",
      "Iteration 46, loss = 0.06770509\n",
      "Iteration 47, loss = 0.06782805\n",
      "Iteration 48, loss = 0.06733565\n",
      "Iteration 49, loss = 0.07285742\n",
      "Iteration 50, loss = 0.06488909\n",
      "Iteration 51, loss = 0.06252499\n",
      "Iteration 52, loss = 0.05880216\n",
      "Iteration 53, loss = 0.06191727\n",
      "Iteration 54, loss = 0.06930664\n",
      "Iteration 55, loss = 0.06132000\n",
      "Iteration 56, loss = 0.06135158\n",
      "Iteration 57, loss = 0.05959206\n",
      "Iteration 58, loss = 0.06162300\n",
      "Iteration 59, loss = 0.06144110\n",
      "Iteration 60, loss = 0.06775966\n",
      "Iteration 61, loss = 0.05800608\n",
      "Iteration 62, loss = 0.05730090\n",
      "Iteration 63, loss = 0.05385976\n",
      "Iteration 64, loss = 0.05514254\n",
      "Iteration 65, loss = 0.05927696\n",
      "Iteration 66, loss = 0.05492732\n",
      "Iteration 67, loss = 0.05602012\n",
      "Iteration 68, loss = 0.05554914\n",
      "Iteration 69, loss = 0.06300838\n",
      "Iteration 70, loss = 0.06067785\n",
      "Iteration 71, loss = 0.05876505\n",
      "Iteration 72, loss = 0.05694347\n",
      "Iteration 73, loss = 0.05620448\n",
      "Iteration 74, loss = 0.05954506\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 96.98608964451314\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.58985376\n",
      "Iteration 2, loss = 0.21584959\n",
      "Iteration 3, loss = 0.18283371\n",
      "Iteration 4, loss = 0.16447145\n",
      "Iteration 5, loss = 0.15222556\n",
      "Iteration 6, loss = 0.14243905\n",
      "Iteration 7, loss = 0.13642319\n",
      "Iteration 8, loss = 0.12767669\n",
      "Iteration 9, loss = 0.12406540\n",
      "Iteration 10, loss = 0.12110956\n",
      "Iteration 11, loss = 0.12064449\n",
      "Iteration 12, loss = 0.11735982\n",
      "Iteration 13, loss = 0.11174763\n",
      "Iteration 14, loss = 0.10462176\n",
      "Iteration 15, loss = 0.10744364\n",
      "Iteration 16, loss = 0.09921601\n",
      "Iteration 17, loss = 0.09352876\n",
      "Iteration 18, loss = 0.09403475\n",
      "Iteration 19, loss = 0.09449478\n",
      "Iteration 20, loss = 0.09564695\n",
      "Iteration 21, loss = 0.08732972\n",
      "Iteration 22, loss = 0.08967642\n",
      "Iteration 23, loss = 0.08375187\n",
      "Iteration 24, loss = 0.08564305\n",
      "Iteration 25, loss = 0.08291429\n",
      "Iteration 26, loss = 0.08288153\n",
      "Iteration 27, loss = 0.08031530\n",
      "Iteration 28, loss = 0.07965218\n",
      "Iteration 29, loss = 0.07865006\n",
      "Iteration 30, loss = 0.07624417\n",
      "Iteration 31, loss = 0.07572457\n",
      "Iteration 32, loss = 0.07763009\n",
      "Iteration 33, loss = 0.07158071\n",
      "Iteration 34, loss = 0.07041992\n",
      "Iteration 35, loss = 0.07092196\n",
      "Iteration 36, loss = 0.07264501\n",
      "Iteration 37, loss = 0.07264464\n",
      "Iteration 38, loss = 0.07170151\n",
      "Iteration 39, loss = 0.06702924\n",
      "Iteration 40, loss = 0.06623884\n",
      "Iteration 41, loss = 0.06665127\n",
      "Iteration 42, loss = 0.06987434\n",
      "Iteration 43, loss = 0.06314231\n",
      "Iteration 44, loss = 0.06231505\n",
      "Iteration 45, loss = 0.05934876\n",
      "Iteration 46, loss = 0.06147052\n",
      "Iteration 47, loss = 0.06568939\n",
      "Iteration 48, loss = 0.06225893\n",
      "Iteration 49, loss = 0.06148025\n",
      "Iteration 50, loss = 0.05992888\n",
      "Iteration 51, loss = 0.05723824\n",
      "Iteration 52, loss = 0.05771726\n",
      "Iteration 53, loss = 0.06129032\n",
      "Iteration 54, loss = 0.06258702\n",
      "Iteration 55, loss = 0.07461383\n",
      "Iteration 56, loss = 0.06606860\n",
      "Iteration 57, loss = 0.05843294\n",
      "Iteration 58, loss = 0.05656067\n",
      "Iteration 59, loss = 0.06000505\n",
      "Iteration 60, loss = 0.05639282\n",
      "Iteration 61, loss = 0.05504099\n",
      "Iteration 62, loss = 0.05777093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63, loss = 0.05676613\n",
      "Iteration 64, loss = 0.05247538\n",
      "Iteration 65, loss = 0.05624558\n",
      "Iteration 66, loss = 0.05187407\n",
      "Iteration 67, loss = 0.05003512\n",
      "Iteration 68, loss = 0.04841918\n",
      "Iteration 69, loss = 0.05200677\n",
      "Iteration 70, loss = 0.05461590\n",
      "Iteration 71, loss = 0.06369813\n",
      "Iteration 72, loss = 0.05742996\n",
      "Iteration 73, loss = 0.05027307\n",
      "Iteration 74, loss = 0.04646316\n",
      "Iteration 75, loss = 0.04740301\n",
      "Iteration 76, loss = 0.04880688\n",
      "Iteration 77, loss = 0.05436594\n",
      "Iteration 78, loss = 0.05224846\n",
      "Iteration 79, loss = 0.04831154\n",
      "Iteration 80, loss = 0.04495785\n",
      "Iteration 81, loss = 0.04663157\n",
      "Iteration 82, loss = 0.04556237\n",
      "Iteration 83, loss = 0.04820973\n",
      "Iteration 84, loss = 0.05078051\n",
      "Iteration 85, loss = 0.05202978\n",
      "Iteration 86, loss = 0.04605520\n",
      "Iteration 87, loss = 0.04738584\n",
      "Iteration 88, loss = 0.04406531\n",
      "Iteration 89, loss = 0.04251847\n",
      "Iteration 90, loss = 0.04495850\n",
      "Iteration 91, loss = 0.04197180\n",
      "Iteration 92, loss = 0.04411705\n",
      "Iteration 93, loss = 0.04744805\n",
      "Iteration 94, loss = 0.04160236\n",
      "Iteration 95, loss = 0.04781550\n",
      "Iteration 96, loss = 0.05767826\n",
      "Iteration 97, loss = 0.04962351\n",
      "Iteration 98, loss = 0.04681587\n",
      "Iteration 99, loss = 0.04610944\n",
      "Iteration 100, loss = 0.04597117\n",
      "Iteration 101, loss = 0.04013006\n",
      "Iteration 102, loss = 0.04226798\n",
      "Iteration 103, loss = 0.04034531\n",
      "Iteration 104, loss = 0.04250132\n",
      "Iteration 105, loss = 0.04318518\n",
      "Iteration 106, loss = 0.04103619\n",
      "Iteration 107, loss = 0.03998004\n",
      "Iteration 108, loss = 0.03815720\n",
      "Iteration 109, loss = 0.03940923\n",
      "Iteration 110, loss = 0.03824342\n",
      "Iteration 111, loss = 0.03739857\n",
      "Iteration 112, loss = 0.03846580\n",
      "Iteration 113, loss = 0.03641219\n",
      "Iteration 114, loss = 0.05257126\n",
      "Iteration 115, loss = 0.05042992\n",
      "Iteration 116, loss = 0.03797154\n",
      "Iteration 117, loss = 0.03799165\n",
      "Iteration 118, loss = 0.03410321\n",
      "Iteration 119, loss = 0.03505826\n",
      "Iteration 120, loss = 0.04143413\n",
      "Iteration 121, loss = 0.04187558\n",
      "Iteration 122, loss = 0.04018335\n",
      "Iteration 123, loss = 0.03527040\n",
      "Iteration 124, loss = 0.03879511\n",
      "Iteration 125, loss = 0.03327331\n",
      "Iteration 126, loss = 0.03863086\n",
      "Iteration 127, loss = 0.03695194\n",
      "Iteration 128, loss = 0.03438849\n",
      "Iteration 129, loss = 0.03387651\n",
      "Iteration 130, loss = 0.03398168\n",
      "Iteration 131, loss = 0.03267777\n",
      "Iteration 132, loss = 0.03200116\n",
      "Iteration 133, loss = 0.03402067\n",
      "Iteration 134, loss = 0.03725835\n",
      "Iteration 135, loss = 0.04777970\n",
      "Iteration 136, loss = 0.03894969\n",
      "Iteration 137, loss = 0.04115496\n",
      "Iteration 138, loss = 0.03758923\n",
      "Iteration 139, loss = 0.03155038\n",
      "Iteration 140, loss = 0.03179933\n",
      "Iteration 141, loss = 0.03244963\n",
      "Iteration 142, loss = 0.03412089\n",
      "Iteration 143, loss = 0.03101177\n",
      "Iteration 144, loss = 0.03672685\n",
      "Iteration 145, loss = 0.03951344\n",
      "Iteration 146, loss = 0.03345786\n",
      "Iteration 147, loss = 0.03768496\n",
      "Iteration 148, loss = 0.03360997\n",
      "Iteration 149, loss = 0.03229030\n",
      "Iteration 150, loss = 0.03309387\n",
      "Iteration 151, loss = 0.02993459\n",
      "Iteration 152, loss = 0.03017659\n",
      "Iteration 153, loss = 0.02985945\n",
      "Iteration 154, loss = 0.02953586\n",
      "Iteration 155, loss = 0.03119763\n",
      "Iteration 156, loss = 0.03264894\n",
      "Iteration 157, loss = 0.03368396\n",
      "Iteration 158, loss = 0.03243378\n",
      "Iteration 159, loss = 0.03185178\n",
      "Iteration 160, loss = 0.03380743\n",
      "Iteration 161, loss = 0.04219006\n",
      "Iteration 162, loss = 0.04030943\n",
      "Iteration 163, loss = 0.03930346\n",
      "Iteration 164, loss = 0.03024375\n",
      "Iteration 165, loss = 0.02851651\n",
      "Iteration 166, loss = 0.02975643\n",
      "Iteration 167, loss = 0.03150463\n",
      "Iteration 168, loss = 0.03557290\n",
      "Iteration 169, loss = 0.03817032\n",
      "Iteration 170, loss = 0.03658046\n",
      "Iteration 171, loss = 0.03092733\n",
      "Iteration 172, loss = 0.03071956\n",
      "Iteration 173, loss = 0.02987493\n",
      "Iteration 174, loss = 0.03757103\n",
      "Iteration 175, loss = 0.03182898\n",
      "Iteration 176, loss = 0.03512810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 96.13601236476043\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.62811062\n",
      "Iteration 2, loss = 0.22561659\n",
      "Iteration 3, loss = 0.17660716\n",
      "Iteration 4, loss = 0.15728533\n",
      "Iteration 5, loss = 0.14490736\n",
      "Iteration 6, loss = 0.13945982\n",
      "Iteration 7, loss = 0.12872848\n",
      "Iteration 8, loss = 0.12065973\n",
      "Iteration 9, loss = 0.11636200\n",
      "Iteration 10, loss = 0.11679055\n",
      "Iteration 11, loss = 0.11197096\n",
      "Iteration 12, loss = 0.10665649\n",
      "Iteration 13, loss = 0.10304377\n",
      "Iteration 14, loss = 0.09981461\n",
      "Iteration 15, loss = 0.09755475\n",
      "Iteration 16, loss = 0.09855519\n",
      "Iteration 17, loss = 0.10449862\n",
      "Iteration 18, loss = 0.09622633\n",
      "Iteration 19, loss = 0.08759268\n",
      "Iteration 20, loss = 0.08683855\n",
      "Iteration 21, loss = 0.08560936\n",
      "Iteration 22, loss = 0.08372108\n",
      "Iteration 23, loss = 0.08406262\n",
      "Iteration 24, loss = 0.07826743\n",
      "Iteration 25, loss = 0.07810786\n",
      "Iteration 26, loss = 0.07534956\n",
      "Iteration 27, loss = 0.07310703\n",
      "Iteration 28, loss = 0.07401350\n",
      "Iteration 29, loss = 0.07528860\n",
      "Iteration 30, loss = 0.08450614\n",
      "Iteration 31, loss = 0.07636357\n",
      "Iteration 32, loss = 0.07104863\n",
      "Iteration 33, loss = 0.07050594\n",
      "Iteration 34, loss = 0.07217289\n",
      "Iteration 35, loss = 0.07205871\n",
      "Iteration 36, loss = 0.06692710\n",
      "Iteration 37, loss = 0.06602128\n",
      "Iteration 38, loss = 0.06971079\n",
      "Iteration 39, loss = 0.06749234\n",
      "Iteration 40, loss = 0.06452522\n",
      "Iteration 41, loss = 0.06558745\n",
      "Iteration 42, loss = 0.06769960\n",
      "Iteration 43, loss = 0.06300849\n",
      "Iteration 44, loss = 0.06668837\n",
      "Iteration 45, loss = 0.06460442\n",
      "Iteration 46, loss = 0.06106335\n",
      "Iteration 47, loss = 0.05999521\n",
      "Iteration 48, loss = 0.06439673\n",
      "Iteration 49, loss = 0.06191697\n",
      "Iteration 50, loss = 0.05788517\n",
      "Iteration 51, loss = 0.06190842\n",
      "Iteration 52, loss = 0.05706909\n",
      "Iteration 53, loss = 0.05979999\n",
      "Iteration 54, loss = 0.05660539\n",
      "Iteration 55, loss = 0.06314164\n",
      "Iteration 56, loss = 0.06182321\n",
      "Iteration 57, loss = 0.06218208\n",
      "Iteration 58, loss = 0.06205865\n",
      "Iteration 59, loss = 0.05873133\n",
      "Iteration 60, loss = 0.05289334\n",
      "Iteration 61, loss = 0.05703008\n",
      "Iteration 62, loss = 0.05391886\n",
      "Iteration 63, loss = 0.05824329\n",
      "Iteration 64, loss = 0.05703654\n",
      "Iteration 65, loss = 0.05403719\n",
      "Iteration 66, loss = 0.05057542\n",
      "Iteration 67, loss = 0.05163531\n",
      "Iteration 68, loss = 0.06698330\n",
      "Iteration 69, loss = 0.06683784\n",
      "Iteration 70, loss = 0.05375389\n",
      "Iteration 71, loss = 0.04948757\n",
      "Iteration 72, loss = 0.04659371\n",
      "Iteration 73, loss = 0.04934668\n",
      "Iteration 74, loss = 0.05021753\n",
      "Iteration 75, loss = 0.05171334\n",
      "Iteration 76, loss = 0.05896883\n",
      "Iteration 77, loss = 0.04766134\n",
      "Iteration 78, loss = 0.05217747\n",
      "Iteration 79, loss = 0.05277815\n",
      "Iteration 80, loss = 0.04857348\n",
      "Iteration 81, loss = 0.04773052\n",
      "Iteration 82, loss = 0.04256770\n",
      "Iteration 83, loss = 0.05225645\n",
      "Iteration 84, loss = 0.04891859\n",
      "Iteration 85, loss = 0.05329646\n",
      "Iteration 86, loss = 0.05380165\n",
      "Iteration 87, loss = 0.04978525\n",
      "Iteration 88, loss = 0.05377289\n",
      "Iteration 89, loss = 0.05659971\n",
      "Iteration 90, loss = 0.04572103\n",
      "Iteration 91, loss = 0.04952808\n",
      "Iteration 92, loss = 0.04886612\n",
      "Iteration 93, loss = 0.04665797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 96.21329211746522\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.50669175\n",
      "Iteration 2, loss = 0.20250748\n",
      "Iteration 3, loss = 0.16520433\n",
      "Iteration 4, loss = 0.15287597\n",
      "Iteration 5, loss = 0.13839551\n",
      "Iteration 6, loss = 0.13143729\n",
      "Iteration 7, loss = 0.12454228\n",
      "Iteration 8, loss = 0.11618477\n",
      "Iteration 9, loss = 0.11053223\n",
      "Iteration 10, loss = 0.10491765\n",
      "Iteration 11, loss = 0.11062859\n",
      "Iteration 12, loss = 0.10061601\n",
      "Iteration 13, loss = 0.10292346\n",
      "Iteration 14, loss = 0.09221395\n",
      "Iteration 15, loss = 0.09094241\n",
      "Iteration 16, loss = 0.09316360\n",
      "Iteration 17, loss = 0.08608513\n",
      "Iteration 18, loss = 0.08585502\n",
      "Iteration 19, loss = 0.08473700\n",
      "Iteration 20, loss = 0.08199535\n",
      "Iteration 21, loss = 0.07963665\n",
      "Iteration 22, loss = 0.07985047\n",
      "Iteration 23, loss = 0.07887163\n",
      "Iteration 24, loss = 0.07960688\n",
      "Iteration 25, loss = 0.07387251\n",
      "Iteration 26, loss = 0.07352337\n",
      "Iteration 27, loss = 0.07081730\n",
      "Iteration 28, loss = 0.07368656\n",
      "Iteration 29, loss = 0.07041075\n",
      "Iteration 30, loss = 0.07138844\n",
      "Iteration 31, loss = 0.07656032\n",
      "Iteration 32, loss = 0.07237210\n",
      "Iteration 33, loss = 0.06691286\n",
      "Iteration 34, loss = 0.06871662\n",
      "Iteration 35, loss = 0.07018536\n",
      "Iteration 36, loss = 0.06946075\n",
      "Iteration 37, loss = 0.06505138\n",
      "Iteration 38, loss = 0.06742306\n",
      "Iteration 39, loss = 0.06283863\n",
      "Iteration 40, loss = 0.06115782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41, loss = 0.06368000\n",
      "Iteration 42, loss = 0.06436388\n",
      "Iteration 43, loss = 0.06449367\n",
      "Iteration 44, loss = 0.06057242\n",
      "Iteration 45, loss = 0.06026464\n",
      "Iteration 46, loss = 0.05938903\n",
      "Iteration 47, loss = 0.06253857\n",
      "Iteration 48, loss = 0.05830020\n",
      "Iteration 49, loss = 0.06135026\n",
      "Iteration 50, loss = 0.05699039\n",
      "Iteration 51, loss = 0.05463173\n",
      "Iteration 52, loss = 0.05485185\n",
      "Iteration 53, loss = 0.05645568\n",
      "Iteration 54, loss = 0.05545418\n",
      "Iteration 55, loss = 0.07629331\n",
      "Iteration 56, loss = 0.05841932\n",
      "Iteration 57, loss = 0.05389843\n",
      "Iteration 58, loss = 0.05761762\n",
      "Iteration 59, loss = 0.05715388\n",
      "Iteration 60, loss = 0.05165050\n",
      "Iteration 61, loss = 0.05971964\n",
      "Iteration 62, loss = 0.06272465\n",
      "Iteration 63, loss = 0.05438823\n",
      "Iteration 64, loss = 0.05305028\n",
      "Iteration 65, loss = 0.04757162\n",
      "Iteration 66, loss = 0.05069938\n",
      "Iteration 67, loss = 0.05009496\n",
      "Iteration 68, loss = 0.05021030\n",
      "Iteration 69, loss = 0.05337547\n",
      "Iteration 70, loss = 0.05176444\n",
      "Iteration 71, loss = 0.04634174\n",
      "Iteration 72, loss = 0.05019837\n",
      "Iteration 73, loss = 0.04619560\n",
      "Iteration 74, loss = 0.04636524\n",
      "Iteration 75, loss = 0.05241978\n",
      "Iteration 76, loss = 0.04628233\n",
      "Iteration 77, loss = 0.04730984\n",
      "Iteration 78, loss = 0.04698074\n",
      "Iteration 79, loss = 0.04700071\n",
      "Iteration 80, loss = 0.04747651\n",
      "Iteration 81, loss = 0.04390992\n",
      "Iteration 82, loss = 0.05188504\n",
      "Iteration 83, loss = 0.05350879\n",
      "Iteration 84, loss = 0.04960393\n",
      "Iteration 85, loss = 0.04276675\n",
      "Iteration 86, loss = 0.04859959\n",
      "Iteration 87, loss = 0.04558329\n",
      "Iteration 88, loss = 0.04950284\n",
      "Iteration 89, loss = 0.05371937\n",
      "Iteration 90, loss = 0.04393552\n",
      "Iteration 91, loss = 0.04204339\n",
      "Iteration 92, loss = 0.04372532\n",
      "Iteration 93, loss = 0.04547178\n",
      "Iteration 94, loss = 0.04349155\n",
      "Iteration 95, loss = 0.05796407\n",
      "Iteration 96, loss = 0.04738300\n",
      "Iteration 97, loss = 0.05202764\n",
      "Iteration 98, loss = 0.05085160\n",
      "Iteration 99, loss = 0.04265500\n",
      "Iteration 100, loss = 0.04070960\n",
      "Iteration 101, loss = 0.03752318\n",
      "Iteration 102, loss = 0.04390670\n",
      "Iteration 103, loss = 0.04422670\n",
      "Iteration 104, loss = 0.03980361\n",
      "Iteration 105, loss = 0.03695919\n",
      "Iteration 106, loss = 0.03967782\n",
      "Iteration 107, loss = 0.03983031\n",
      "Iteration 108, loss = 0.03838575\n",
      "Iteration 109, loss = 0.04411072\n",
      "Iteration 110, loss = 0.04245815\n",
      "Iteration 111, loss = 0.04651530\n",
      "Iteration 112, loss = 0.04029724\n",
      "Iteration 113, loss = 0.03719720\n",
      "Iteration 114, loss = 0.03883674\n",
      "Iteration 115, loss = 0.03672468\n",
      "Iteration 116, loss = 0.03884796\n",
      "Iteration 117, loss = 0.04070908\n",
      "Iteration 118, loss = 0.03746581\n",
      "Iteration 119, loss = 0.03969107\n",
      "Iteration 120, loss = 0.04187683\n",
      "Iteration 121, loss = 0.04361624\n",
      "Iteration 122, loss = 0.03918179\n",
      "Iteration 123, loss = 0.04200541\n",
      "Iteration 124, loss = 0.03862547\n",
      "Iteration 125, loss = 0.03610462\n",
      "Iteration 126, loss = 0.03915231\n",
      "Iteration 127, loss = 0.03710015\n",
      "Iteration 128, loss = 0.03845455\n",
      "Iteration 129, loss = 0.03556362\n",
      "Iteration 130, loss = 0.04255120\n",
      "Iteration 131, loss = 0.03815319\n",
      "Iteration 132, loss = 0.03837410\n",
      "Iteration 133, loss = 0.03557426\n",
      "Iteration 134, loss = 0.04260982\n",
      "Iteration 135, loss = 0.03845911\n",
      "Iteration 136, loss = 0.04033916\n",
      "Iteration 137, loss = 0.03737260\n",
      "Iteration 138, loss = 0.03464919\n",
      "Iteration 139, loss = 0.03722649\n",
      "Iteration 140, loss = 0.03372648\n",
      "Iteration 141, loss = 0.03825051\n",
      "Iteration 142, loss = 0.03306001\n",
      "Iteration 143, loss = 0.04076864\n",
      "Iteration 144, loss = 0.03822161\n",
      "Iteration 145, loss = 0.03547818\n",
      "Iteration 146, loss = 0.03874464\n",
      "Iteration 147, loss = 0.03411037\n",
      "Iteration 148, loss = 0.03509899\n",
      "Iteration 149, loss = 0.03450564\n",
      "Iteration 150, loss = 0.03218318\n",
      "Iteration 151, loss = 0.03264690\n",
      "Iteration 152, loss = 0.03395498\n",
      "Iteration 153, loss = 0.03286830\n",
      "Iteration 154, loss = 0.03402184\n",
      "Iteration 155, loss = 0.03012508\n",
      "Iteration 156, loss = 0.03735277\n",
      "Iteration 157, loss = 0.03125785\n",
      "Iteration 158, loss = 0.03287123\n",
      "Iteration 159, loss = 0.03271210\n",
      "Iteration 160, loss = 0.03374467\n",
      "Iteration 161, loss = 0.03065008\n",
      "Iteration 162, loss = 0.03069537\n",
      "Iteration 163, loss = 0.03343385\n",
      "Iteration 164, loss = 0.03272641\n",
      "Iteration 165, loss = 0.03360591\n",
      "Iteration 166, loss = 0.03770640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 95.90417310664606\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 96.52241112828439 %\n",
      "Fold 1: 96.98608964451314 %\n",
      "Fold 2: 96.13601236476043 %\n",
      "Fold 3: 96.21329211746522 %\n",
      "Fold 4: 95.90417310664606 %\n",
      "Average: 96.35239567233386 %\n",
      "Iteration 1, loss = 0.44973465\n",
      "Iteration 2, loss = 0.19131034\n",
      "Iteration 3, loss = 0.16254266\n",
      "Iteration 4, loss = 0.14885141\n",
      "Iteration 5, loss = 0.13558297\n",
      "Iteration 6, loss = 0.12742419\n",
      "Iteration 7, loss = 0.12449176\n",
      "Iteration 8, loss = 0.11662749\n",
      "Iteration 9, loss = 0.11211572\n",
      "Iteration 10, loss = 0.10848823\n",
      "Iteration 11, loss = 0.10384619\n",
      "Iteration 12, loss = 0.10063247\n",
      "Iteration 13, loss = 0.09622249\n",
      "Iteration 14, loss = 0.09379762\n",
      "Iteration 15, loss = 0.09239063\n",
      "Iteration 16, loss = 0.08817886\n",
      "Iteration 17, loss = 0.08866974\n",
      "Iteration 18, loss = 0.09213572\n",
      "Iteration 19, loss = 0.08607914\n",
      "Iteration 20, loss = 0.08362493\n",
      "Iteration 21, loss = 0.08366861\n",
      "Iteration 22, loss = 0.08231734\n",
      "Iteration 23, loss = 0.08344860\n",
      "Iteration 24, loss = 0.08180772\n",
      "Iteration 25, loss = 0.07591671\n",
      "Iteration 26, loss = 0.07400447\n",
      "Iteration 27, loss = 0.07575672\n",
      "Iteration 28, loss = 0.08253327\n",
      "Iteration 29, loss = 0.07434735\n",
      "Iteration 30, loss = 0.07476574\n",
      "Iteration 31, loss = 0.07829727\n",
      "Iteration 32, loss = 0.07031234\n",
      "Iteration 33, loss = 0.07125201\n",
      "Iteration 34, loss = 0.08079523\n",
      "Iteration 35, loss = 0.06691330\n",
      "Iteration 36, loss = 0.06853406\n",
      "Iteration 37, loss = 0.07367809\n",
      "Iteration 38, loss = 0.06859822\n",
      "Iteration 39, loss = 0.06529639\n",
      "Iteration 40, loss = 0.06738439\n",
      "Iteration 41, loss = 0.06681993\n",
      "Iteration 42, loss = 0.06346308\n",
      "Iteration 43, loss = 0.06336304\n",
      "Iteration 44, loss = 0.06946581\n",
      "Iteration 45, loss = 0.06830367\n",
      "Iteration 46, loss = 0.07276963\n",
      "Iteration 47, loss = 0.06920403\n",
      "Iteration 48, loss = 0.06268512\n",
      "Iteration 49, loss = 0.07090239\n",
      "Iteration 50, loss = 0.06116107\n",
      "Iteration 51, loss = 0.06364579\n",
      "Iteration 52, loss = 0.05913118\n",
      "Iteration 53, loss = 0.06255138\n",
      "Iteration 54, loss = 0.06154346\n",
      "Iteration 55, loss = 0.06239368\n",
      "Iteration 56, loss = 0.05907836\n",
      "Iteration 57, loss = 0.05782393\n",
      "Iteration 58, loss = 0.05722564\n",
      "Iteration 59, loss = 0.06203870\n",
      "Iteration 60, loss = 0.05701956\n",
      "Iteration 61, loss = 0.06087181\n",
      "Iteration 62, loss = 0.05847829\n",
      "Iteration 63, loss = 0.06069845\n",
      "Iteration 64, loss = 0.05475085\n",
      "Iteration 65, loss = 0.05542709\n",
      "Iteration 66, loss = 0.05360078\n",
      "Iteration 67, loss = 0.05348380\n",
      "Iteration 68, loss = 0.05458012\n",
      "Iteration 69, loss = 0.05477656\n",
      "Iteration 70, loss = 0.06259370\n",
      "Iteration 71, loss = 0.06149291\n",
      "Iteration 72, loss = 0.05415731\n",
      "Iteration 73, loss = 0.05856490\n",
      "Iteration 74, loss = 0.05480656\n",
      "Iteration 75, loss = 0.05408753\n",
      "Iteration 76, loss = 0.06229599\n",
      "Iteration 77, loss = 0.05334199\n",
      "Iteration 78, loss = 0.04965292\n",
      "Iteration 79, loss = 0.05198722\n",
      "Iteration 80, loss = 0.05047528\n",
      "Iteration 81, loss = 0.06065504\n",
      "Iteration 82, loss = 0.05771788\n",
      "Iteration 83, loss = 0.05847383\n",
      "Iteration 84, loss = 0.05142048\n",
      "Iteration 85, loss = 0.04916523\n",
      "Iteration 86, loss = 0.04987332\n",
      "Iteration 87, loss = 0.05894244\n",
      "Iteration 88, loss = 0.05635123\n",
      "Iteration 89, loss = 0.05230642\n",
      "Iteration 90, loss = 0.05031986\n",
      "Iteration 91, loss = 0.05447538\n",
      "Iteration 92, loss = 0.06063665\n",
      "Iteration 93, loss = 0.04947072\n",
      "Iteration 94, loss = 0.05005404\n",
      "Iteration 95, loss = 0.04843069\n",
      "Iteration 96, loss = 0.04579622\n",
      "Iteration 97, loss = 0.05136875\n",
      "Iteration 98, loss = 0.05124266\n",
      "Iteration 99, loss = 0.04689623\n",
      "Iteration 100, loss = 0.05458592\n",
      "Iteration 101, loss = 0.04881840\n",
      "Iteration 102, loss = 0.04795605\n",
      "Iteration 103, loss = 0.04942821\n",
      "Iteration 104, loss = 0.04968270\n",
      "Iteration 105, loss = 0.04813455\n",
      "Iteration 106, loss = 0.04530537\n",
      "Iteration 107, loss = 0.04772182\n",
      "Iteration 108, loss = 0.04225020\n",
      "Iteration 109, loss = 0.04585469\n",
      "Iteration 110, loss = 0.04459011\n",
      "Iteration 111, loss = 0.04447585\n",
      "Iteration 112, loss = 0.04494617\n",
      "Iteration 113, loss = 0.04928429\n",
      "Iteration 114, loss = 0.04185952\n",
      "Iteration 115, loss = 0.04265246\n",
      "Iteration 116, loss = 0.04204566\n",
      "Iteration 117, loss = 0.04120281\n",
      "Iteration 118, loss = 0.04769783\n",
      "Iteration 119, loss = 0.04495291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 120, loss = 0.04292267\n",
      "Iteration 121, loss = 0.04305897\n",
      "Iteration 122, loss = 0.04272387\n",
      "Iteration 123, loss = 0.04150147\n",
      "Iteration 124, loss = 0.03909700\n",
      "Iteration 125, loss = 0.04716310\n",
      "Iteration 126, loss = 0.04213437\n",
      "Iteration 127, loss = 0.04833481\n",
      "Iteration 128, loss = 0.04601542\n",
      "Iteration 129, loss = 0.04079304\n",
      "Iteration 130, loss = 0.03911799\n",
      "Iteration 131, loss = 0.04463215\n",
      "Iteration 132, loss = 0.05190130\n",
      "Iteration 133, loss = 0.04381389\n",
      "Iteration 134, loss = 0.04459000\n",
      "Iteration 135, loss = 0.04155226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy:  0.9264524103831892\n",
      "F1-Score:  0.9280096793708409\n",
      "Precision:  0.918562874251497\n",
      "Recall:  0.9376528117359413\n",
      "AUC:  0.9263264058679707\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_mlp(X_Train,Y_Train.ravel())\n",
    "clf=MLPClassifier(random_state=102, max_iter=3000, verbose=True).fit(X_Train, Y_Train.ravel())\n",
    "y_pred=clf.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c921ecf",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcf31873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cbdfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_rforest(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        random_forest = RandomForestClassifier(n_estimators=100,criterion='gini',random_state=102)\n",
    "        random_forest = random_forest.fit(X_train, y_train.ravel())\n",
    "        y_pred = random_forest.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del random_forest\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53453763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 95.6723338485317\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 96.52241112828439\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 96.21329211746522\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 95.5950540958269\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 96.90880989180835\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 95.6723338485317 %\n",
      "Fold 1: 96.52241112828439 %\n",
      "Fold 2: 96.21329211746522 %\n",
      "Fold 3: 95.5950540958269 %\n",
      "Fold 4: 96.90880989180835 %\n",
      "Average: 96.18238021638331 %\n",
      "Accuracy:  0.9184177997527813\n",
      "F1-Score:  0.9201934703748489\n",
      "Precision:  0.9102870813397129\n",
      "Recall:  0.9303178484107579\n",
      "AUC:  0.9182839242053789\n"
     ]
    }
   ],
   "source": [
    "random_forest=k_fold_cv_rforest(X_Train,Y_Train.ravel())\n",
    "random_forest = RandomForestClassifier(n_estimators=100,criterion='gini',random_state=102)\n",
    "random_forest = random_forest.fit(X_Train, Y_Train.ravel())\n",
    "y_pred=random_forest.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a19f6d",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac3c2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e4857b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cumulative explained variance')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kpca = KernelPCA(kernel = 'rbf')\n",
    "kpca_transform = kpca.fit_transform(X_Train_FeatureMap)\n",
    "explained_variance = np.var(kpca_transform, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "plt.yticks([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "plt.plot(np.cumsum(explained_variance_ratio))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2cc5d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6470, 3500)\n"
     ]
    }
   ],
   "source": [
    "kpca = KernelPCA(kernel = 'rbf',n_components=3500)\n",
    "X_Train_Transformed_FeatureMap = kpca.fit_transform(X_Train_FeatureMap)\n",
    "print(X_Train_Transformed_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90088260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1618, 18432) (1618, 3500)\n"
     ]
    }
   ],
   "source": [
    "X_Test_Transformed_FeatureMap = kpca.transform(X_Test_FeatureMap)\n",
    "\n",
    "print(X_Test_FeatureMap.shape,X_Test_Transformed_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38abb860",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2c3d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 92.19474497681608\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 93.97217928902627\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 93.74034003091191\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 92.27202472952087\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 91.80834621329211\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 92.19474497681608 %\n",
      "Fold 1: 93.97217928902627 %\n",
      "Fold 2: 93.74034003091191 %\n",
      "Fold 3: 92.27202472952087 %\n",
      "Fold 4: 91.80834621329211 %\n",
      "Average: 92.79752704791345 %\n",
      "Accuracy:  0.9239802224969098\n",
      "F1-Score:  0.9259482239614689\n",
      "Precision:  0.9122182680901542\n",
      "Recall:  0.9400977995110025\n",
      "AUC:  0.9237988997555012\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_svm(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "clf=SVC(C=10,kernel='rbf',gamma=10)\n",
    "clf.fit(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=clf.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e06bfb",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad39f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 76.04327666151468\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 78.67078825347758\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 80.83462132921174\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 80.91190108191654\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 79.05718701700154\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 76.04327666151468 %\n",
      "Fold 1: 78.67078825347758 %\n",
      "Fold 2: 80.83462132921174 %\n",
      "Fold 3: 80.91190108191654 %\n",
      "Fold 4: 79.05718701700154 %\n",
      "Average: 79.10355486862441 %\n",
      "Accuracy:  0.7323856613102596\n",
      "F1-Score:  0.7361365021328458\n",
      "Precision:  0.7339003645200486\n",
      "Recall:  0.7383863080684596\n",
      "AUC:  0.7323181540342298\n"
     ]
    }
   ],
   "source": [
    "dtree=k_fold_cv_dtree(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "dtree = DecisionTreeClassifier(random_state=102)\n",
    "dtree = dtree.fit(X_Train_Transformed_FeatureMap, Y_Train_FeatureMap.ravel())\n",
    "y_pred=dtree.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f5bea",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cb9236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "[15:31:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 90.41731066460588\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "[15:32:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 90.1854714064915\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "[15:34:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 90.80370942812984\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "[15:36:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 89.41267387944359\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "[15:38:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 89.87635239567233\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 90.41731066460588 %\n",
      "Fold 1: 90.1854714064915 %\n",
      "Fold 2: 90.80370942812984 %\n",
      "Fold 3: 89.41267387944359 %\n",
      "Fold 4: 89.87635239567233 %\n",
      "Average: 90.13910355486863 %\n",
      "[15:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy:  0.9035846724351051\n",
      "F1-Score:  0.9071428571428571\n",
      "Precision:  0.8839907192575406\n",
      "Recall:  0.9315403422982885\n",
      "AUC:  0.9032701711491443\n"
     ]
    }
   ],
   "source": [
    "xg=k_fold_cv_xgb(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "xg = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=102,use_label_encoder=False)\n",
    "xg.fit(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=xg.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b372834",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99e04df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69245675\n",
      "Iteration 2, loss = 0.68733761\n",
      "Iteration 3, loss = 0.68105464\n",
      "Iteration 4, loss = 0.67195539\n",
      "Iteration 5, loss = 0.65953595\n",
      "Iteration 6, loss = 0.64402692\n",
      "Iteration 7, loss = 0.62522771\n",
      "Iteration 8, loss = 0.60419041\n",
      "Iteration 9, loss = 0.58160169\n",
      "Iteration 10, loss = 0.55698631\n",
      "Iteration 11, loss = 0.53135738\n",
      "Iteration 12, loss = 0.50576385\n",
      "Iteration 13, loss = 0.48067037\n",
      "Iteration 14, loss = 0.45522149\n",
      "Iteration 15, loss = 0.43115951\n",
      "Iteration 16, loss = 0.40855466\n",
      "Iteration 17, loss = 0.38667101\n",
      "Iteration 18, loss = 0.36659555\n",
      "Iteration 19, loss = 0.34754179\n",
      "Iteration 20, loss = 0.32998773\n",
      "Iteration 21, loss = 0.31406867\n",
      "Iteration 22, loss = 0.29849619\n",
      "Iteration 23, loss = 0.28483278\n",
      "Iteration 24, loss = 0.27200059\n",
      "Iteration 25, loss = 0.26045714\n",
      "Iteration 26, loss = 0.24934823\n",
      "Iteration 27, loss = 0.23895536\n",
      "Iteration 28, loss = 0.22976361\n",
      "Iteration 29, loss = 0.22121396\n",
      "Iteration 30, loss = 0.21290267\n",
      "Iteration 31, loss = 0.20558051\n",
      "Iteration 32, loss = 0.19828135\n",
      "Iteration 33, loss = 0.19176006\n",
      "Iteration 34, loss = 0.18566606\n",
      "Iteration 35, loss = 0.18013999\n",
      "Iteration 36, loss = 0.17458593\n",
      "Iteration 37, loss = 0.16942163\n",
      "Iteration 38, loss = 0.16470766\n",
      "Iteration 39, loss = 0.16027190\n",
      "Iteration 40, loss = 0.15596598\n",
      "Iteration 41, loss = 0.15206234\n",
      "Iteration 42, loss = 0.14819070\n",
      "Iteration 43, loss = 0.14452793\n",
      "Iteration 44, loss = 0.14106282\n",
      "Iteration 45, loss = 0.13777210\n",
      "Iteration 46, loss = 0.13480931\n",
      "Iteration 47, loss = 0.13169610\n",
      "Iteration 48, loss = 0.12878418\n",
      "Iteration 49, loss = 0.12616627\n",
      "Iteration 50, loss = 0.12349730\n",
      "Iteration 51, loss = 0.12095188\n",
      "Iteration 52, loss = 0.11854237\n",
      "Iteration 53, loss = 0.11659103\n",
      "Iteration 54, loss = 0.11413635\n",
      "Iteration 55, loss = 0.11193914\n",
      "Iteration 56, loss = 0.11001546\n",
      "Iteration 57, loss = 0.10809657\n",
      "Iteration 58, loss = 0.10595860\n",
      "Iteration 59, loss = 0.10446109\n",
      "Iteration 60, loss = 0.10233686\n",
      "Iteration 61, loss = 0.10063150\n",
      "Iteration 62, loss = 0.09907670\n",
      "Iteration 63, loss = 0.09761977\n",
      "Iteration 64, loss = 0.09625716\n",
      "Iteration 65, loss = 0.09450875\n",
      "Iteration 66, loss = 0.09302416\n",
      "Iteration 67, loss = 0.09182153\n",
      "Iteration 68, loss = 0.09031021\n",
      "Iteration 69, loss = 0.08891556\n",
      "Iteration 70, loss = 0.08773141\n",
      "Iteration 71, loss = 0.08647503\n",
      "Iteration 72, loss = 0.08522677\n",
      "Iteration 73, loss = 0.08416291\n",
      "Iteration 74, loss = 0.08298908\n",
      "Iteration 75, loss = 0.08185841\n",
      "Iteration 76, loss = 0.08078313\n",
      "Iteration 77, loss = 0.07977114\n",
      "Iteration 78, loss = 0.07878291\n",
      "Iteration 79, loss = 0.07778834\n",
      "Iteration 80, loss = 0.07692439\n",
      "Iteration 81, loss = 0.07590767\n",
      "Iteration 82, loss = 0.07499977\n",
      "Iteration 83, loss = 0.07418918\n",
      "Iteration 84, loss = 0.07353240\n",
      "Iteration 85, loss = 0.07233243\n",
      "Iteration 86, loss = 0.07163606\n",
      "Iteration 87, loss = 0.07092538\n",
      "Iteration 88, loss = 0.07006929\n",
      "Iteration 89, loss = 0.06935488\n",
      "Iteration 90, loss = 0.06857633\n",
      "Iteration 91, loss = 0.06770308\n",
      "Iteration 92, loss = 0.06703965\n",
      "Iteration 93, loss = 0.06637471\n",
      "Iteration 94, loss = 0.06564608\n",
      "Iteration 95, loss = 0.06512892\n",
      "Iteration 96, loss = 0.06432223\n",
      "Iteration 97, loss = 0.06407180\n",
      "Iteration 98, loss = 0.06316401\n",
      "Iteration 99, loss = 0.06242992\n",
      "Iteration 100, loss = 0.06197200\n",
      "Iteration 101, loss = 0.06133719\n",
      "Iteration 102, loss = 0.06077861\n",
      "Iteration 103, loss = 0.06018586\n",
      "Iteration 104, loss = 0.05980132\n",
      "Iteration 105, loss = 0.05916739\n",
      "Iteration 106, loss = 0.05863736\n",
      "Iteration 107, loss = 0.05809337\n",
      "Iteration 108, loss = 0.05788756\n",
      "Iteration 109, loss = 0.05701781\n",
      "Iteration 110, loss = 0.05661281\n",
      "Iteration 111, loss = 0.05615865\n",
      "Iteration 112, loss = 0.05576377\n",
      "Iteration 113, loss = 0.05523151\n",
      "Iteration 114, loss = 0.05494888\n",
      "Iteration 115, loss = 0.05439667\n",
      "Iteration 116, loss = 0.05406419\n",
      "Iteration 117, loss = 0.05368883\n",
      "Iteration 118, loss = 0.05306460\n",
      "Iteration 119, loss = 0.05277815\n",
      "Iteration 120, loss = 0.05242955\n",
      "Iteration 121, loss = 0.05205008\n",
      "Iteration 122, loss = 0.05191896\n",
      "Iteration 123, loss = 0.05131294\n",
      "Iteration 124, loss = 0.05080699\n",
      "Iteration 125, loss = 0.05046064\n",
      "Iteration 126, loss = 0.05026036\n",
      "Iteration 127, loss = 0.04977404\n",
      "Iteration 128, loss = 0.04952937\n",
      "Iteration 129, loss = 0.04903107\n",
      "Iteration 130, loss = 0.04878478\n",
      "Iteration 131, loss = 0.04871859\n",
      "Iteration 132, loss = 0.04826728\n",
      "Iteration 133, loss = 0.04772142\n",
      "Iteration 134, loss = 0.04751213\n",
      "Iteration 135, loss = 0.04740776\n",
      "Iteration 136, loss = 0.04706564\n",
      "Iteration 137, loss = 0.04713112\n",
      "Iteration 138, loss = 0.04645953\n",
      "Iteration 139, loss = 0.04631882\n",
      "Iteration 140, loss = 0.04570529\n",
      "Iteration 141, loss = 0.04559082\n",
      "Iteration 142, loss = 0.04605862\n",
      "Iteration 143, loss = 0.04535522\n",
      "Iteration 144, loss = 0.04477557\n",
      "Iteration 145, loss = 0.04442390\n",
      "Iteration 146, loss = 0.04445113\n",
      "Iteration 147, loss = 0.04393442\n",
      "Iteration 148, loss = 0.04403632\n",
      "Iteration 149, loss = 0.04366685\n",
      "Iteration 150, loss = 0.04349366\n",
      "Iteration 151, loss = 0.04311869\n",
      "Iteration 152, loss = 0.04277324\n",
      "Iteration 153, loss = 0.04270096\n",
      "Iteration 154, loss = 0.04266115\n",
      "Iteration 155, loss = 0.04237042\n",
      "Iteration 156, loss = 0.04276363\n",
      "Iteration 157, loss = 0.04159178\n",
      "Iteration 158, loss = 0.04151970\n",
      "Iteration 159, loss = 0.04129220\n",
      "Iteration 160, loss = 0.04113608\n",
      "Iteration 161, loss = 0.04103464\n",
      "Iteration 162, loss = 0.04074604\n",
      "Iteration 163, loss = 0.04104209\n",
      "Iteration 164, loss = 0.04064268\n",
      "Iteration 165, loss = 0.04037971\n",
      "Iteration 166, loss = 0.04031478\n",
      "Iteration 167, loss = 0.03991629\n",
      "Iteration 168, loss = 0.03976410\n",
      "Iteration 169, loss = 0.03986587\n",
      "Iteration 170, loss = 0.03932611\n",
      "Iteration 171, loss = 0.03937346\n",
      "Iteration 172, loss = 0.03905616\n",
      "Iteration 173, loss = 0.03897466\n",
      "Iteration 174, loss = 0.03871311\n",
      "Iteration 175, loss = 0.03860809\n",
      "Iteration 176, loss = 0.03847026\n",
      "Iteration 177, loss = 0.03851436\n",
      "Iteration 178, loss = 0.03831576\n",
      "Iteration 179, loss = 0.03832722\n",
      "Iteration 180, loss = 0.03776671\n",
      "Iteration 181, loss = 0.03776305\n",
      "Iteration 182, loss = 0.03785670\n",
      "Iteration 183, loss = 0.03760338\n",
      "Iteration 184, loss = 0.03731643\n",
      "Iteration 185, loss = 0.03749990\n",
      "Iteration 186, loss = 0.03723192\n",
      "Iteration 187, loss = 0.03704182\n",
      "Iteration 188, loss = 0.03666408\n",
      "Iteration 189, loss = 0.03661698\n",
      "Iteration 190, loss = 0.03655266\n",
      "Iteration 191, loss = 0.03635203\n",
      "Iteration 192, loss = 0.03634613\n",
      "Iteration 193, loss = 0.03608877\n",
      "Iteration 194, loss = 0.03596585\n",
      "Iteration 195, loss = 0.03587321\n",
      "Iteration 196, loss = 0.03578610\n",
      "Iteration 197, loss = 0.03571523\n",
      "Iteration 198, loss = 0.03559410\n",
      "Iteration 199, loss = 0.03526345\n",
      "Iteration 200, loss = 0.03531363\n",
      "Iteration 201, loss = 0.03514980\n",
      "Iteration 202, loss = 0.03511101\n",
      "Iteration 203, loss = 0.03535202\n",
      "Iteration 204, loss = 0.03500501\n",
      "Iteration 205, loss = 0.03471746\n",
      "Iteration 206, loss = 0.03467877\n",
      "Iteration 207, loss = 0.03452316\n",
      "Iteration 208, loss = 0.03441249\n",
      "Iteration 209, loss = 0.03444317\n",
      "Iteration 210, loss = 0.03420949\n",
      "Iteration 211, loss = 0.03425841\n",
      "Iteration 212, loss = 0.03384520\n",
      "Iteration 213, loss = 0.03401273\n",
      "Iteration 214, loss = 0.03387088\n",
      "Iteration 215, loss = 0.03371732\n",
      "Iteration 216, loss = 0.03389467\n",
      "Iteration 217, loss = 0.03362872\n",
      "Iteration 218, loss = 0.03346465\n",
      "Iteration 219, loss = 0.03334228\n",
      "Iteration 220, loss = 0.03334835\n",
      "Iteration 221, loss = 0.03307046\n",
      "Iteration 222, loss = 0.03311773\n",
      "Iteration 223, loss = 0.03294315\n",
      "Iteration 224, loss = 0.03310553\n",
      "Iteration 225, loss = 0.03311542\n",
      "Iteration 226, loss = 0.03284290\n",
      "Iteration 227, loss = 0.03278822\n",
      "Iteration 228, loss = 0.03264720\n",
      "Iteration 229, loss = 0.03256009\n",
      "Iteration 230, loss = 0.03230899\n",
      "Iteration 231, loss = 0.03241803\n",
      "Iteration 232, loss = 0.03242050\n",
      "Iteration 233, loss = 0.03212426\n",
      "Iteration 234, loss = 0.03207748\n",
      "Iteration 235, loss = 0.03197020\n",
      "Iteration 236, loss = 0.03219604\n",
      "Iteration 237, loss = 0.03177085\n",
      "Iteration 238, loss = 0.03193434\n",
      "Iteration 239, loss = 0.03186048\n",
      "Iteration 240, loss = 0.03157706\n",
      "Iteration 241, loss = 0.03167308\n",
      "Iteration 242, loss = 0.03159128\n",
      "Iteration 243, loss = 0.03148995\n",
      "Iteration 244, loss = 0.03142703\n",
      "Iteration 245, loss = 0.03109630\n",
      "Iteration 246, loss = 0.03101097\n",
      "Iteration 247, loss = 0.03138029\n",
      "Iteration 248, loss = 0.03145271\n",
      "Iteration 249, loss = 0.03124591\n",
      "Iteration 250, loss = 0.03083172\n",
      "Iteration 251, loss = 0.03078361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.03049256\n",
      "Iteration 253, loss = 0.03092535\n",
      "Iteration 254, loss = 0.03056373\n",
      "Iteration 255, loss = 0.03055983\n",
      "Iteration 256, loss = 0.03054875\n",
      "Iteration 257, loss = 0.03042846\n",
      "Iteration 258, loss = 0.03062706\n",
      "Iteration 259, loss = 0.03014350\n",
      "Iteration 260, loss = 0.03021395\n",
      "Iteration 261, loss = 0.03029304\n",
      "Iteration 262, loss = 0.03014770\n",
      "Iteration 263, loss = 0.03007826\n",
      "Iteration 264, loss = 0.02981735\n",
      "Iteration 265, loss = 0.03066866\n",
      "Iteration 266, loss = 0.03045059\n",
      "Iteration 267, loss = 0.03020172\n",
      "Iteration 268, loss = 0.02988657\n",
      "Iteration 269, loss = 0.02992207\n",
      "Iteration 270, loss = 0.02958670\n",
      "Iteration 271, loss = 0.02941852\n",
      "Iteration 272, loss = 0.02942995\n",
      "Iteration 273, loss = 0.02944803\n",
      "Iteration 274, loss = 0.02925863\n",
      "Iteration 275, loss = 0.02944256\n",
      "Iteration 276, loss = 0.02921638\n",
      "Iteration 277, loss = 0.02920217\n",
      "Iteration 278, loss = 0.02903720\n",
      "Iteration 279, loss = 0.02898636\n",
      "Iteration 280, loss = 0.02924372\n",
      "Iteration 281, loss = 0.02896741\n",
      "Iteration 282, loss = 0.02880700\n",
      "Iteration 283, loss = 0.02877133\n",
      "Iteration 284, loss = 0.02895209\n",
      "Iteration 285, loss = 0.02868654\n",
      "Iteration 286, loss = 0.02900051\n",
      "Iteration 287, loss = 0.02874030\n",
      "Iteration 288, loss = 0.02857559\n",
      "Iteration 289, loss = 0.02846034\n",
      "Iteration 290, loss = 0.02831146\n",
      "Iteration 291, loss = 0.02823897\n",
      "Iteration 292, loss = 0.02855661\n",
      "Iteration 293, loss = 0.02819762\n",
      "Iteration 294, loss = 0.02820111\n",
      "Iteration 295, loss = 0.02818498\n",
      "Iteration 296, loss = 0.02802577\n",
      "Iteration 297, loss = 0.02800210\n",
      "Iteration 298, loss = 0.02813261\n",
      "Iteration 299, loss = 0.02802434\n",
      "Iteration 300, loss = 0.02798891\n",
      "Iteration 301, loss = 0.02768024\n",
      "Iteration 302, loss = 0.02769012\n",
      "Iteration 303, loss = 0.02759099\n",
      "Iteration 304, loss = 0.02767767\n",
      "Iteration 305, loss = 0.02790014\n",
      "Iteration 306, loss = 0.02792622\n",
      "Iteration 307, loss = 0.02757163\n",
      "Iteration 308, loss = 0.02761932\n",
      "Iteration 309, loss = 0.02732135\n",
      "Iteration 310, loss = 0.02733899\n",
      "Iteration 311, loss = 0.02724389\n",
      "Iteration 312, loss = 0.02745633\n",
      "Iteration 313, loss = 0.02730664\n",
      "Iteration 314, loss = 0.02713652\n",
      "Iteration 315, loss = 0.02712803\n",
      "Iteration 316, loss = 0.02715325\n",
      "Iteration 317, loss = 0.02711775\n",
      "Iteration 318, loss = 0.02712977\n",
      "Iteration 319, loss = 0.02696519\n",
      "Iteration 320, loss = 0.02694963\n",
      "Iteration 321, loss = 0.02680508\n",
      "Iteration 322, loss = 0.02697698\n",
      "Iteration 323, loss = 0.02707746\n",
      "Iteration 324, loss = 0.02657444\n",
      "Iteration 325, loss = 0.02682053\n",
      "Iteration 326, loss = 0.02715787\n",
      "Iteration 327, loss = 0.02678407\n",
      "Iteration 328, loss = 0.02669015\n",
      "Iteration 329, loss = 0.02681574\n",
      "Iteration 330, loss = 0.02647080\n",
      "Iteration 331, loss = 0.02641070\n",
      "Iteration 332, loss = 0.02640302\n",
      "Iteration 333, loss = 0.02662897\n",
      "Iteration 334, loss = 0.02660178\n",
      "Iteration 335, loss = 0.02614524\n",
      "Iteration 336, loss = 0.02668581\n",
      "Iteration 337, loss = 0.02665193\n",
      "Iteration 338, loss = 0.02648905\n",
      "Iteration 339, loss = 0.02623627\n",
      "Iteration 340, loss = 0.02596501\n",
      "Iteration 341, loss = 0.02639373\n",
      "Iteration 342, loss = 0.02674967\n",
      "Iteration 343, loss = 0.02614625\n",
      "Iteration 344, loss = 0.02615112\n",
      "Iteration 345, loss = 0.02596823\n",
      "Iteration 346, loss = 0.02613190\n",
      "Iteration 347, loss = 0.02598170\n",
      "Iteration 348, loss = 0.02618392\n",
      "Iteration 349, loss = 0.02582272\n",
      "Iteration 350, loss = 0.02566870\n",
      "Iteration 351, loss = 0.02637321\n",
      "Iteration 352, loss = 0.02573337\n",
      "Iteration 353, loss = 0.02550469\n",
      "Iteration 354, loss = 0.02571111\n",
      "Iteration 355, loss = 0.02609654\n",
      "Iteration 356, loss = 0.02565276\n",
      "Iteration 357, loss = 0.02560293\n",
      "Iteration 358, loss = 0.02547873\n",
      "Iteration 359, loss = 0.02535528\n",
      "Iteration 360, loss = 0.02585482\n",
      "Iteration 361, loss = 0.02526724\n",
      "Iteration 362, loss = 0.02547361\n",
      "Iteration 363, loss = 0.02546156\n",
      "Iteration 364, loss = 0.02582567\n",
      "Iteration 365, loss = 0.02555877\n",
      "Iteration 366, loss = 0.02537228\n",
      "Iteration 367, loss = 0.02575003\n",
      "Iteration 368, loss = 0.02518528\n",
      "Iteration 369, loss = 0.02546858\n",
      "Iteration 370, loss = 0.02522891\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 90.57187017001546\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69260448\n",
      "Iteration 2, loss = 0.68748220\n",
      "Iteration 3, loss = 0.68169109\n",
      "Iteration 4, loss = 0.67346344\n",
      "Iteration 5, loss = 0.66233893\n",
      "Iteration 6, loss = 0.64828350\n",
      "Iteration 7, loss = 0.63110927\n",
      "Iteration 8, loss = 0.61197458\n",
      "Iteration 9, loss = 0.59016875\n",
      "Iteration 10, loss = 0.56720636\n",
      "Iteration 11, loss = 0.54341369\n",
      "Iteration 12, loss = 0.51889070\n",
      "Iteration 13, loss = 0.49413379\n",
      "Iteration 14, loss = 0.46979284\n",
      "Iteration 15, loss = 0.44654587\n",
      "Iteration 16, loss = 0.42385069\n",
      "Iteration 17, loss = 0.40253375\n",
      "Iteration 18, loss = 0.38265382\n",
      "Iteration 19, loss = 0.36326111\n",
      "Iteration 20, loss = 0.34556434\n",
      "Iteration 21, loss = 0.32896764\n",
      "Iteration 22, loss = 0.31364286\n",
      "Iteration 23, loss = 0.29934312\n",
      "Iteration 24, loss = 0.28637501\n",
      "Iteration 25, loss = 0.27412913\n",
      "Iteration 26, loss = 0.26265544\n",
      "Iteration 27, loss = 0.25245626\n",
      "Iteration 28, loss = 0.24253247\n",
      "Iteration 29, loss = 0.23356609\n",
      "Iteration 30, loss = 0.22501059\n",
      "Iteration 31, loss = 0.21812813\n",
      "Iteration 32, loss = 0.21041886\n",
      "Iteration 33, loss = 0.20305292\n",
      "Iteration 34, loss = 0.19695738\n",
      "Iteration 35, loss = 0.19100295\n",
      "Iteration 36, loss = 0.18509505\n",
      "Iteration 37, loss = 0.18004724\n",
      "Iteration 38, loss = 0.17508761\n",
      "Iteration 39, loss = 0.17079807\n",
      "Iteration 40, loss = 0.16577295\n",
      "Iteration 41, loss = 0.16226260\n",
      "Iteration 42, loss = 0.15763755\n",
      "Iteration 43, loss = 0.15384372\n",
      "Iteration 44, loss = 0.15032652\n",
      "Iteration 45, loss = 0.14654861\n",
      "Iteration 46, loss = 0.14346097\n",
      "Iteration 47, loss = 0.14026894\n",
      "Iteration 48, loss = 0.13717435\n",
      "Iteration 49, loss = 0.13484196\n",
      "Iteration 50, loss = 0.13157589\n",
      "Iteration 51, loss = 0.12910251\n",
      "Iteration 52, loss = 0.12676657\n",
      "Iteration 53, loss = 0.12400125\n",
      "Iteration 54, loss = 0.12178392\n",
      "Iteration 55, loss = 0.11951408\n",
      "Iteration 56, loss = 0.11754954\n",
      "Iteration 57, loss = 0.11554944\n",
      "Iteration 58, loss = 0.11330328\n",
      "Iteration 59, loss = 0.11161437\n",
      "Iteration 60, loss = 0.10948251\n",
      "Iteration 61, loss = 0.10776748\n",
      "Iteration 62, loss = 0.10597002\n",
      "Iteration 63, loss = 0.10426527\n",
      "Iteration 64, loss = 0.10285759\n",
      "Iteration 65, loss = 0.10104728\n",
      "Iteration 66, loss = 0.09969893\n",
      "Iteration 67, loss = 0.09799595\n",
      "Iteration 68, loss = 0.09664454\n",
      "Iteration 69, loss = 0.09533147\n",
      "Iteration 70, loss = 0.09397702\n",
      "Iteration 71, loss = 0.09270041\n",
      "Iteration 72, loss = 0.09117241\n",
      "Iteration 73, loss = 0.08999721\n",
      "Iteration 74, loss = 0.08892615\n",
      "Iteration 75, loss = 0.08795756\n",
      "Iteration 76, loss = 0.08691506\n",
      "Iteration 77, loss = 0.08557661\n",
      "Iteration 78, loss = 0.08472277\n",
      "Iteration 79, loss = 0.08351899\n",
      "Iteration 80, loss = 0.08231757\n",
      "Iteration 81, loss = 0.08132431\n",
      "Iteration 82, loss = 0.08033426\n",
      "Iteration 83, loss = 0.07942213\n",
      "Iteration 84, loss = 0.07851887\n",
      "Iteration 85, loss = 0.07750050\n",
      "Iteration 86, loss = 0.07689782\n",
      "Iteration 87, loss = 0.07599079\n",
      "Iteration 88, loss = 0.07551779\n",
      "Iteration 89, loss = 0.07429977\n",
      "Iteration 90, loss = 0.07350565\n",
      "Iteration 91, loss = 0.07264735\n",
      "Iteration 92, loss = 0.07202527\n",
      "Iteration 93, loss = 0.07118723\n",
      "Iteration 94, loss = 0.07021874\n",
      "Iteration 95, loss = 0.07034192\n",
      "Iteration 96, loss = 0.06927294\n",
      "Iteration 97, loss = 0.06872993\n",
      "Iteration 98, loss = 0.06798250\n",
      "Iteration 99, loss = 0.06705888\n",
      "Iteration 100, loss = 0.06641951\n",
      "Iteration 101, loss = 0.06566846\n",
      "Iteration 102, loss = 0.06552443\n",
      "Iteration 103, loss = 0.06462153\n",
      "Iteration 104, loss = 0.06407655\n",
      "Iteration 105, loss = 0.06352847\n",
      "Iteration 106, loss = 0.06293259\n",
      "Iteration 107, loss = 0.06254484\n",
      "Iteration 108, loss = 0.06192973\n",
      "Iteration 109, loss = 0.06141985\n",
      "Iteration 110, loss = 0.06102374\n",
      "Iteration 111, loss = 0.06042518\n",
      "Iteration 112, loss = 0.05989030\n",
      "Iteration 113, loss = 0.05956798\n",
      "Iteration 114, loss = 0.05918869\n",
      "Iteration 115, loss = 0.05855620\n",
      "Iteration 116, loss = 0.05815251\n",
      "Iteration 117, loss = 0.05777392\n",
      "Iteration 118, loss = 0.05750577\n",
      "Iteration 119, loss = 0.05668786\n",
      "Iteration 120, loss = 0.05637453\n",
      "Iteration 121, loss = 0.05606973\n",
      "Iteration 122, loss = 0.05557207\n",
      "Iteration 123, loss = 0.05525150\n",
      "Iteration 124, loss = 0.05487722\n",
      "Iteration 125, loss = 0.05447299\n",
      "Iteration 126, loss = 0.05431009\n",
      "Iteration 127, loss = 0.05380547\n",
      "Iteration 128, loss = 0.05346262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 129, loss = 0.05301476\n",
      "Iteration 130, loss = 0.05286122\n",
      "Iteration 131, loss = 0.05241198\n",
      "Iteration 132, loss = 0.05214983\n",
      "Iteration 133, loss = 0.05187056\n",
      "Iteration 134, loss = 0.05169402\n",
      "Iteration 135, loss = 0.05101754\n",
      "Iteration 136, loss = 0.05074442\n",
      "Iteration 137, loss = 0.05063462\n",
      "Iteration 138, loss = 0.05060660\n",
      "Iteration 139, loss = 0.04996894\n",
      "Iteration 140, loss = 0.04973051\n",
      "Iteration 141, loss = 0.04955975\n",
      "Iteration 142, loss = 0.04896788\n",
      "Iteration 143, loss = 0.04884069\n",
      "Iteration 144, loss = 0.04845407\n",
      "Iteration 145, loss = 0.04820047\n",
      "Iteration 146, loss = 0.04793596\n",
      "Iteration 147, loss = 0.04788225\n",
      "Iteration 148, loss = 0.04748541\n",
      "Iteration 149, loss = 0.04727335\n",
      "Iteration 150, loss = 0.04694843\n",
      "Iteration 151, loss = 0.04673062\n",
      "Iteration 152, loss = 0.04645468\n",
      "Iteration 153, loss = 0.04634089\n",
      "Iteration 154, loss = 0.04614454\n",
      "Iteration 155, loss = 0.04604369\n",
      "Iteration 156, loss = 0.04549570\n",
      "Iteration 157, loss = 0.04550576\n",
      "Iteration 158, loss = 0.04520877\n",
      "Iteration 159, loss = 0.04541860\n",
      "Iteration 160, loss = 0.04464938\n",
      "Iteration 161, loss = 0.04455262\n",
      "Iteration 162, loss = 0.04470341\n",
      "Iteration 163, loss = 0.04429089\n",
      "Iteration 164, loss = 0.04397406\n",
      "Iteration 165, loss = 0.04392536\n",
      "Iteration 166, loss = 0.04367085\n",
      "Iteration 167, loss = 0.04372652\n",
      "Iteration 168, loss = 0.04314131\n",
      "Iteration 169, loss = 0.04314175\n",
      "Iteration 170, loss = 0.04277253\n",
      "Iteration 171, loss = 0.04260634\n",
      "Iteration 172, loss = 0.04238308\n",
      "Iteration 173, loss = 0.04255202\n",
      "Iteration 174, loss = 0.04215113\n",
      "Iteration 175, loss = 0.04194198\n",
      "Iteration 176, loss = 0.04180778\n",
      "Iteration 177, loss = 0.04181640\n",
      "Iteration 178, loss = 0.04143801\n",
      "Iteration 179, loss = 0.04130279\n",
      "Iteration 180, loss = 0.04107450\n",
      "Iteration 181, loss = 0.04099327\n",
      "Iteration 182, loss = 0.04075582\n",
      "Iteration 183, loss = 0.04073030\n",
      "Iteration 184, loss = 0.04047578\n",
      "Iteration 185, loss = 0.04053418\n",
      "Iteration 186, loss = 0.04034622\n",
      "Iteration 187, loss = 0.04031430\n",
      "Iteration 188, loss = 0.03994840\n",
      "Iteration 189, loss = 0.04026633\n",
      "Iteration 190, loss = 0.03985517\n",
      "Iteration 191, loss = 0.03962156\n",
      "Iteration 192, loss = 0.03954064\n",
      "Iteration 193, loss = 0.03933047\n",
      "Iteration 194, loss = 0.03914615\n",
      "Iteration 195, loss = 0.03899323\n",
      "Iteration 196, loss = 0.03893430\n",
      "Iteration 197, loss = 0.03905182\n",
      "Iteration 198, loss = 0.03886396\n",
      "Iteration 199, loss = 0.03849659\n",
      "Iteration 200, loss = 0.03857700\n",
      "Iteration 201, loss = 0.03845955\n",
      "Iteration 202, loss = 0.03832865\n",
      "Iteration 203, loss = 0.03838758\n",
      "Iteration 204, loss = 0.03801432\n",
      "Iteration 205, loss = 0.03781675\n",
      "Iteration 206, loss = 0.03788346\n",
      "Iteration 207, loss = 0.03767743\n",
      "Iteration 208, loss = 0.03748123\n",
      "Iteration 209, loss = 0.03737358\n",
      "Iteration 210, loss = 0.03745961\n",
      "Iteration 211, loss = 0.03730745\n",
      "Iteration 212, loss = 0.03704534\n",
      "Iteration 213, loss = 0.03696078\n",
      "Iteration 214, loss = 0.03680975\n",
      "Iteration 215, loss = 0.03687480\n",
      "Iteration 216, loss = 0.03664351\n",
      "Iteration 217, loss = 0.03692235\n",
      "Iteration 218, loss = 0.03644970\n",
      "Iteration 219, loss = 0.03641264\n",
      "Iteration 220, loss = 0.03642160\n",
      "Iteration 221, loss = 0.03628385\n",
      "Iteration 222, loss = 0.03623567\n",
      "Iteration 223, loss = 0.03596439\n",
      "Iteration 224, loss = 0.03642629\n",
      "Iteration 225, loss = 0.03612297\n",
      "Iteration 226, loss = 0.03579346\n",
      "Iteration 227, loss = 0.03585537\n",
      "Iteration 228, loss = 0.03547749\n",
      "Iteration 229, loss = 0.03552991\n",
      "Iteration 230, loss = 0.03529533\n",
      "Iteration 231, loss = 0.03533037\n",
      "Iteration 232, loss = 0.03519865\n",
      "Iteration 233, loss = 0.03532116\n",
      "Iteration 234, loss = 0.03505500\n",
      "Iteration 235, loss = 0.03521623\n",
      "Iteration 236, loss = 0.03485413\n",
      "Iteration 237, loss = 0.03473802\n",
      "Iteration 238, loss = 0.03458211\n",
      "Iteration 239, loss = 0.03452405\n",
      "Iteration 240, loss = 0.03450360\n",
      "Iteration 241, loss = 0.03447795\n",
      "Iteration 242, loss = 0.03438894\n",
      "Iteration 243, loss = 0.03446455\n",
      "Iteration 244, loss = 0.03423451\n",
      "Iteration 245, loss = 0.03414707\n",
      "Iteration 246, loss = 0.03431305\n",
      "Iteration 247, loss = 0.03410765\n",
      "Iteration 248, loss = 0.03402013\n",
      "Iteration 249, loss = 0.03365347\n",
      "Iteration 250, loss = 0.03415654\n",
      "Iteration 251, loss = 0.03370014\n",
      "Iteration 252, loss = 0.03366360\n",
      "Iteration 253, loss = 0.03394435\n",
      "Iteration 254, loss = 0.03343438\n",
      "Iteration 255, loss = 0.03332353\n",
      "Iteration 256, loss = 0.03360114\n",
      "Iteration 257, loss = 0.03326073\n",
      "Iteration 258, loss = 0.03373297\n",
      "Iteration 259, loss = 0.03319668\n",
      "Iteration 260, loss = 0.03343471\n",
      "Iteration 261, loss = 0.03331146\n",
      "Iteration 262, loss = 0.03305609\n",
      "Iteration 263, loss = 0.03283796\n",
      "Iteration 264, loss = 0.03299158\n",
      "Iteration 265, loss = 0.03290743\n",
      "Iteration 266, loss = 0.03278614\n",
      "Iteration 267, loss = 0.03290914\n",
      "Iteration 268, loss = 0.03273294\n",
      "Iteration 269, loss = 0.03277538\n",
      "Iteration 270, loss = 0.03224208\n",
      "Iteration 271, loss = 0.03266777\n",
      "Iteration 272, loss = 0.03235398\n",
      "Iteration 273, loss = 0.03252191\n",
      "Iteration 274, loss = 0.03243142\n",
      "Iteration 275, loss = 0.03181045\n",
      "Iteration 276, loss = 0.03233369\n",
      "Iteration 277, loss = 0.03206488\n",
      "Iteration 278, loss = 0.03177630\n",
      "Iteration 279, loss = 0.03246711\n",
      "Iteration 280, loss = 0.03274868\n",
      "Iteration 281, loss = 0.03173142\n",
      "Iteration 282, loss = 0.03160186\n",
      "Iteration 283, loss = 0.03151640\n",
      "Iteration 284, loss = 0.03185392\n",
      "Iteration 285, loss = 0.03176394\n",
      "Iteration 286, loss = 0.03132920\n",
      "Iteration 287, loss = 0.03122396\n",
      "Iteration 288, loss = 0.03130916\n",
      "Iteration 289, loss = 0.03115001\n",
      "Iteration 290, loss = 0.03121238\n",
      "Iteration 291, loss = 0.03134393\n",
      "Iteration 292, loss = 0.03111873\n",
      "Iteration 293, loss = 0.03112706\n",
      "Iteration 294, loss = 0.03164751\n",
      "Iteration 295, loss = 0.03124108\n",
      "Iteration 296, loss = 0.03095383\n",
      "Iteration 297, loss = 0.03090188\n",
      "Iteration 298, loss = 0.03100646\n",
      "Iteration 299, loss = 0.03084368\n",
      "Iteration 300, loss = 0.03082617\n",
      "Iteration 301, loss = 0.03078834\n",
      "Iteration 302, loss = 0.03073850\n",
      "Iteration 303, loss = 0.03041800\n",
      "Iteration 304, loss = 0.03127717\n",
      "Iteration 305, loss = 0.03076887\n",
      "Iteration 306, loss = 0.03095349\n",
      "Iteration 307, loss = 0.03089877\n",
      "Iteration 308, loss = 0.03036349\n",
      "Iteration 309, loss = 0.03039409\n",
      "Iteration 310, loss = 0.03046028\n",
      "Iteration 311, loss = 0.03021631\n",
      "Iteration 312, loss = 0.03044114\n",
      "Iteration 313, loss = 0.03001254\n",
      "Iteration 314, loss = 0.03009582\n",
      "Iteration 315, loss = 0.03004478\n",
      "Iteration 316, loss = 0.02993709\n",
      "Iteration 317, loss = 0.03014328\n",
      "Iteration 318, loss = 0.03001005\n",
      "Iteration 319, loss = 0.02984849\n",
      "Iteration 320, loss = 0.02986788\n",
      "Iteration 321, loss = 0.03005060\n",
      "Iteration 322, loss = 0.03027312\n",
      "Iteration 323, loss = 0.02960761\n",
      "Iteration 324, loss = 0.02947248\n",
      "Iteration 325, loss = 0.02990360\n",
      "Iteration 326, loss = 0.02971109\n",
      "Iteration 327, loss = 0.02957283\n",
      "Iteration 328, loss = 0.02976763\n",
      "Iteration 329, loss = 0.02956025\n",
      "Iteration 330, loss = 0.02939507\n",
      "Iteration 331, loss = 0.02926302\n",
      "Iteration 332, loss = 0.02954122\n",
      "Iteration 333, loss = 0.02928752\n",
      "Iteration 334, loss = 0.02933997\n",
      "Iteration 335, loss = 0.02924748\n",
      "Iteration 336, loss = 0.02918608\n",
      "Iteration 337, loss = 0.02906482\n",
      "Iteration 338, loss = 0.02888848\n",
      "Iteration 339, loss = 0.02898563\n",
      "Iteration 340, loss = 0.02926696\n",
      "Iteration 341, loss = 0.02861705\n",
      "Iteration 342, loss = 0.02924847\n",
      "Iteration 343, loss = 0.02872041\n",
      "Iteration 344, loss = 0.02887641\n",
      "Iteration 345, loss = 0.02864095\n",
      "Iteration 346, loss = 0.02859067\n",
      "Iteration 347, loss = 0.02877022\n",
      "Iteration 348, loss = 0.02862500\n",
      "Iteration 349, loss = 0.02885844\n",
      "Iteration 350, loss = 0.02864544\n",
      "Iteration 351, loss = 0.02861440\n",
      "Iteration 352, loss = 0.02857142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 92.89026275115918\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69225273\n",
      "Iteration 2, loss = 0.68773990\n",
      "Iteration 3, loss = 0.68192726\n",
      "Iteration 4, loss = 0.67351315\n",
      "Iteration 5, loss = 0.66238817\n",
      "Iteration 6, loss = 0.64836252\n",
      "Iteration 7, loss = 0.63167996\n",
      "Iteration 8, loss = 0.61247267\n",
      "Iteration 9, loss = 0.59125268\n",
      "Iteration 10, loss = 0.56847775\n",
      "Iteration 11, loss = 0.54418801\n",
      "Iteration 12, loss = 0.51949529\n",
      "Iteration 13, loss = 0.49474193\n",
      "Iteration 14, loss = 0.47000921\n",
      "Iteration 15, loss = 0.44637991\n",
      "Iteration 16, loss = 0.42389916\n",
      "Iteration 17, loss = 0.40138502\n",
      "Iteration 18, loss = 0.38076798\n",
      "Iteration 19, loss = 0.36144120\n",
      "Iteration 20, loss = 0.34348743\n",
      "Iteration 21, loss = 0.32682335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.31106273\n",
      "Iteration 23, loss = 0.29657821\n",
      "Iteration 24, loss = 0.28329263\n",
      "Iteration 25, loss = 0.27089391\n",
      "Iteration 26, loss = 0.25949317\n",
      "Iteration 27, loss = 0.24898009\n",
      "Iteration 28, loss = 0.23921400\n",
      "Iteration 29, loss = 0.23015413\n",
      "Iteration 30, loss = 0.22212065\n",
      "Iteration 31, loss = 0.21382922\n",
      "Iteration 32, loss = 0.20651571\n",
      "Iteration 33, loss = 0.19951409\n",
      "Iteration 34, loss = 0.19306137\n",
      "Iteration 35, loss = 0.18722023\n",
      "Iteration 36, loss = 0.18209224\n",
      "Iteration 37, loss = 0.17614903\n",
      "Iteration 38, loss = 0.17157380\n",
      "Iteration 39, loss = 0.16699396\n",
      "Iteration 40, loss = 0.16300323\n",
      "Iteration 41, loss = 0.15802809\n",
      "Iteration 42, loss = 0.15419290\n",
      "Iteration 43, loss = 0.15036347\n",
      "Iteration 44, loss = 0.14681558\n",
      "Iteration 45, loss = 0.14304779\n",
      "Iteration 46, loss = 0.13993012\n",
      "Iteration 47, loss = 0.13674163\n",
      "Iteration 48, loss = 0.13390740\n",
      "Iteration 49, loss = 0.13126146\n",
      "Iteration 50, loss = 0.12817093\n",
      "Iteration 51, loss = 0.12568422\n",
      "Iteration 52, loss = 0.12293308\n",
      "Iteration 53, loss = 0.12062038\n",
      "Iteration 54, loss = 0.11828656\n",
      "Iteration 55, loss = 0.11601918\n",
      "Iteration 56, loss = 0.11385000\n",
      "Iteration 57, loss = 0.11176087\n",
      "Iteration 58, loss = 0.10997064\n",
      "Iteration 59, loss = 0.10779657\n",
      "Iteration 60, loss = 0.10599205\n",
      "Iteration 61, loss = 0.10424793\n",
      "Iteration 62, loss = 0.10244196\n",
      "Iteration 63, loss = 0.10082934\n",
      "Iteration 64, loss = 0.09907284\n",
      "Iteration 65, loss = 0.09769024\n",
      "Iteration 66, loss = 0.09704277\n",
      "Iteration 67, loss = 0.09464964\n",
      "Iteration 68, loss = 0.09319434\n",
      "Iteration 69, loss = 0.09176381\n",
      "Iteration 70, loss = 0.09038290\n",
      "Iteration 71, loss = 0.08926770\n",
      "Iteration 72, loss = 0.08793871\n",
      "Iteration 73, loss = 0.08692889\n",
      "Iteration 74, loss = 0.08553323\n",
      "Iteration 75, loss = 0.08424530\n",
      "Iteration 76, loss = 0.08308503\n",
      "Iteration 77, loss = 0.08230709\n",
      "Iteration 78, loss = 0.08101631\n",
      "Iteration 79, loss = 0.07989426\n",
      "Iteration 80, loss = 0.07921137\n",
      "Iteration 81, loss = 0.07800042\n",
      "Iteration 82, loss = 0.07728216\n",
      "Iteration 83, loss = 0.07687349\n",
      "Iteration 84, loss = 0.07548722\n",
      "Iteration 85, loss = 0.07424885\n",
      "Iteration 86, loss = 0.07357032\n",
      "Iteration 87, loss = 0.07252232\n",
      "Iteration 88, loss = 0.07185248\n",
      "Iteration 89, loss = 0.07126502\n",
      "Iteration 90, loss = 0.07014999\n",
      "Iteration 91, loss = 0.06941856\n",
      "Iteration 92, loss = 0.06896998\n",
      "Iteration 93, loss = 0.06797208\n",
      "Iteration 94, loss = 0.06717009\n",
      "Iteration 95, loss = 0.06680932\n",
      "Iteration 96, loss = 0.06581114\n",
      "Iteration 97, loss = 0.06527322\n",
      "Iteration 98, loss = 0.06457659\n",
      "Iteration 99, loss = 0.06403419\n",
      "Iteration 100, loss = 0.06342521\n",
      "Iteration 101, loss = 0.06270075\n",
      "Iteration 102, loss = 0.06213188\n",
      "Iteration 103, loss = 0.06142680\n",
      "Iteration 104, loss = 0.06093585\n",
      "Iteration 105, loss = 0.06049553\n",
      "Iteration 106, loss = 0.05982582\n",
      "Iteration 107, loss = 0.05924178\n",
      "Iteration 108, loss = 0.05887086\n",
      "Iteration 109, loss = 0.05831061\n",
      "Iteration 110, loss = 0.05772187\n",
      "Iteration 111, loss = 0.05739584\n",
      "Iteration 112, loss = 0.05683820\n",
      "Iteration 113, loss = 0.05640502\n",
      "Iteration 114, loss = 0.05594826\n",
      "Iteration 115, loss = 0.05545461\n",
      "Iteration 116, loss = 0.05500334\n",
      "Iteration 117, loss = 0.05464198\n",
      "Iteration 118, loss = 0.05418586\n",
      "Iteration 119, loss = 0.05378254\n",
      "Iteration 120, loss = 0.05384881\n",
      "Iteration 121, loss = 0.05298722\n",
      "Iteration 122, loss = 0.05248079\n",
      "Iteration 123, loss = 0.05239734\n",
      "Iteration 124, loss = 0.05180162\n",
      "Iteration 125, loss = 0.05153079\n",
      "Iteration 126, loss = 0.05139592\n",
      "Iteration 127, loss = 0.05063331\n",
      "Iteration 128, loss = 0.05032200\n",
      "Iteration 129, loss = 0.05019620\n",
      "Iteration 130, loss = 0.04967512\n",
      "Iteration 131, loss = 0.04941248\n",
      "Iteration 132, loss = 0.04911842\n",
      "Iteration 133, loss = 0.04898129\n",
      "Iteration 134, loss = 0.04839069\n",
      "Iteration 135, loss = 0.04849036\n",
      "Iteration 136, loss = 0.04789222\n",
      "Iteration 137, loss = 0.04762443\n",
      "Iteration 138, loss = 0.04720868\n",
      "Iteration 139, loss = 0.04740583\n",
      "Iteration 140, loss = 0.04686618\n",
      "Iteration 141, loss = 0.04634129\n",
      "Iteration 142, loss = 0.04626410\n",
      "Iteration 143, loss = 0.04615203\n",
      "Iteration 144, loss = 0.04551682\n",
      "Iteration 145, loss = 0.04545836\n",
      "Iteration 146, loss = 0.04520896\n",
      "Iteration 147, loss = 0.04503564\n",
      "Iteration 148, loss = 0.04477185\n",
      "Iteration 149, loss = 0.04444460\n",
      "Iteration 150, loss = 0.04417222\n",
      "Iteration 151, loss = 0.04390430\n",
      "Iteration 152, loss = 0.04366358\n",
      "Iteration 153, loss = 0.04362986\n",
      "Iteration 154, loss = 0.04351842\n",
      "Iteration 155, loss = 0.04301601\n",
      "Iteration 156, loss = 0.04310700\n",
      "Iteration 157, loss = 0.04262234\n",
      "Iteration 158, loss = 0.04258544\n",
      "Iteration 159, loss = 0.04259400\n",
      "Iteration 160, loss = 0.04189461\n",
      "Iteration 161, loss = 0.04226176\n",
      "Iteration 162, loss = 0.04173514\n",
      "Iteration 163, loss = 0.04173757\n",
      "Iteration 164, loss = 0.04136757\n",
      "Iteration 165, loss = 0.04103382\n",
      "Iteration 166, loss = 0.04085670\n",
      "Iteration 167, loss = 0.04092296\n",
      "Iteration 168, loss = 0.04051798\n",
      "Iteration 169, loss = 0.04061044\n",
      "Iteration 170, loss = 0.04024168\n",
      "Iteration 171, loss = 0.04001099\n",
      "Iteration 172, loss = 0.03985617\n",
      "Iteration 173, loss = 0.03967137\n",
      "Iteration 174, loss = 0.03968274\n",
      "Iteration 175, loss = 0.03977172\n",
      "Iteration 176, loss = 0.03920864\n",
      "Iteration 177, loss = 0.03935377\n",
      "Iteration 178, loss = 0.03895213\n",
      "Iteration 179, loss = 0.03885965\n",
      "Iteration 180, loss = 0.03866459\n",
      "Iteration 181, loss = 0.03873993\n",
      "Iteration 182, loss = 0.03829613\n",
      "Iteration 183, loss = 0.03820663\n",
      "Iteration 184, loss = 0.03831798\n",
      "Iteration 185, loss = 0.03808987\n",
      "Iteration 186, loss = 0.03772306\n",
      "Iteration 187, loss = 0.03791979\n",
      "Iteration 188, loss = 0.03853448\n",
      "Iteration 189, loss = 0.03728158\n",
      "Iteration 190, loss = 0.03740570\n",
      "Iteration 191, loss = 0.03736914\n",
      "Iteration 192, loss = 0.03703495\n",
      "Iteration 193, loss = 0.03698081\n",
      "Iteration 194, loss = 0.03681179\n",
      "Iteration 195, loss = 0.03655965\n",
      "Iteration 196, loss = 0.03650991\n",
      "Iteration 197, loss = 0.03679056\n",
      "Iteration 198, loss = 0.03672202\n",
      "Iteration 199, loss = 0.03620863\n",
      "Iteration 200, loss = 0.03598749\n",
      "Iteration 201, loss = 0.03604968\n",
      "Iteration 202, loss = 0.03572016\n",
      "Iteration 203, loss = 0.03587437\n",
      "Iteration 204, loss = 0.03562154\n",
      "Iteration 205, loss = 0.03546642\n",
      "Iteration 206, loss = 0.03545282\n",
      "Iteration 207, loss = 0.03516418\n",
      "Iteration 208, loss = 0.03518844\n",
      "Iteration 209, loss = 0.03518065\n",
      "Iteration 210, loss = 0.03480848\n",
      "Iteration 211, loss = 0.03477046\n",
      "Iteration 212, loss = 0.03485250\n",
      "Iteration 213, loss = 0.03478113\n",
      "Iteration 214, loss = 0.03462041\n",
      "Iteration 215, loss = 0.03442242\n",
      "Iteration 216, loss = 0.03442495\n",
      "Iteration 217, loss = 0.03452411\n",
      "Iteration 218, loss = 0.03414906\n",
      "Iteration 219, loss = 0.03401269\n",
      "Iteration 220, loss = 0.03407401\n",
      "Iteration 221, loss = 0.03385416\n",
      "Iteration 222, loss = 0.03378696\n",
      "Iteration 223, loss = 0.03408596\n",
      "Iteration 224, loss = 0.03397819\n",
      "Iteration 225, loss = 0.03400359\n",
      "Iteration 226, loss = 0.03358093\n",
      "Iteration 227, loss = 0.03344506\n",
      "Iteration 228, loss = 0.03329207\n",
      "Iteration 229, loss = 0.03323409\n",
      "Iteration 230, loss = 0.03344746\n",
      "Iteration 231, loss = 0.03321148\n",
      "Iteration 232, loss = 0.03302275\n",
      "Iteration 233, loss = 0.03310477\n",
      "Iteration 234, loss = 0.03284293\n",
      "Iteration 235, loss = 0.03273106\n",
      "Iteration 236, loss = 0.03285308\n",
      "Iteration 237, loss = 0.03273847\n",
      "Iteration 238, loss = 0.03230081\n",
      "Iteration 239, loss = 0.03309066\n",
      "Iteration 240, loss = 0.03227128\n",
      "Iteration 241, loss = 0.03216759\n",
      "Iteration 242, loss = 0.03243120\n",
      "Iteration 243, loss = 0.03199284\n",
      "Iteration 244, loss = 0.03230660\n",
      "Iteration 245, loss = 0.03212929\n",
      "Iteration 246, loss = 0.03186727\n",
      "Iteration 247, loss = 0.03181188\n",
      "Iteration 248, loss = 0.03170887\n",
      "Iteration 249, loss = 0.03164402\n",
      "Iteration 250, loss = 0.03154377\n",
      "Iteration 251, loss = 0.03161951\n",
      "Iteration 252, loss = 0.03146062\n",
      "Iteration 253, loss = 0.03148212\n",
      "Iteration 254, loss = 0.03149064\n",
      "Iteration 255, loss = 0.03125366\n",
      "Iteration 256, loss = 0.03118942\n",
      "Iteration 257, loss = 0.03112092\n",
      "Iteration 258, loss = 0.03128087\n",
      "Iteration 259, loss = 0.03103000\n",
      "Iteration 260, loss = 0.03198527\n",
      "Iteration 261, loss = 0.03102068\n",
      "Iteration 262, loss = 0.03081674\n",
      "Iteration 263, loss = 0.03073190\n",
      "Iteration 264, loss = 0.03115377\n",
      "Iteration 265, loss = 0.03085682\n",
      "Iteration 266, loss = 0.03048319\n",
      "Iteration 267, loss = 0.03045407\n",
      "Iteration 268, loss = 0.03043621\n",
      "Iteration 269, loss = 0.03033293\n",
      "Iteration 270, loss = 0.03036409\n",
      "Iteration 271, loss = 0.03065100\n",
      "Iteration 272, loss = 0.03010069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 273, loss = 0.03020692\n",
      "Iteration 274, loss = 0.03043320\n",
      "Iteration 275, loss = 0.03031103\n",
      "Iteration 276, loss = 0.02995062\n",
      "Iteration 277, loss = 0.02980704\n",
      "Iteration 278, loss = 0.02977516\n",
      "Iteration 279, loss = 0.02978715\n",
      "Iteration 280, loss = 0.02971916\n",
      "Iteration 281, loss = 0.02963267\n",
      "Iteration 282, loss = 0.02993023\n",
      "Iteration 283, loss = 0.02965464\n",
      "Iteration 284, loss = 0.02950628\n",
      "Iteration 285, loss = 0.02944591\n",
      "Iteration 286, loss = 0.02981918\n",
      "Iteration 287, loss = 0.02967256\n",
      "Iteration 288, loss = 0.02933023\n",
      "Iteration 289, loss = 0.02939003\n",
      "Iteration 290, loss = 0.02927170\n",
      "Iteration 291, loss = 0.02906452\n",
      "Iteration 292, loss = 0.02916834\n",
      "Iteration 293, loss = 0.02915385\n",
      "Iteration 294, loss = 0.02921892\n",
      "Iteration 295, loss = 0.02923947\n",
      "Iteration 296, loss = 0.02878388\n",
      "Iteration 297, loss = 0.02895949\n",
      "Iteration 298, loss = 0.02872438\n",
      "Iteration 299, loss = 0.02869691\n",
      "Iteration 300, loss = 0.02886127\n",
      "Iteration 301, loss = 0.02860081\n",
      "Iteration 302, loss = 0.02876389\n",
      "Iteration 303, loss = 0.02870729\n",
      "Iteration 304, loss = 0.02847878\n",
      "Iteration 305, loss = 0.02848878\n",
      "Iteration 306, loss = 0.02825687\n",
      "Iteration 307, loss = 0.02835166\n",
      "Iteration 308, loss = 0.02844046\n",
      "Iteration 309, loss = 0.02841441\n",
      "Iteration 310, loss = 0.02827587\n",
      "Iteration 311, loss = 0.02838037\n",
      "Iteration 312, loss = 0.02815603\n",
      "Iteration 313, loss = 0.02844603\n",
      "Iteration 314, loss = 0.02817473\n",
      "Iteration 315, loss = 0.02824430\n",
      "Iteration 316, loss = 0.02808573\n",
      "Iteration 317, loss = 0.02793490\n",
      "Iteration 318, loss = 0.02796415\n",
      "Iteration 319, loss = 0.02790083\n",
      "Iteration 320, loss = 0.02781396\n",
      "Iteration 321, loss = 0.02780273\n",
      "Iteration 322, loss = 0.02787385\n",
      "Iteration 323, loss = 0.02802611\n",
      "Iteration 324, loss = 0.02784385\n",
      "Iteration 325, loss = 0.02756160\n",
      "Iteration 326, loss = 0.02762453\n",
      "Iteration 327, loss = 0.02758766\n",
      "Iteration 328, loss = 0.02755617\n",
      "Iteration 329, loss = 0.02770902\n",
      "Iteration 330, loss = 0.02789344\n",
      "Iteration 331, loss = 0.02741609\n",
      "Iteration 332, loss = 0.02753528\n",
      "Iteration 333, loss = 0.02742787\n",
      "Iteration 334, loss = 0.02734546\n",
      "Iteration 335, loss = 0.02726268\n",
      "Iteration 336, loss = 0.02735776\n",
      "Iteration 337, loss = 0.02730009\n",
      "Iteration 338, loss = 0.02733453\n",
      "Iteration 339, loss = 0.02705075\n",
      "Iteration 340, loss = 0.02692482\n",
      "Iteration 341, loss = 0.02699972\n",
      "Iteration 342, loss = 0.02691541\n",
      "Iteration 343, loss = 0.02695487\n",
      "Iteration 344, loss = 0.02664838\n",
      "Iteration 345, loss = 0.02712930\n",
      "Iteration 346, loss = 0.02682465\n",
      "Iteration 347, loss = 0.02692552\n",
      "Iteration 348, loss = 0.02689338\n",
      "Iteration 349, loss = 0.02674632\n",
      "Iteration 350, loss = 0.02672120\n",
      "Iteration 351, loss = 0.02683010\n",
      "Iteration 352, loss = 0.02658918\n",
      "Iteration 353, loss = 0.02671648\n",
      "Iteration 354, loss = 0.02688414\n",
      "Iteration 355, loss = 0.02690538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 90.03091190108191\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69226073\n",
      "Iteration 2, loss = 0.68742171\n",
      "Iteration 3, loss = 0.68143733\n",
      "Iteration 4, loss = 0.67271614\n",
      "Iteration 5, loss = 0.66109619\n",
      "Iteration 6, loss = 0.64631561\n",
      "Iteration 7, loss = 0.62885749\n",
      "Iteration 8, loss = 0.60885296\n",
      "Iteration 9, loss = 0.58703054\n",
      "Iteration 10, loss = 0.56347978\n",
      "Iteration 11, loss = 0.53883907\n",
      "Iteration 12, loss = 0.51406554\n",
      "Iteration 13, loss = 0.48937490\n",
      "Iteration 14, loss = 0.46512726\n",
      "Iteration 15, loss = 0.44132859\n",
      "Iteration 16, loss = 0.41873832\n",
      "Iteration 17, loss = 0.39818509\n",
      "Iteration 18, loss = 0.37806867\n",
      "Iteration 19, loss = 0.35918321\n",
      "Iteration 20, loss = 0.34145300\n",
      "Iteration 21, loss = 0.32496747\n",
      "Iteration 22, loss = 0.30978697\n",
      "Iteration 23, loss = 0.29591641\n",
      "Iteration 24, loss = 0.28293249\n",
      "Iteration 25, loss = 0.27113441\n",
      "Iteration 26, loss = 0.25991096\n",
      "Iteration 27, loss = 0.24992025\n",
      "Iteration 28, loss = 0.24049962\n",
      "Iteration 29, loss = 0.23153809\n",
      "Iteration 30, loss = 0.22323533\n",
      "Iteration 31, loss = 0.21580686\n",
      "Iteration 32, loss = 0.20860175\n",
      "Iteration 33, loss = 0.20212880\n",
      "Iteration 34, loss = 0.19585016\n",
      "Iteration 35, loss = 0.19006888\n",
      "Iteration 36, loss = 0.18463530\n",
      "Iteration 37, loss = 0.17960952\n",
      "Iteration 38, loss = 0.17431615\n",
      "Iteration 39, loss = 0.16984756\n",
      "Iteration 40, loss = 0.16554316\n",
      "Iteration 41, loss = 0.16135379\n",
      "Iteration 42, loss = 0.15762547\n",
      "Iteration 43, loss = 0.15384323\n",
      "Iteration 44, loss = 0.15031874\n",
      "Iteration 45, loss = 0.14688999\n",
      "Iteration 46, loss = 0.14384502\n",
      "Iteration 47, loss = 0.14086807\n",
      "Iteration 48, loss = 0.13804375\n",
      "Iteration 49, loss = 0.13496329\n",
      "Iteration 50, loss = 0.13222652\n",
      "Iteration 51, loss = 0.12988811\n",
      "Iteration 52, loss = 0.12751229\n",
      "Iteration 53, loss = 0.12485184\n",
      "Iteration 54, loss = 0.12268766\n",
      "Iteration 55, loss = 0.12068037\n",
      "Iteration 56, loss = 0.11862172\n",
      "Iteration 57, loss = 0.11640435\n",
      "Iteration 58, loss = 0.11440538\n",
      "Iteration 59, loss = 0.11305703\n",
      "Iteration 60, loss = 0.11114622\n",
      "Iteration 61, loss = 0.10914892\n",
      "Iteration 62, loss = 0.10725162\n",
      "Iteration 63, loss = 0.10558177\n",
      "Iteration 64, loss = 0.10413323\n",
      "Iteration 65, loss = 0.10246524\n",
      "Iteration 66, loss = 0.10097609\n",
      "Iteration 67, loss = 0.09962701\n",
      "Iteration 68, loss = 0.09848893\n",
      "Iteration 69, loss = 0.09710738\n",
      "Iteration 70, loss = 0.09571657\n",
      "Iteration 71, loss = 0.09500659\n",
      "Iteration 72, loss = 0.09344832\n",
      "Iteration 73, loss = 0.09195218\n",
      "Iteration 74, loss = 0.09074466\n",
      "Iteration 75, loss = 0.08960284\n",
      "Iteration 76, loss = 0.08848289\n",
      "Iteration 77, loss = 0.08733122\n",
      "Iteration 78, loss = 0.08687260\n",
      "Iteration 79, loss = 0.08554201\n",
      "Iteration 80, loss = 0.08453104\n",
      "Iteration 81, loss = 0.08338713\n",
      "Iteration 82, loss = 0.08245443\n",
      "Iteration 83, loss = 0.08184782\n",
      "Iteration 84, loss = 0.08074601\n",
      "Iteration 85, loss = 0.07977278\n",
      "Iteration 86, loss = 0.07949740\n",
      "Iteration 87, loss = 0.07857382\n",
      "Iteration 88, loss = 0.07735250\n",
      "Iteration 89, loss = 0.07667947\n",
      "Iteration 90, loss = 0.07584611\n",
      "Iteration 91, loss = 0.07519539\n",
      "Iteration 92, loss = 0.07433059\n",
      "Iteration 93, loss = 0.07359822\n",
      "Iteration 94, loss = 0.07327682\n",
      "Iteration 95, loss = 0.07211936\n",
      "Iteration 96, loss = 0.07171089\n",
      "Iteration 97, loss = 0.07084310\n",
      "Iteration 98, loss = 0.07069658\n",
      "Iteration 99, loss = 0.06985211\n",
      "Iteration 100, loss = 0.06911234\n",
      "Iteration 101, loss = 0.06847795\n",
      "Iteration 102, loss = 0.06796839\n",
      "Iteration 103, loss = 0.06764097\n",
      "Iteration 104, loss = 0.06673576\n",
      "Iteration 105, loss = 0.06619000\n",
      "Iteration 106, loss = 0.06568169\n",
      "Iteration 107, loss = 0.06508207\n",
      "Iteration 108, loss = 0.06489506\n",
      "Iteration 109, loss = 0.06413291\n",
      "Iteration 110, loss = 0.06382555\n",
      "Iteration 111, loss = 0.06323272\n",
      "Iteration 112, loss = 0.06270045\n",
      "Iteration 113, loss = 0.06227005\n",
      "Iteration 114, loss = 0.06161819\n",
      "Iteration 115, loss = 0.06133307\n",
      "Iteration 116, loss = 0.06081691\n",
      "Iteration 117, loss = 0.06030587\n",
      "Iteration 118, loss = 0.05987219\n",
      "Iteration 119, loss = 0.05954764\n",
      "Iteration 120, loss = 0.05916401\n",
      "Iteration 121, loss = 0.05869351\n",
      "Iteration 122, loss = 0.05845845\n",
      "Iteration 123, loss = 0.05773099\n",
      "Iteration 124, loss = 0.05767652\n",
      "Iteration 125, loss = 0.05750001\n",
      "Iteration 126, loss = 0.05682852\n",
      "Iteration 127, loss = 0.05719130\n",
      "Iteration 128, loss = 0.05660154\n",
      "Iteration 129, loss = 0.05574285\n",
      "Iteration 130, loss = 0.05550277\n",
      "Iteration 131, loss = 0.05527915\n",
      "Iteration 132, loss = 0.05505248\n",
      "Iteration 133, loss = 0.05444818\n",
      "Iteration 134, loss = 0.05411327\n",
      "Iteration 135, loss = 0.05371067\n",
      "Iteration 136, loss = 0.05352773\n",
      "Iteration 137, loss = 0.05321328\n",
      "Iteration 138, loss = 0.05319381\n",
      "Iteration 139, loss = 0.05271126\n",
      "Iteration 140, loss = 0.05228618\n",
      "Iteration 141, loss = 0.05220085\n",
      "Iteration 142, loss = 0.05186714\n",
      "Iteration 143, loss = 0.05146089\n",
      "Iteration 144, loss = 0.05113468\n",
      "Iteration 145, loss = 0.05080833\n",
      "Iteration 146, loss = 0.05059552\n",
      "Iteration 147, loss = 0.05027594\n",
      "Iteration 148, loss = 0.05009019\n",
      "Iteration 149, loss = 0.04984298\n",
      "Iteration 150, loss = 0.04972666\n",
      "Iteration 151, loss = 0.04935586\n",
      "Iteration 152, loss = 0.04937264\n",
      "Iteration 153, loss = 0.04893639\n",
      "Iteration 154, loss = 0.04895878\n",
      "Iteration 155, loss = 0.04859734\n",
      "Iteration 156, loss = 0.04817798\n",
      "Iteration 157, loss = 0.04805983\n",
      "Iteration 158, loss = 0.04873379\n",
      "Iteration 159, loss = 0.04780097\n",
      "Iteration 160, loss = 0.04777445\n",
      "Iteration 161, loss = 0.04728599\n",
      "Iteration 162, loss = 0.04690280\n",
      "Iteration 163, loss = 0.04685287\n",
      "Iteration 164, loss = 0.04637601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 165, loss = 0.04686852\n",
      "Iteration 166, loss = 0.04625838\n",
      "Iteration 167, loss = 0.04600351\n",
      "Iteration 168, loss = 0.04580540\n",
      "Iteration 169, loss = 0.04589553\n",
      "Iteration 170, loss = 0.04563505\n",
      "Iteration 171, loss = 0.04552641\n",
      "Iteration 172, loss = 0.04516498\n",
      "Iteration 173, loss = 0.04504002\n",
      "Iteration 174, loss = 0.04477905\n",
      "Iteration 175, loss = 0.04483713\n",
      "Iteration 176, loss = 0.04483025\n",
      "Iteration 177, loss = 0.04432378\n",
      "Iteration 178, loss = 0.04438273\n",
      "Iteration 179, loss = 0.04407028\n",
      "Iteration 180, loss = 0.04400376\n",
      "Iteration 181, loss = 0.04395303\n",
      "Iteration 182, loss = 0.04365836\n",
      "Iteration 183, loss = 0.04336913\n",
      "Iteration 184, loss = 0.04351894\n",
      "Iteration 185, loss = 0.04312575\n",
      "Iteration 186, loss = 0.04325483\n",
      "Iteration 187, loss = 0.04276570\n",
      "Iteration 188, loss = 0.04290063\n",
      "Iteration 189, loss = 0.04249920\n",
      "Iteration 190, loss = 0.04232850\n",
      "Iteration 191, loss = 0.04222230\n",
      "Iteration 192, loss = 0.04226942\n",
      "Iteration 193, loss = 0.04176511\n",
      "Iteration 194, loss = 0.04184955\n",
      "Iteration 195, loss = 0.04199563\n",
      "Iteration 196, loss = 0.04157717\n",
      "Iteration 197, loss = 0.04151260\n",
      "Iteration 198, loss = 0.04126876\n",
      "Iteration 199, loss = 0.04115082\n",
      "Iteration 200, loss = 0.04097829\n",
      "Iteration 201, loss = 0.04108314\n",
      "Iteration 202, loss = 0.04088602\n",
      "Iteration 203, loss = 0.04060336\n",
      "Iteration 204, loss = 0.04049609\n",
      "Iteration 205, loss = 0.04052323\n",
      "Iteration 206, loss = 0.04029342\n",
      "Iteration 207, loss = 0.04043052\n",
      "Iteration 208, loss = 0.04020520\n",
      "Iteration 209, loss = 0.03999586\n",
      "Iteration 210, loss = 0.04004366\n",
      "Iteration 211, loss = 0.04007351\n",
      "Iteration 212, loss = 0.04028975\n",
      "Iteration 213, loss = 0.04015367\n",
      "Iteration 214, loss = 0.03972581\n",
      "Iteration 215, loss = 0.03940564\n",
      "Iteration 216, loss = 0.03929741\n",
      "Iteration 217, loss = 0.03919639\n",
      "Iteration 218, loss = 0.03911423\n",
      "Iteration 219, loss = 0.03895342\n",
      "Iteration 220, loss = 0.03891682\n",
      "Iteration 221, loss = 0.03890046\n",
      "Iteration 222, loss = 0.03857989\n",
      "Iteration 223, loss = 0.03855107\n",
      "Iteration 224, loss = 0.03852146\n",
      "Iteration 225, loss = 0.03849826\n",
      "Iteration 226, loss = 0.03840712\n",
      "Iteration 227, loss = 0.03875225\n",
      "Iteration 228, loss = 0.03796040\n",
      "Iteration 229, loss = 0.03832991\n",
      "Iteration 230, loss = 0.03801805\n",
      "Iteration 231, loss = 0.03786734\n",
      "Iteration 232, loss = 0.03761907\n",
      "Iteration 233, loss = 0.03754331\n",
      "Iteration 234, loss = 0.03755688\n",
      "Iteration 235, loss = 0.03745767\n",
      "Iteration 236, loss = 0.03764115\n",
      "Iteration 237, loss = 0.03739868\n",
      "Iteration 238, loss = 0.03733669\n",
      "Iteration 239, loss = 0.03719828\n",
      "Iteration 240, loss = 0.03731054\n",
      "Iteration 241, loss = 0.03692750\n",
      "Iteration 242, loss = 0.03707512\n",
      "Iteration 243, loss = 0.03717031\n",
      "Iteration 244, loss = 0.03660837\n",
      "Iteration 245, loss = 0.03671987\n",
      "Iteration 246, loss = 0.03661242\n",
      "Iteration 247, loss = 0.03651256\n",
      "Iteration 248, loss = 0.03648272\n",
      "Iteration 249, loss = 0.03659485\n",
      "Iteration 250, loss = 0.03616373\n",
      "Iteration 251, loss = 0.03593848\n",
      "Iteration 252, loss = 0.03633529\n",
      "Iteration 253, loss = 0.03601907\n",
      "Iteration 254, loss = 0.03582686\n",
      "Iteration 255, loss = 0.03584222\n",
      "Iteration 256, loss = 0.03601877\n",
      "Iteration 257, loss = 0.03577555\n",
      "Iteration 258, loss = 0.03581233\n",
      "Iteration 259, loss = 0.03577176\n",
      "Iteration 260, loss = 0.03547837\n",
      "Iteration 261, loss = 0.03536991\n",
      "Iteration 262, loss = 0.03540950\n",
      "Iteration 263, loss = 0.03515004\n",
      "Iteration 264, loss = 0.03526956\n",
      "Iteration 265, loss = 0.03526210\n",
      "Iteration 266, loss = 0.03541609\n",
      "Iteration 267, loss = 0.03508067\n",
      "Iteration 268, loss = 0.03541858\n",
      "Iteration 269, loss = 0.03494956\n",
      "Iteration 270, loss = 0.03510449\n",
      "Iteration 271, loss = 0.03501686\n",
      "Iteration 272, loss = 0.03475851\n",
      "Iteration 273, loss = 0.03470764\n",
      "Iteration 274, loss = 0.03557160\n",
      "Iteration 275, loss = 0.03532534\n",
      "Iteration 276, loss = 0.03476776\n",
      "Iteration 277, loss = 0.03465106\n",
      "Iteration 278, loss = 0.03419098\n",
      "Iteration 279, loss = 0.03484651\n",
      "Iteration 280, loss = 0.03422875\n",
      "Iteration 281, loss = 0.03410599\n",
      "Iteration 282, loss = 0.03447262\n",
      "Iteration 283, loss = 0.03423797\n",
      "Iteration 284, loss = 0.03408774\n",
      "Iteration 285, loss = 0.03426285\n",
      "Iteration 286, loss = 0.03430144\n",
      "Iteration 287, loss = 0.03399765\n",
      "Iteration 288, loss = 0.03389636\n",
      "Iteration 289, loss = 0.03390409\n",
      "Iteration 290, loss = 0.03375503\n",
      "Iteration 291, loss = 0.03394580\n",
      "Iteration 292, loss = 0.03330814\n",
      "Iteration 293, loss = 0.03370916\n",
      "Iteration 294, loss = 0.03367568\n",
      "Iteration 295, loss = 0.03353954\n",
      "Iteration 296, loss = 0.03326424\n",
      "Iteration 297, loss = 0.03343935\n",
      "Iteration 298, loss = 0.03321591\n",
      "Iteration 299, loss = 0.03315201\n",
      "Iteration 300, loss = 0.03341948\n",
      "Iteration 301, loss = 0.03334531\n",
      "Iteration 302, loss = 0.03298600\n",
      "Iteration 303, loss = 0.03286662\n",
      "Iteration 304, loss = 0.03333726\n",
      "Iteration 305, loss = 0.03320003\n",
      "Iteration 306, loss = 0.03298552\n",
      "Iteration 307, loss = 0.03274440\n",
      "Iteration 308, loss = 0.03278003\n",
      "Iteration 309, loss = 0.03275692\n",
      "Iteration 310, loss = 0.03276421\n",
      "Iteration 311, loss = 0.03265498\n",
      "Iteration 312, loss = 0.03291472\n",
      "Iteration 313, loss = 0.03265437\n",
      "Iteration 314, loss = 0.03285837\n",
      "Iteration 315, loss = 0.03284191\n",
      "Iteration 316, loss = 0.03236705\n",
      "Iteration 317, loss = 0.03226237\n",
      "Iteration 318, loss = 0.03242561\n",
      "Iteration 319, loss = 0.03223358\n",
      "Iteration 320, loss = 0.03206345\n",
      "Iteration 321, loss = 0.03244971\n",
      "Iteration 322, loss = 0.03240872\n",
      "Iteration 323, loss = 0.03226275\n",
      "Iteration 324, loss = 0.03197132\n",
      "Iteration 325, loss = 0.03193799\n",
      "Iteration 326, loss = 0.03195716\n",
      "Iteration 327, loss = 0.03197991\n",
      "Iteration 328, loss = 0.03199550\n",
      "Iteration 329, loss = 0.03163042\n",
      "Iteration 330, loss = 0.03264804\n",
      "Iteration 331, loss = 0.03228865\n",
      "Iteration 332, loss = 0.03178666\n",
      "Iteration 333, loss = 0.03187700\n",
      "Iteration 334, loss = 0.03160659\n",
      "Iteration 335, loss = 0.03157277\n",
      "Iteration 336, loss = 0.03146596\n",
      "Iteration 337, loss = 0.03151520\n",
      "Iteration 338, loss = 0.03131328\n",
      "Iteration 339, loss = 0.03141304\n",
      "Iteration 340, loss = 0.03152400\n",
      "Iteration 341, loss = 0.03137325\n",
      "Iteration 342, loss = 0.03144738\n",
      "Iteration 343, loss = 0.03113998\n",
      "Iteration 344, loss = 0.03111011\n",
      "Iteration 345, loss = 0.03170136\n",
      "Iteration 346, loss = 0.03112022\n",
      "Iteration 347, loss = 0.03104595\n",
      "Iteration 348, loss = 0.03114469\n",
      "Iteration 349, loss = 0.03135073\n",
      "Iteration 350, loss = 0.03118779\n",
      "Iteration 351, loss = 0.03107645\n",
      "Iteration 352, loss = 0.03087519\n",
      "Iteration 353, loss = 0.03108230\n",
      "Iteration 354, loss = 0.03108178\n",
      "Iteration 355, loss = 0.03120362\n",
      "Iteration 356, loss = 0.03088351\n",
      "Iteration 357, loss = 0.03090244\n",
      "Iteration 358, loss = 0.03113332\n",
      "Iteration 359, loss = 0.03091270\n",
      "Iteration 360, loss = 0.03107806\n",
      "Iteration 361, loss = 0.03083088\n",
      "Iteration 362, loss = 0.03065006\n",
      "Iteration 363, loss = 0.03056975\n",
      "Iteration 364, loss = 0.03084726\n",
      "Iteration 365, loss = 0.03044270\n",
      "Iteration 366, loss = 0.03074495\n",
      "Iteration 367, loss = 0.03082129\n",
      "Iteration 368, loss = 0.03040317\n",
      "Iteration 369, loss = 0.03063526\n",
      "Iteration 370, loss = 0.03046501\n",
      "Iteration 371, loss = 0.03046839\n",
      "Iteration 372, loss = 0.03022359\n",
      "Iteration 373, loss = 0.03039122\n",
      "Iteration 374, loss = 0.03005766\n",
      "Iteration 375, loss = 0.03033087\n",
      "Iteration 376, loss = 0.03017155\n",
      "Iteration 377, loss = 0.03009201\n",
      "Iteration 378, loss = 0.02988947\n",
      "Iteration 379, loss = 0.02993761\n",
      "Iteration 380, loss = 0.02992390\n",
      "Iteration 381, loss = 0.03016312\n",
      "Iteration 382, loss = 0.02990283\n",
      "Iteration 383, loss = 0.03013190\n",
      "Iteration 384, loss = 0.02975781\n",
      "Iteration 385, loss = 0.02995333\n",
      "Iteration 386, loss = 0.03027111\n",
      "Iteration 387, loss = 0.02976634\n",
      "Iteration 388, loss = 0.02999396\n",
      "Iteration 389, loss = 0.02968439\n",
      "Iteration 390, loss = 0.02942865\n",
      "Iteration 391, loss = 0.03030199\n",
      "Iteration 392, loss = 0.02995309\n",
      "Iteration 393, loss = 0.02974100\n",
      "Iteration 394, loss = 0.02978692\n",
      "Iteration 395, loss = 0.02942520\n",
      "Iteration 396, loss = 0.02977071\n",
      "Iteration 397, loss = 0.02956487\n",
      "Iteration 398, loss = 0.02962543\n",
      "Iteration 399, loss = 0.02954141\n",
      "Iteration 400, loss = 0.02940225\n",
      "Iteration 401, loss = 0.02974600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 92.50386398763524\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69239606\n",
      "Iteration 2, loss = 0.68766681\n",
      "Iteration 3, loss = 0.68189706\n",
      "Iteration 4, loss = 0.67405593\n",
      "Iteration 5, loss = 0.66341911\n",
      "Iteration 6, loss = 0.64977027\n",
      "Iteration 7, loss = 0.63332914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.61442282\n",
      "Iteration 9, loss = 0.59338731\n",
      "Iteration 10, loss = 0.57047422\n",
      "Iteration 11, loss = 0.54682193\n",
      "Iteration 12, loss = 0.52283548\n",
      "Iteration 13, loss = 0.49813000\n",
      "Iteration 14, loss = 0.47369339\n",
      "Iteration 15, loss = 0.45009306\n",
      "Iteration 16, loss = 0.42743933\n",
      "Iteration 17, loss = 0.40567406\n",
      "Iteration 18, loss = 0.38536413\n",
      "Iteration 19, loss = 0.36666469\n",
      "Iteration 20, loss = 0.34835994\n",
      "Iteration 21, loss = 0.33173765\n",
      "Iteration 22, loss = 0.31621334\n",
      "Iteration 23, loss = 0.30227429\n",
      "Iteration 24, loss = 0.28880644\n",
      "Iteration 25, loss = 0.27641511\n",
      "Iteration 26, loss = 0.26492589\n",
      "Iteration 27, loss = 0.25440137\n",
      "Iteration 28, loss = 0.24462015\n",
      "Iteration 29, loss = 0.23621419\n",
      "Iteration 30, loss = 0.22726680\n",
      "Iteration 31, loss = 0.21921316\n",
      "Iteration 32, loss = 0.21226022\n",
      "Iteration 33, loss = 0.20503147\n",
      "Iteration 34, loss = 0.19863005\n",
      "Iteration 35, loss = 0.19271704\n",
      "Iteration 36, loss = 0.18711549\n",
      "Iteration 37, loss = 0.18180398\n",
      "Iteration 38, loss = 0.17707679\n",
      "Iteration 39, loss = 0.17186634\n",
      "Iteration 40, loss = 0.16752708\n",
      "Iteration 41, loss = 0.16337086\n",
      "Iteration 42, loss = 0.15933675\n",
      "Iteration 43, loss = 0.15540294\n",
      "Iteration 44, loss = 0.15218242\n",
      "Iteration 45, loss = 0.14871717\n",
      "Iteration 46, loss = 0.14505445\n",
      "Iteration 47, loss = 0.14181323\n",
      "Iteration 48, loss = 0.13908987\n",
      "Iteration 49, loss = 0.13609471\n",
      "Iteration 50, loss = 0.13325986\n",
      "Iteration 51, loss = 0.13052320\n",
      "Iteration 52, loss = 0.12852796\n",
      "Iteration 53, loss = 0.12563217\n",
      "Iteration 54, loss = 0.12338445\n",
      "Iteration 55, loss = 0.12135310\n",
      "Iteration 56, loss = 0.11901108\n",
      "Iteration 57, loss = 0.11703457\n",
      "Iteration 58, loss = 0.11488081\n",
      "Iteration 59, loss = 0.11293299\n",
      "Iteration 60, loss = 0.11100752\n",
      "Iteration 61, loss = 0.10910170\n",
      "Iteration 62, loss = 0.10762402\n",
      "Iteration 63, loss = 0.10565086\n",
      "Iteration 64, loss = 0.10407810\n",
      "Iteration 65, loss = 0.10255745\n",
      "Iteration 66, loss = 0.10145233\n",
      "Iteration 67, loss = 0.09931231\n",
      "Iteration 68, loss = 0.09815547\n",
      "Iteration 69, loss = 0.09654221\n",
      "Iteration 70, loss = 0.09536939\n",
      "Iteration 71, loss = 0.09418536\n",
      "Iteration 72, loss = 0.09282807\n",
      "Iteration 73, loss = 0.09188965\n",
      "Iteration 74, loss = 0.09028662\n",
      "Iteration 75, loss = 0.08906372\n",
      "Iteration 76, loss = 0.08842939\n",
      "Iteration 77, loss = 0.08720209\n",
      "Iteration 78, loss = 0.08576686\n",
      "Iteration 79, loss = 0.08479627\n",
      "Iteration 80, loss = 0.08438456\n",
      "Iteration 81, loss = 0.08282798\n",
      "Iteration 82, loss = 0.08192243\n",
      "Iteration 83, loss = 0.08099154\n",
      "Iteration 84, loss = 0.08011814\n",
      "Iteration 85, loss = 0.07902946\n",
      "Iteration 86, loss = 0.07815971\n",
      "Iteration 87, loss = 0.07769310\n",
      "Iteration 88, loss = 0.07697053\n",
      "Iteration 89, loss = 0.07563934\n",
      "Iteration 90, loss = 0.07491436\n",
      "Iteration 91, loss = 0.07415683\n",
      "Iteration 92, loss = 0.07330998\n",
      "Iteration 93, loss = 0.07264255\n",
      "Iteration 94, loss = 0.07199864\n",
      "Iteration 95, loss = 0.07112932\n",
      "Iteration 96, loss = 0.07043131\n",
      "Iteration 97, loss = 0.06968442\n",
      "Iteration 98, loss = 0.06910025\n",
      "Iteration 99, loss = 0.06848061\n",
      "Iteration 100, loss = 0.06783716\n",
      "Iteration 101, loss = 0.06718710\n",
      "Iteration 102, loss = 0.06662042\n",
      "Iteration 103, loss = 0.06596791\n",
      "Iteration 104, loss = 0.06560595\n",
      "Iteration 105, loss = 0.06485235\n",
      "Iteration 106, loss = 0.06459490\n",
      "Iteration 107, loss = 0.06367143\n",
      "Iteration 108, loss = 0.06313150\n",
      "Iteration 109, loss = 0.06278557\n",
      "Iteration 110, loss = 0.06221519\n",
      "Iteration 111, loss = 0.06167049\n",
      "Iteration 112, loss = 0.06122086\n",
      "Iteration 113, loss = 0.06057812\n",
      "Iteration 114, loss = 0.06016143\n",
      "Iteration 115, loss = 0.05995842\n",
      "Iteration 116, loss = 0.05978889\n",
      "Iteration 117, loss = 0.05909980\n",
      "Iteration 118, loss = 0.05859667\n",
      "Iteration 119, loss = 0.05803722\n",
      "Iteration 120, loss = 0.05759024\n",
      "Iteration 121, loss = 0.05719331\n",
      "Iteration 122, loss = 0.05667425\n",
      "Iteration 123, loss = 0.05661892\n",
      "Iteration 124, loss = 0.05600108\n",
      "Iteration 125, loss = 0.05581428\n",
      "Iteration 126, loss = 0.05536852\n",
      "Iteration 127, loss = 0.05483527\n",
      "Iteration 128, loss = 0.05463476\n",
      "Iteration 129, loss = 0.05450040\n",
      "Iteration 130, loss = 0.05389013\n",
      "Iteration 131, loss = 0.05362988\n",
      "Iteration 132, loss = 0.05328965\n",
      "Iteration 133, loss = 0.05309976\n",
      "Iteration 134, loss = 0.05255971\n",
      "Iteration 135, loss = 0.05238894\n",
      "Iteration 136, loss = 0.05184800\n",
      "Iteration 137, loss = 0.05150565\n",
      "Iteration 138, loss = 0.05120898\n",
      "Iteration 139, loss = 0.05096876\n",
      "Iteration 140, loss = 0.05058716\n",
      "Iteration 141, loss = 0.05031577\n",
      "Iteration 142, loss = 0.05004795\n",
      "Iteration 143, loss = 0.04995611\n",
      "Iteration 144, loss = 0.04968404\n",
      "Iteration 145, loss = 0.04928811\n",
      "Iteration 146, loss = 0.04918098\n",
      "Iteration 147, loss = 0.04881723\n",
      "Iteration 148, loss = 0.04851699\n",
      "Iteration 149, loss = 0.04835043\n",
      "Iteration 150, loss = 0.04814377\n",
      "Iteration 151, loss = 0.04796563\n",
      "Iteration 152, loss = 0.04763490\n",
      "Iteration 153, loss = 0.04739009\n",
      "Iteration 154, loss = 0.04703942\n",
      "Iteration 155, loss = 0.04677205\n",
      "Iteration 156, loss = 0.04661071\n",
      "Iteration 157, loss = 0.04663708\n",
      "Iteration 158, loss = 0.04631263\n",
      "Iteration 159, loss = 0.04593517\n",
      "Iteration 160, loss = 0.04581932\n",
      "Iteration 161, loss = 0.04559263\n",
      "Iteration 162, loss = 0.04515546\n",
      "Iteration 163, loss = 0.04541583\n",
      "Iteration 164, loss = 0.04504969\n",
      "Iteration 165, loss = 0.04474835\n",
      "Iteration 166, loss = 0.04496238\n",
      "Iteration 167, loss = 0.04429270\n",
      "Iteration 168, loss = 0.04410847\n",
      "Iteration 169, loss = 0.04399028\n",
      "Iteration 170, loss = 0.04392681\n",
      "Iteration 171, loss = 0.04383166\n",
      "Iteration 172, loss = 0.04352633\n",
      "Iteration 173, loss = 0.04326974\n",
      "Iteration 174, loss = 0.04327912\n",
      "Iteration 175, loss = 0.04289233\n",
      "Iteration 176, loss = 0.04287873\n",
      "Iteration 177, loss = 0.04267761\n",
      "Iteration 178, loss = 0.04259254\n",
      "Iteration 179, loss = 0.04246686\n",
      "Iteration 180, loss = 0.04226167\n",
      "Iteration 181, loss = 0.04217274\n",
      "Iteration 182, loss = 0.04197463\n",
      "Iteration 183, loss = 0.04171259\n",
      "Iteration 184, loss = 0.04141859\n",
      "Iteration 185, loss = 0.04141568\n",
      "Iteration 186, loss = 0.04165045\n",
      "Iteration 187, loss = 0.04111173\n",
      "Iteration 188, loss = 0.04108037\n",
      "Iteration 189, loss = 0.04079830\n",
      "Iteration 190, loss = 0.04082676\n",
      "Iteration 191, loss = 0.04052856\n",
      "Iteration 192, loss = 0.04056797\n",
      "Iteration 193, loss = 0.04033978\n",
      "Iteration 194, loss = 0.04040113\n",
      "Iteration 195, loss = 0.04009915\n",
      "Iteration 196, loss = 0.04000841\n",
      "Iteration 197, loss = 0.03997394\n",
      "Iteration 198, loss = 0.03986251\n",
      "Iteration 199, loss = 0.03992547\n",
      "Iteration 200, loss = 0.03962219\n",
      "Iteration 201, loss = 0.03930719\n",
      "Iteration 202, loss = 0.03913552\n",
      "Iteration 203, loss = 0.03929648\n",
      "Iteration 204, loss = 0.03892540\n",
      "Iteration 205, loss = 0.03882301\n",
      "Iteration 206, loss = 0.03868030\n",
      "Iteration 207, loss = 0.03851194\n",
      "Iteration 208, loss = 0.03864696\n",
      "Iteration 209, loss = 0.03836161\n",
      "Iteration 210, loss = 0.03841829\n",
      "Iteration 211, loss = 0.03821740\n",
      "Iteration 212, loss = 0.03828752\n",
      "Iteration 213, loss = 0.03806811\n",
      "Iteration 214, loss = 0.03810572\n",
      "Iteration 215, loss = 0.03759017\n",
      "Iteration 216, loss = 0.03780942\n",
      "Iteration 217, loss = 0.03736246\n",
      "Iteration 218, loss = 0.03748119\n",
      "Iteration 219, loss = 0.03739927\n",
      "Iteration 220, loss = 0.03713597\n",
      "Iteration 221, loss = 0.03724915\n",
      "Iteration 222, loss = 0.03704365\n",
      "Iteration 223, loss = 0.03702870\n",
      "Iteration 224, loss = 0.03688175\n",
      "Iteration 225, loss = 0.03686266\n",
      "Iteration 226, loss = 0.03661246\n",
      "Iteration 227, loss = 0.03666955\n",
      "Iteration 228, loss = 0.03649914\n",
      "Iteration 229, loss = 0.03635019\n",
      "Iteration 230, loss = 0.03622559\n",
      "Iteration 231, loss = 0.03623597\n",
      "Iteration 232, loss = 0.03651630\n",
      "Iteration 233, loss = 0.03648768\n",
      "Iteration 234, loss = 0.03594446\n",
      "Iteration 235, loss = 0.03629761\n",
      "Iteration 236, loss = 0.03604694\n",
      "Iteration 237, loss = 0.03602016\n",
      "Iteration 238, loss = 0.03613717\n",
      "Iteration 239, loss = 0.03545449\n",
      "Iteration 240, loss = 0.03556539\n",
      "Iteration 241, loss = 0.03543411\n",
      "Iteration 242, loss = 0.03530009\n",
      "Iteration 243, loss = 0.03566994\n",
      "Iteration 244, loss = 0.03528642\n",
      "Iteration 245, loss = 0.03510634\n",
      "Iteration 246, loss = 0.03505318\n",
      "Iteration 247, loss = 0.03482042\n",
      "Iteration 248, loss = 0.03493347\n",
      "Iteration 249, loss = 0.03495287\n",
      "Iteration 250, loss = 0.03546777\n",
      "Iteration 251, loss = 0.03491892\n",
      "Iteration 252, loss = 0.03457966\n",
      "Iteration 253, loss = 0.03453112\n",
      "Iteration 254, loss = 0.03465449\n",
      "Iteration 255, loss = 0.03429595\n",
      "Iteration 256, loss = 0.03441865\n",
      "Iteration 257, loss = 0.03443968\n",
      "Iteration 258, loss = 0.03412498\n",
      "Iteration 259, loss = 0.03434990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.03411497\n",
      "Iteration 261, loss = 0.03401480\n",
      "Iteration 262, loss = 0.03381037\n",
      "Iteration 263, loss = 0.03368534\n",
      "Iteration 264, loss = 0.03373159\n",
      "Iteration 265, loss = 0.03364335\n",
      "Iteration 266, loss = 0.03398964\n",
      "Iteration 267, loss = 0.03370111\n",
      "Iteration 268, loss = 0.03338684\n",
      "Iteration 269, loss = 0.03379216\n",
      "Iteration 270, loss = 0.03373127\n",
      "Iteration 271, loss = 0.03341619\n",
      "Iteration 272, loss = 0.03315899\n",
      "Iteration 273, loss = 0.03310344\n",
      "Iteration 274, loss = 0.03392488\n",
      "Iteration 275, loss = 0.03304096\n",
      "Iteration 276, loss = 0.03320017\n",
      "Iteration 277, loss = 0.03274599\n",
      "Iteration 278, loss = 0.03292198\n",
      "Iteration 279, loss = 0.03300159\n",
      "Iteration 280, loss = 0.03277176\n",
      "Iteration 281, loss = 0.03288740\n",
      "Iteration 282, loss = 0.03240777\n",
      "Iteration 283, loss = 0.03349554\n",
      "Iteration 284, loss = 0.03301059\n",
      "Iteration 285, loss = 0.03271980\n",
      "Iteration 286, loss = 0.03273149\n",
      "Iteration 287, loss = 0.03208609\n",
      "Iteration 288, loss = 0.03237879\n",
      "Iteration 289, loss = 0.03222502\n",
      "Iteration 290, loss = 0.03255838\n",
      "Iteration 291, loss = 0.03316449\n",
      "Iteration 292, loss = 0.03335250\n",
      "Iteration 293, loss = 0.03261767\n",
      "Iteration 294, loss = 0.03183939\n",
      "Iteration 295, loss = 0.03190359\n",
      "Iteration 296, loss = 0.03214952\n",
      "Iteration 297, loss = 0.03177922\n",
      "Iteration 298, loss = 0.03185490\n",
      "Iteration 299, loss = 0.03172453\n",
      "Iteration 300, loss = 0.03164212\n",
      "Iteration 301, loss = 0.03171543\n",
      "Iteration 302, loss = 0.03146860\n",
      "Iteration 303, loss = 0.03170327\n",
      "Iteration 304, loss = 0.03166006\n",
      "Iteration 305, loss = 0.03177843\n",
      "Iteration 306, loss = 0.03152365\n",
      "Iteration 307, loss = 0.03146652\n",
      "Iteration 308, loss = 0.03147407\n",
      "Iteration 309, loss = 0.03120803\n",
      "Iteration 310, loss = 0.03114134\n",
      "Iteration 311, loss = 0.03134001\n",
      "Iteration 312, loss = 0.03122738\n",
      "Iteration 313, loss = 0.03101212\n",
      "Iteration 314, loss = 0.03104543\n",
      "Iteration 315, loss = 0.03112210\n",
      "Iteration 316, loss = 0.03108395\n",
      "Iteration 317, loss = 0.03089963\n",
      "Iteration 318, loss = 0.03087581\n",
      "Iteration 319, loss = 0.03103975\n",
      "Iteration 320, loss = 0.03115486\n",
      "Iteration 321, loss = 0.03142490\n",
      "Iteration 322, loss = 0.03108940\n",
      "Iteration 323, loss = 0.03057831\n",
      "Iteration 324, loss = 0.03135404\n",
      "Iteration 325, loss = 0.03092015\n",
      "Iteration 326, loss = 0.03053181\n",
      "Iteration 327, loss = 0.03039016\n",
      "Iteration 328, loss = 0.03073075\n",
      "Iteration 329, loss = 0.03039506\n",
      "Iteration 330, loss = 0.03048964\n",
      "Iteration 331, loss = 0.03049680\n",
      "Iteration 332, loss = 0.03034253\n",
      "Iteration 333, loss = 0.03045517\n",
      "Iteration 334, loss = 0.02999151\n",
      "Iteration 335, loss = 0.03028661\n",
      "Iteration 336, loss = 0.03063212\n",
      "Iteration 337, loss = 0.02992940\n",
      "Iteration 338, loss = 0.02995934\n",
      "Iteration 339, loss = 0.03020823\n",
      "Iteration 340, loss = 0.03002208\n",
      "Iteration 341, loss = 0.03017225\n",
      "Iteration 342, loss = 0.02993462\n",
      "Iteration 343, loss = 0.03036648\n",
      "Iteration 344, loss = 0.03007849\n",
      "Iteration 345, loss = 0.02982765\n",
      "Iteration 346, loss = 0.02996716\n",
      "Iteration 347, loss = 0.03003554\n",
      "Iteration 348, loss = 0.02948457\n",
      "Iteration 349, loss = 0.03006958\n",
      "Iteration 350, loss = 0.02995143\n",
      "Iteration 351, loss = 0.02983615\n",
      "Iteration 352, loss = 0.02977278\n",
      "Iteration 353, loss = 0.02951942\n",
      "Iteration 354, loss = 0.02967879\n",
      "Iteration 355, loss = 0.02954664\n",
      "Iteration 356, loss = 0.02949676\n",
      "Iteration 357, loss = 0.02948093\n",
      "Iteration 358, loss = 0.02934599\n",
      "Iteration 359, loss = 0.02932992\n",
      "Iteration 360, loss = 0.02957508\n",
      "Iteration 361, loss = 0.02957743\n",
      "Iteration 362, loss = 0.02910087\n",
      "Iteration 363, loss = 0.02924682\n",
      "Iteration 364, loss = 0.02933841\n",
      "Iteration 365, loss = 0.02909108\n",
      "Iteration 366, loss = 0.02921248\n",
      "Iteration 367, loss = 0.02892069\n",
      "Iteration 368, loss = 0.02933317\n",
      "Iteration 369, loss = 0.02878142\n",
      "Iteration 370, loss = 0.02914927\n",
      "Iteration 371, loss = 0.02884268\n",
      "Iteration 372, loss = 0.02892794\n",
      "Iteration 373, loss = 0.02857408\n",
      "Iteration 374, loss = 0.02882433\n",
      "Iteration 375, loss = 0.02924950\n",
      "Iteration 376, loss = 0.02917909\n",
      "Iteration 377, loss = 0.02886120\n",
      "Iteration 378, loss = 0.02909444\n",
      "Iteration 379, loss = 0.02866946\n",
      "Iteration 380, loss = 0.02900077\n",
      "Iteration 381, loss = 0.02895911\n",
      "Iteration 382, loss = 0.02881237\n",
      "Iteration 383, loss = 0.02863923\n",
      "Iteration 384, loss = 0.02834250\n",
      "Iteration 385, loss = 0.02877220\n",
      "Iteration 386, loss = 0.02854246\n",
      "Iteration 387, loss = 0.02870813\n",
      "Iteration 388, loss = 0.02838604\n",
      "Iteration 389, loss = 0.02901383\n",
      "Iteration 390, loss = 0.02861826\n",
      "Iteration 391, loss = 0.02873169\n",
      "Iteration 392, loss = 0.02823848\n",
      "Iteration 393, loss = 0.02826540\n",
      "Iteration 394, loss = 0.02834714\n",
      "Iteration 395, loss = 0.02846180\n",
      "Iteration 396, loss = 0.02806671\n",
      "Iteration 397, loss = 0.02823097\n",
      "Iteration 398, loss = 0.02803517\n",
      "Iteration 399, loss = 0.02818990\n",
      "Iteration 400, loss = 0.02825892\n",
      "Iteration 401, loss = 0.02798666\n",
      "Iteration 402, loss = 0.02787683\n",
      "Iteration 403, loss = 0.02811589\n",
      "Iteration 404, loss = 0.02814258\n",
      "Iteration 405, loss = 0.02797057\n",
      "Iteration 406, loss = 0.02782985\n",
      "Iteration 407, loss = 0.02799410\n",
      "Iteration 408, loss = 0.02795777\n",
      "Iteration 409, loss = 0.02809106\n",
      "Iteration 410, loss = 0.02818696\n",
      "Iteration 411, loss = 0.02764230\n",
      "Iteration 412, loss = 0.02787536\n",
      "Iteration 413, loss = 0.02845652\n",
      "Iteration 414, loss = 0.02774311\n",
      "Iteration 415, loss = 0.02784579\n",
      "Iteration 416, loss = 0.02834948\n",
      "Iteration 417, loss = 0.02838764\n",
      "Iteration 418, loss = 0.02773476\n",
      "Iteration 419, loss = 0.02757779\n",
      "Iteration 420, loss = 0.02814083\n",
      "Iteration 421, loss = 0.02780699\n",
      "Iteration 422, loss = 0.02780042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 92.11746522411129\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 90.57187017001546 %\n",
      "Fold 1: 92.89026275115918 %\n",
      "Fold 2: 90.03091190108191 %\n",
      "Fold 3: 92.50386398763524 %\n",
      "Fold 4: 92.11746522411129 %\n",
      "Average: 91.62287480680062 %\n",
      "Iteration 1, loss = 0.69185110\n",
      "Iteration 2, loss = 0.68562951\n",
      "Iteration 3, loss = 0.67691527\n",
      "Iteration 4, loss = 0.66335327\n",
      "Iteration 5, loss = 0.64516733\n",
      "Iteration 6, loss = 0.62285181\n",
      "Iteration 7, loss = 0.59680025\n",
      "Iteration 8, loss = 0.56770157\n",
      "Iteration 9, loss = 0.53825349\n",
      "Iteration 10, loss = 0.50675220\n",
      "Iteration 11, loss = 0.47645134\n",
      "Iteration 12, loss = 0.44670486\n",
      "Iteration 13, loss = 0.41871156\n",
      "Iteration 14, loss = 0.39308413\n",
      "Iteration 15, loss = 0.36825869\n",
      "Iteration 16, loss = 0.34672062\n",
      "Iteration 17, loss = 0.32611849\n",
      "Iteration 18, loss = 0.30783623\n",
      "Iteration 19, loss = 0.29095443\n",
      "Iteration 20, loss = 0.27588110\n",
      "Iteration 21, loss = 0.26247610\n",
      "Iteration 22, loss = 0.25019281\n",
      "Iteration 23, loss = 0.23881412\n",
      "Iteration 24, loss = 0.22840626\n",
      "Iteration 25, loss = 0.21951531\n",
      "Iteration 26, loss = 0.21030844\n",
      "Iteration 27, loss = 0.20229282\n",
      "Iteration 28, loss = 0.19526890\n",
      "Iteration 29, loss = 0.18863898\n",
      "Iteration 30, loss = 0.18212850\n",
      "Iteration 31, loss = 0.17669373\n",
      "Iteration 32, loss = 0.17105234\n",
      "Iteration 33, loss = 0.16635187\n",
      "Iteration 34, loss = 0.16149741\n",
      "Iteration 35, loss = 0.15712576\n",
      "Iteration 36, loss = 0.15264050\n",
      "Iteration 37, loss = 0.14908411\n",
      "Iteration 38, loss = 0.14512226\n",
      "Iteration 39, loss = 0.14182392\n",
      "Iteration 40, loss = 0.13854477\n",
      "Iteration 41, loss = 0.13504726\n",
      "Iteration 42, loss = 0.13239342\n",
      "Iteration 43, loss = 0.12935631\n",
      "Iteration 44, loss = 0.12682870\n",
      "Iteration 45, loss = 0.12432768\n",
      "Iteration 46, loss = 0.12153680\n",
      "Iteration 47, loss = 0.11927340\n",
      "Iteration 48, loss = 0.11739544\n",
      "Iteration 49, loss = 0.11477746\n",
      "Iteration 50, loss = 0.11259644\n",
      "Iteration 51, loss = 0.11064368\n",
      "Iteration 52, loss = 0.10860334\n",
      "Iteration 53, loss = 0.10681832\n",
      "Iteration 54, loss = 0.10519913\n",
      "Iteration 55, loss = 0.10357246\n",
      "Iteration 56, loss = 0.10166466\n",
      "Iteration 57, loss = 0.09999290\n",
      "Iteration 58, loss = 0.09862633\n",
      "Iteration 59, loss = 0.09713911\n",
      "Iteration 60, loss = 0.09551212\n",
      "Iteration 61, loss = 0.09447802\n",
      "Iteration 62, loss = 0.09296444\n",
      "Iteration 63, loss = 0.09159852\n",
      "Iteration 64, loss = 0.09073423\n",
      "Iteration 65, loss = 0.08965275\n",
      "Iteration 66, loss = 0.08801889\n",
      "Iteration 67, loss = 0.08706286\n",
      "Iteration 68, loss = 0.08592853\n",
      "Iteration 69, loss = 0.08489744\n",
      "Iteration 70, loss = 0.08344065\n",
      "Iteration 71, loss = 0.08246802\n",
      "Iteration 72, loss = 0.08182183\n",
      "Iteration 73, loss = 0.08058117\n",
      "Iteration 74, loss = 0.07987344\n",
      "Iteration 75, loss = 0.07878869\n",
      "Iteration 76, loss = 0.07777856\n",
      "Iteration 77, loss = 0.07701327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78, loss = 0.07614571\n",
      "Iteration 79, loss = 0.07543463\n",
      "Iteration 80, loss = 0.07447994\n",
      "Iteration 81, loss = 0.07380048\n",
      "Iteration 82, loss = 0.07313302\n",
      "Iteration 83, loss = 0.07226238\n",
      "Iteration 84, loss = 0.07132014\n",
      "Iteration 85, loss = 0.07096714\n",
      "Iteration 86, loss = 0.06992939\n",
      "Iteration 87, loss = 0.06970840\n",
      "Iteration 88, loss = 0.06878990\n",
      "Iteration 89, loss = 0.06812351\n",
      "Iteration 90, loss = 0.06778631\n",
      "Iteration 91, loss = 0.06751325\n",
      "Iteration 92, loss = 0.06678062\n",
      "Iteration 93, loss = 0.06568221\n",
      "Iteration 94, loss = 0.06510103\n",
      "Iteration 95, loss = 0.06469583\n",
      "Iteration 96, loss = 0.06404568\n",
      "Iteration 97, loss = 0.06337619\n",
      "Iteration 98, loss = 0.06289101\n",
      "Iteration 99, loss = 0.06246061\n",
      "Iteration 100, loss = 0.06202081\n",
      "Iteration 101, loss = 0.06150372\n",
      "Iteration 102, loss = 0.06098353\n",
      "Iteration 103, loss = 0.06095469\n",
      "Iteration 104, loss = 0.06026510\n",
      "Iteration 105, loss = 0.05993029\n",
      "Iteration 106, loss = 0.05936500\n",
      "Iteration 107, loss = 0.05874390\n",
      "Iteration 108, loss = 0.05823768\n",
      "Iteration 109, loss = 0.05794518\n",
      "Iteration 110, loss = 0.05743984\n",
      "Iteration 111, loss = 0.05709230\n",
      "Iteration 112, loss = 0.05662943\n",
      "Iteration 113, loss = 0.05644948\n",
      "Iteration 114, loss = 0.05614640\n",
      "Iteration 115, loss = 0.05593271\n",
      "Iteration 116, loss = 0.05551891\n",
      "Iteration 117, loss = 0.05484881\n",
      "Iteration 118, loss = 0.05442349\n",
      "Iteration 119, loss = 0.05413177\n",
      "Iteration 120, loss = 0.05382186\n",
      "Iteration 121, loss = 0.05342830\n",
      "Iteration 122, loss = 0.05341009\n",
      "Iteration 123, loss = 0.05294381\n",
      "Iteration 124, loss = 0.05245520\n",
      "Iteration 125, loss = 0.05238544\n",
      "Iteration 126, loss = 0.05206250\n",
      "Iteration 127, loss = 0.05171406\n",
      "Iteration 128, loss = 0.05134899\n",
      "Iteration 129, loss = 0.05134050\n",
      "Iteration 130, loss = 0.05114725\n",
      "Iteration 131, loss = 0.05058862\n",
      "Iteration 132, loss = 0.05013259\n",
      "Iteration 133, loss = 0.04982528\n",
      "Iteration 134, loss = 0.05009241\n",
      "Iteration 135, loss = 0.04953659\n",
      "Iteration 136, loss = 0.04915655\n",
      "Iteration 137, loss = 0.04884253\n",
      "Iteration 138, loss = 0.04848733\n",
      "Iteration 139, loss = 0.04876444\n",
      "Iteration 140, loss = 0.04804848\n",
      "Iteration 141, loss = 0.04789740\n",
      "Iteration 142, loss = 0.04795497\n",
      "Iteration 143, loss = 0.04752636\n",
      "Iteration 144, loss = 0.04737326\n",
      "Iteration 145, loss = 0.04704448\n",
      "Iteration 146, loss = 0.04738929\n",
      "Iteration 147, loss = 0.04666885\n",
      "Iteration 148, loss = 0.04648072\n",
      "Iteration 149, loss = 0.04677733\n",
      "Iteration 150, loss = 0.04621753\n",
      "Iteration 151, loss = 0.04580473\n",
      "Iteration 152, loss = 0.04559781\n",
      "Iteration 153, loss = 0.04559204\n",
      "Iteration 154, loss = 0.04523947\n",
      "Iteration 155, loss = 0.04532645\n",
      "Iteration 156, loss = 0.04469935\n",
      "Iteration 157, loss = 0.04464317\n",
      "Iteration 158, loss = 0.04434353\n",
      "Iteration 159, loss = 0.04419629\n",
      "Iteration 160, loss = 0.04411248\n",
      "Iteration 161, loss = 0.04402096\n",
      "Iteration 162, loss = 0.04374235\n",
      "Iteration 163, loss = 0.04355899\n",
      "Iteration 164, loss = 0.04362180\n",
      "Iteration 165, loss = 0.04324430\n",
      "Iteration 166, loss = 0.04319657\n",
      "Iteration 167, loss = 0.04290617\n",
      "Iteration 168, loss = 0.04292803\n",
      "Iteration 169, loss = 0.04253077\n",
      "Iteration 170, loss = 0.04275162\n",
      "Iteration 171, loss = 0.04258735\n",
      "Iteration 172, loss = 0.04216723\n",
      "Iteration 173, loss = 0.04217528\n",
      "Iteration 174, loss = 0.04222921\n",
      "Iteration 175, loss = 0.04191591\n",
      "Iteration 176, loss = 0.04160978\n",
      "Iteration 177, loss = 0.04167221\n",
      "Iteration 178, loss = 0.04110982\n",
      "Iteration 179, loss = 0.04126201\n",
      "Iteration 180, loss = 0.04108845\n",
      "Iteration 181, loss = 0.04079489\n",
      "Iteration 182, loss = 0.04074086\n",
      "Iteration 183, loss = 0.04077037\n",
      "Iteration 184, loss = 0.04045346\n",
      "Iteration 185, loss = 0.04093919\n",
      "Iteration 186, loss = 0.04032997\n",
      "Iteration 187, loss = 0.04049620\n",
      "Iteration 188, loss = 0.04007493\n",
      "Iteration 189, loss = 0.03988729\n",
      "Iteration 190, loss = 0.04015169\n",
      "Iteration 191, loss = 0.03975068\n",
      "Iteration 192, loss = 0.03979479\n",
      "Iteration 193, loss = 0.03968653\n",
      "Iteration 194, loss = 0.03931045\n",
      "Iteration 195, loss = 0.03934019\n",
      "Iteration 196, loss = 0.03917886\n",
      "Iteration 197, loss = 0.03942089\n",
      "Iteration 198, loss = 0.03925156\n",
      "Iteration 199, loss = 0.03918738\n",
      "Iteration 200, loss = 0.03904372\n",
      "Iteration 201, loss = 0.03889191\n",
      "Iteration 202, loss = 0.03846897\n",
      "Iteration 203, loss = 0.03841978\n",
      "Iteration 204, loss = 0.03824362\n",
      "Iteration 205, loss = 0.03830675\n",
      "Iteration 206, loss = 0.03809607\n",
      "Iteration 207, loss = 0.03798747\n",
      "Iteration 208, loss = 0.03808899\n",
      "Iteration 209, loss = 0.03783297\n",
      "Iteration 210, loss = 0.03795110\n",
      "Iteration 211, loss = 0.03766692\n",
      "Iteration 212, loss = 0.03740853\n",
      "Iteration 213, loss = 0.03769750\n",
      "Iteration 214, loss = 0.03759323\n",
      "Iteration 215, loss = 0.03723672\n",
      "Iteration 216, loss = 0.03707708\n",
      "Iteration 217, loss = 0.03696196\n",
      "Iteration 218, loss = 0.03689361\n",
      "Iteration 219, loss = 0.03704518\n",
      "Iteration 220, loss = 0.03695088\n",
      "Iteration 221, loss = 0.03676061\n",
      "Iteration 222, loss = 0.03682454\n",
      "Iteration 223, loss = 0.03705437\n",
      "Iteration 224, loss = 0.03657889\n",
      "Iteration 225, loss = 0.03664803\n",
      "Iteration 226, loss = 0.03653878\n",
      "Iteration 227, loss = 0.03633669\n",
      "Iteration 228, loss = 0.03614411\n",
      "Iteration 229, loss = 0.03611959\n",
      "Iteration 230, loss = 0.03591780\n",
      "Iteration 231, loss = 0.03611025\n",
      "Iteration 232, loss = 0.03607854\n",
      "Iteration 233, loss = 0.03568097\n",
      "Iteration 234, loss = 0.03588827\n",
      "Iteration 235, loss = 0.03571907\n",
      "Iteration 236, loss = 0.03542416\n",
      "Iteration 237, loss = 0.03554082\n",
      "Iteration 238, loss = 0.03549867\n",
      "Iteration 239, loss = 0.03515807\n",
      "Iteration 240, loss = 0.03512799\n",
      "Iteration 241, loss = 0.03528831\n",
      "Iteration 242, loss = 0.03523825\n",
      "Iteration 243, loss = 0.03577354\n",
      "Iteration 244, loss = 0.03482854\n",
      "Iteration 245, loss = 0.03472608\n",
      "Iteration 246, loss = 0.03494532\n",
      "Iteration 247, loss = 0.03476219\n",
      "Iteration 248, loss = 0.03494913\n",
      "Iteration 249, loss = 0.03519871\n",
      "Iteration 250, loss = 0.03455916\n",
      "Iteration 251, loss = 0.03471448\n",
      "Iteration 252, loss = 0.03497778\n",
      "Iteration 253, loss = 0.03434969\n",
      "Iteration 254, loss = 0.03440695\n",
      "Iteration 255, loss = 0.03466315\n",
      "Iteration 256, loss = 0.03416439\n",
      "Iteration 257, loss = 0.03436487\n",
      "Iteration 258, loss = 0.03416440\n",
      "Iteration 259, loss = 0.03469373\n",
      "Iteration 260, loss = 0.03401067\n",
      "Iteration 261, loss = 0.03374391\n",
      "Iteration 262, loss = 0.03455487\n",
      "Iteration 263, loss = 0.03402102\n",
      "Iteration 264, loss = 0.03382702\n",
      "Iteration 265, loss = 0.03402629\n",
      "Iteration 266, loss = 0.03457082\n",
      "Iteration 267, loss = 0.03400569\n",
      "Iteration 268, loss = 0.03366000\n",
      "Iteration 269, loss = 0.03378886\n",
      "Iteration 270, loss = 0.03360598\n",
      "Iteration 271, loss = 0.03382432\n",
      "Iteration 272, loss = 0.03435270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy:  0.8930778739184178\n",
      "F1-Score:  0.8965929468021516\n",
      "Precision:  0.8771929824561403\n",
      "Recall:  0.9168704156479217\n",
      "AUC:  0.8928102078239608\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_mlp(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "clf=MLPClassifier(random_state=102, max_iter=3000, verbose=True).fit(X_Train_Transformed_FeatureMap, Y_Train_FeatureMap.ravel())\n",
    "y_pred=clf.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0d26f",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9953869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 81.60741885625966\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 83.84853168469861\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 82.61205564142195\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 79.67542503863989\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 83.0757341576507\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 81.60741885625966 %\n",
      "Fold 1: 83.84853168469861 %\n",
      "Fold 2: 82.61205564142195 %\n",
      "Fold 3: 79.67542503863989 %\n",
      "Fold 4: 83.0757341576507 %\n",
      "Average: 82.16383307573416 %\n",
      "Accuracy:  0.8059332509270705\n",
      "F1-Score:  0.8047263681592038\n",
      "Precision:  0.8189873417721519\n",
      "Recall:  0.7909535452322738\n",
      "AUC:  0.8061017726161369\n"
     ]
    }
   ],
   "source": [
    "random_forest=k_fold_cv_rforest(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "random_forest = RandomForestClassifier(n_estimators=100,criterion='gini',random_state=102)\n",
    "random_forest = random_forest.fit(X_Train_Transformed_FeatureMap, Y_Train_FeatureMap.ravel())\n",
    "y_pred=random_forest.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

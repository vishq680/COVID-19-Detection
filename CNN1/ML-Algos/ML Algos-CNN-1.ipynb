{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea81c203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import copy\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0045d3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd360b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['Covid','No Covid']\n",
    "num_classes=2\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8190008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=\"./data\",transform=transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Resize([227,227]),\n",
    "#                                                             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "                                                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a588543d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8088\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "trainset,testset=torch.utils.data.random_split(dataset,[round(0.8*len(dataset)),round(0.2*len(dataset))],generator=torch.Generator().manual_seed(42))\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0bb4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def get_first_FC_Layer(self,x):\n",
    "            x=self.representation_network(x).flatten(1)\n",
    "            x=self.classification_network[0](x)\n",
    "            return x;\n",
    "    def get_Representation_Net(self,x):\n",
    "            x=self.representation_network(x).flatten(1)\n",
    "            return  x;\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        self.representation_network=nn.Sequential(\n",
    "            nn.Conv2d(3,32,3), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "            nn.Conv2d(32,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "        )\n",
    "        self.classification_network=nn.Sequential(\n",
    "            nn.Linear(18432,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        x=self.representation_network(x)\n",
    "#         print(x.shape)\n",
    "        # flattening of the vector=> same dimension of first index(batch size) , everythign else is flattened(-1)\n",
    "        x=x.view(x.size(0),-1)\n",
    "#         print(x.shape)\n",
    "        x=self.classification_network(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5daef9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CNN()\n",
    "net.load_state_dict(torch.load(\"CNN-1_final.pth\").state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b34ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataloader,model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total,correct=0,0\n",
    "        for data in dataloader:\n",
    "            inputs,labels=data\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "            outputs=model(inputs)\n",
    "    #         print(outputs)\n",
    "    #         print(outputs,labels)\n",
    "            m = nn.Sigmoid()\n",
    "            outputs=m(outputs)\n",
    "            pred=outputs>=0.5\n",
    "            pred=pred.flatten()\n",
    "            total+=labels.size(0)\n",
    "            # labels=torch.add(labels,-1)\n",
    "            # print(pred,labels)\n",
    "    #         print(list(map(lambda a: classes[a],pred)),list(map(lambda a: classes[a],labels)))\n",
    "            correct+=(pred==labels).sum().item()\n",
    "    print(correct,total)\n",
    "    model.train()\n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8398b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 512)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n",
      "Done with the batch: 332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "Done with the batch: 405\n",
      "Done with the batch: 406\n",
      "Done with the batch: 407\n",
      "Done with the batch: 408\n",
      "Done with the batch: 409\n",
      "Done with the batch: 410\n",
      "Done with the batch: 411\n",
      "Done with the batch: 412\n",
      "Done with the batch: 413\n",
      "Done with the batch: 414\n",
      "Done with the batch: 415\n",
      "Done with the batch: 416\n",
      "Done with the batch: 417\n",
      "Done with the batch: 418\n",
      "Done with the batch: 419\n",
      "Done with the batch: 420\n",
      "Done with the batch: 421\n",
      "Done with the batch: 422\n",
      "Done with the batch: 423\n",
      "Done with the batch: 424\n",
      "Done with the batch: 425\n",
      "Done with the batch: 426\n",
      "Done with the batch: 427\n",
      "Done with the batch: 428\n",
      "Done with the batch: 429\n",
      "Done with the batch: 430\n",
      "Done with the batch: 431\n",
      "Done with the batch: 432\n",
      "Done with the batch: 433\n",
      "Done with the batch: 434\n",
      "Done with the batch: 435\n",
      "Done with the batch: 436\n",
      "Done with the batch: 437\n",
      "Done with the batch: 438\n",
      "Done with the batch: 439\n",
      "Done with the batch: 440\n",
      "Done with the batch: 441\n",
      "Done with the batch: 442\n",
      "Done with the batch: 443\n",
      "Done with the batch: 444\n",
      "Done with the batch: 445\n",
      "Done with the batch: 446\n",
      "Done with the batch: 447\n",
      "Done with the batch: 448\n",
      "Done with the batch: 449\n",
      "Done with the batch: 450\n",
      "Done with the batch: 451\n",
      "Done with the batch: 452\n",
      "Done with the batch: 453\n",
      "Done with the batch: 454\n",
      "Done with the batch: 455\n",
      "Done with the batch: 456\n",
      "Done with the batch: 457\n",
      "Done with the batch: 458\n",
      "Done with the batch: 459\n",
      "Done with the batch: 460\n",
      "Done with the batch: 461\n",
      "Done with the batch: 462\n",
      "Done with the batch: 463\n",
      "Done with the batch: 464\n",
      "Done with the batch: 465\n",
      "Done with the batch: 466\n",
      "Done with the batch: 467\n",
      "Done with the batch: 468\n",
      "Done with the batch: 469\n",
      "Done with the batch: 470\n",
      "Done with the batch: 471\n",
      "Done with the batch: 472\n",
      "Done with the batch: 473\n",
      "Done with the batch: 474\n",
      "Done with the batch: 475\n",
      "Done with the batch: 476\n",
      "Done with the batch: 477\n",
      "Done with the batch: 478\n",
      "Done with the batch: 479\n",
      "Done with the batch: 480\n",
      "Done with the batch: 481\n",
      "Done with the batch: 482\n",
      "Done with the batch: 483\n",
      "Done with the batch: 484\n",
      "Done with the batch: 485\n",
      "Done with the batch: 486\n",
      "Done with the batch: 487\n",
      "Done with the batch: 488\n",
      "Done with the batch: 489\n",
      "Done with the batch: 490\n",
      "Done with the batch: 491\n",
      "Done with the batch: 492\n",
      "Done with the batch: 493\n",
      "Done with the batch: 494\n",
      "Done with the batch: 495\n",
      "Done with the batch: 496\n",
      "Done with the batch: 497\n",
      "Done with the batch: 498\n",
      "Done with the batch: 499\n",
      "Done with the batch: 500\n",
      "Done with the batch: 501\n",
      "Done with the batch: 502\n",
      "Done with the batch: 503\n",
      "Done with the batch: 504\n",
      "Done with the batch: 505\n",
      "Done with the batch: 506\n",
      "Done with the batch: 507\n",
      "Done with the batch: 508\n",
      "Done with the batch: 509\n",
      "Done with the batch: 510\n",
      "Done with the batch: 511\n",
      "Done with the batch: 512\n",
      "Done with the batch: 513\n",
      "Done with the batch: 514\n",
      "Done with the batch: 515\n",
      "Done with the batch: 516\n",
      "Done with the batch: 517\n",
      "Done with the batch: 518\n",
      "Done with the batch: 519\n",
      "Done with the batch: 520\n",
      "Done with the batch: 521\n",
      "Done with the batch: 522\n",
      "Done with the batch: 523\n",
      "Done with the batch: 524\n",
      "Done with the batch: 525\n",
      "Done with the batch: 526\n",
      "Done with the batch: 527\n",
      "Done with the batch: 528\n",
      "Done with the batch: 529\n",
      "Done with the batch: 530\n",
      "Done with the batch: 531\n",
      "Done with the batch: 532\n",
      "Done with the batch: 533\n",
      "Done with the batch: 534\n",
      "Done with the batch: 535\n",
      "Done with the batch: 536\n",
      "Done with the batch: 537\n",
      "Done with the batch: 538\n",
      "Done with the batch: 539\n",
      "Done with the batch: 540\n",
      "Done with the batch: 541\n",
      "Done with the batch: 542\n",
      "Done with the batch: 543\n",
      "Done with the batch: 544\n",
      "Done with the batch: 545\n",
      "Done with the batch: 546\n",
      "Done with the batch: 547\n",
      "Done with the batch: 548\n",
      "Done with the batch: 549\n",
      "Done with the batch: 550\n",
      "Done with the batch: 551\n",
      "Done with the batch: 552\n",
      "Done with the batch: 553\n",
      "Done with the batch: 554\n",
      "Done with the batch: 555\n",
      "Done with the batch: 556\n",
      "Done with the batch: 557\n",
      "Done with the batch: 558\n",
      "Done with the batch: 559\n",
      "Done with the batch: 560\n",
      "Done with the batch: 561\n",
      "Done with the batch: 562\n",
      "Done with the batch: 563\n",
      "Done with the batch: 564\n",
      "Done with the batch: 565\n",
      "Done with the batch: 566\n",
      "Done with the batch: 567\n",
      "Done with the batch: 568\n",
      "Done with the batch: 569\n",
      "Done with the batch: 570\n",
      "Done with the batch: 571\n",
      "Done with the batch: 572\n",
      "Done with the batch: 573\n",
      "Done with the batch: 574\n",
      "Done with the batch: 575\n",
      "Done with the batch: 576\n",
      "Done with the batch: 577\n",
      "Done with the batch: 578\n",
      "Done with the batch: 579\n",
      "Done with the batch: 580\n",
      "Done with the batch: 581\n",
      "Done with the batch: 582\n",
      "Done with the batch: 583\n",
      "Done with the batch: 584\n",
      "Done with the batch: 585\n",
      "Done with the batch: 586\n",
      "Done with the batch: 587\n",
      "Done with the batch: 588\n",
      "Done with the batch: 589\n",
      "Done with the batch: 590\n",
      "Done with the batch: 591\n",
      "Done with the batch: 592\n",
      "Done with the batch: 593\n",
      "Done with the batch: 594\n",
      "Done with the batch: 595\n",
      "Done with the batch: 596\n",
      "Done with the batch: 597\n",
      "Done with the batch: 598\n",
      "Done with the batch: 599\n",
      "Done with the batch: 600\n",
      "Done with the batch: 601\n",
      "Done with the batch: 602\n",
      "Done with the batch: 603\n",
      "Done with the batch: 604\n",
      "Done with the batch: 605\n",
      "Done with the batch: 606\n",
      "Done with the batch: 607\n",
      "Done with the batch: 608\n",
      "Done with the batch: 609\n",
      "Done with the batch: 610\n",
      "Done with the batch: 611\n",
      "Done with the batch: 612\n",
      "Done with the batch: 613\n",
      "Done with the batch: 614\n",
      "Done with the batch: 615\n",
      "Done with the batch: 616\n",
      "Done with the batch: 617\n",
      "Done with the batch: 618\n",
      "Done with the batch: 619\n",
      "Done with the batch: 620\n",
      "Done with the batch: 621\n",
      "Done with the batch: 622\n",
      "Done with the batch: 623\n",
      "Done with the batch: 624\n",
      "Done with the batch: 625\n",
      "Done with the batch: 626\n",
      "Done with the batch: 627\n",
      "Done with the batch: 628\n",
      "Done with the batch: 629\n",
      "Done with the batch: 630\n",
      "Done with the batch: 631\n",
      "Done with the batch: 632\n",
      "Done with the batch: 633\n",
      "Done with the batch: 634\n",
      "Done with the batch: 635\n",
      "Done with the batch: 636\n",
      "Done with the batch: 637\n",
      "Done with the batch: 638\n",
      "Done with the batch: 639\n",
      "Done with the batch: 640\n",
      "Done with the batch: 641\n",
      "Done with the batch: 642\n",
      "Done with the batch: 643\n",
      "Done with the batch: 644\n",
      "Done with the batch: 645\n",
      "Done with the batch: 646\n",
      "Done with the batch: 647\n",
      "Done with the batch: 648\n",
      "Done with the batch: 649\n",
      "Done with the batch: 650\n",
      "Done with the batch: 651\n",
      "Done with the batch: 652\n",
      "Done with the batch: 653\n",
      "Done with the batch: 654\n",
      "Done with the batch: 655\n",
      "Done with the batch: 656\n",
      "Done with the batch: 657\n",
      "Done with the batch: 658\n",
      "Done with the batch: 659\n",
      "Done with the batch: 660\n",
      "Done with the batch: 661\n",
      "Done with the batch: 662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 663\n",
      "Done with the batch: 664\n",
      "Done with the batch: 665\n",
      "Done with the batch: 666\n",
      "Done with the batch: 667\n",
      "Done with the batch: 668\n",
      "Done with the batch: 669\n",
      "Done with the batch: 670\n",
      "Done with the batch: 671\n",
      "Done with the batch: 672\n",
      "Done with the batch: 673\n",
      "Done with the batch: 674\n",
      "Done with the batch: 675\n",
      "Done with the batch: 676\n",
      "Done with the batch: 677\n",
      "Done with the batch: 678\n",
      "Done with the batch: 679\n",
      "Done with the batch: 680\n",
      "Done with the batch: 681\n",
      "Done with the batch: 682\n",
      "Done with the batch: 683\n",
      "Done with the batch: 684\n",
      "Done with the batch: 685\n",
      "Done with the batch: 686\n",
      "Done with the batch: 687\n",
      "Done with the batch: 688\n",
      "Done with the batch: 689\n",
      "Done with the batch: 690\n",
      "Done with the batch: 691\n",
      "Done with the batch: 692\n",
      "Done with the batch: 693\n",
      "Done with the batch: 694\n",
      "Done with the batch: 695\n",
      "Done with the batch: 696\n",
      "Done with the batch: 697\n",
      "Done with the batch: 698\n",
      "Done with the batch: 699\n",
      "Done with the batch: 700\n",
      "Done with the batch: 701\n",
      "Done with the batch: 702\n",
      "Done with the batch: 703\n",
      "Done with the batch: 704\n",
      "Done with the batch: 705\n",
      "Done with the batch: 706\n",
      "Done with the batch: 707\n",
      "Done with the batch: 708\n",
      "Done with the batch: 709\n",
      "Done with the batch: 710\n",
      "Done with the batch: 711\n",
      "Done with the batch: 712\n",
      "Done with the batch: 713\n",
      "Done with the batch: 714\n",
      "Done with the batch: 715\n",
      "Done with the batch: 716\n",
      "Done with the batch: 717\n",
      "Done with the batch: 718\n",
      "Done with the batch: 719\n",
      "Done with the batch: 720\n",
      "Done with the batch: 721\n",
      "Done with the batch: 722\n",
      "Done with the batch: 723\n",
      "Done with the batch: 724\n",
      "Done with the batch: 725\n",
      "Done with the batch: 726\n",
      "Done with the batch: 727\n",
      "Done with the batch: 728\n",
      "Done with the batch: 729\n",
      "Done with the batch: 730\n",
      "Done with the batch: 731\n",
      "Done with the batch: 732\n",
      "Done with the batch: 733\n",
      "Done with the batch: 734\n",
      "Done with the batch: 735\n",
      "Done with the batch: 736\n",
      "Done with the batch: 737\n",
      "Done with the batch: 738\n",
      "Done with the batch: 739\n",
      "Done with the batch: 740\n",
      "Done with the batch: 741\n",
      "Done with the batch: 742\n",
      "Done with the batch: 743\n",
      "Done with the batch: 744\n",
      "Done with the batch: 745\n",
      "Done with the batch: 746\n",
      "Done with the batch: 747\n",
      "Done with the batch: 748\n",
      "Done with the batch: 749\n",
      "Done with the batch: 750\n",
      "Done with the batch: 751\n",
      "Done with the batch: 752\n",
      "Done with the batch: 753\n",
      "Done with the batch: 754\n",
      "Done with the batch: 755\n",
      "Done with the batch: 756\n",
      "Done with the batch: 757\n",
      "Done with the batch: 758\n",
      "Done with the batch: 759\n",
      "Done with the batch: 760\n",
      "Done with the batch: 761\n",
      "Done with the batch: 762\n",
      "Done with the batch: 763\n",
      "Done with the batch: 764\n",
      "Done with the batch: 765\n",
      "Done with the batch: 766\n",
      "Done with the batch: 767\n",
      "Done with the batch: 768\n",
      "Done with the batch: 769\n",
      "Done with the batch: 770\n",
      "Done with the batch: 771\n",
      "Done with the batch: 772\n",
      "Done with the batch: 773\n",
      "Done with the batch: 774\n",
      "Done with the batch: 775\n",
      "Done with the batch: 776\n",
      "Done with the batch: 777\n",
      "Done with the batch: 778\n",
      "Done with the batch: 779\n",
      "Done with the batch: 780\n",
      "Done with the batch: 781\n",
      "Done with the batch: 782\n",
      "Done with the batch: 783\n",
      "Done with the batch: 784\n",
      "Done with the batch: 785\n",
      "Done with the batch: 786\n",
      "Done with the batch: 787\n",
      "Done with the batch: 788\n",
      "Done with the batch: 789\n",
      "Done with the batch: 790\n",
      "Done with the batch: 791\n",
      "Done with the batch: 792\n",
      "Done with the batch: 793\n",
      "Done with the batch: 794\n",
      "Done with the batch: 795\n",
      "Done with the batch: 796\n",
      "Done with the batch: 797\n",
      "Done with the batch: 798\n",
      "Done with the batch: 799\n",
      "Done with the batch: 800\n",
      "Done with the batch: 801\n",
      "Done with the batch: 802\n",
      "Done with the batch: 803\n",
      "Done with the batch: 804\n",
      "Done with the batch: 805\n",
      "Done with the batch: 806\n",
      "Done with the batch: 807\n",
      "Done with the batch: 808\n",
      "Done with the batch: 809\n",
      "Done with the batch: 810\n",
      "Done with the batch: 811\n",
      "Done with the batch: 812\n",
      "Done with the batch: 813\n",
      "Done with the batch: 814\n",
      "Done with the batch: 815\n",
      "Done with the batch: 816\n",
      "Done with the batch: 817\n",
      "Done with the batch: 818\n",
      "Done with the batch: 819\n",
      "Done with the batch: 820\n",
      "Done with the batch: 821\n",
      "Done with the batch: 822\n",
      "Done with the batch: 823\n",
      "Done with the batch: 824\n",
      "Done with the batch: 825\n",
      "Done with the batch: 826\n",
      "Done with the batch: 827\n",
      "Done with the batch: 828\n",
      "Done with the batch: 829\n",
      "Done with the batch: 830\n",
      "Done with the batch: 831\n",
      "Done with the batch: 832\n",
      "Done with the batch: 833\n",
      "Done with the batch: 834\n",
      "Done with the batch: 835\n",
      "Done with the batch: 836\n",
      "Done with the batch: 837\n",
      "Done with the batch: 838\n",
      "Done with the batch: 839\n",
      "Done with the batch: 840\n",
      "Done with the batch: 841\n",
      "Done with the batch: 842\n",
      "Done with the batch: 843\n",
      "Done with the batch: 844\n",
      "Done with the batch: 845\n",
      "Done with the batch: 846\n",
      "Done with the batch: 847\n",
      "Done with the batch: 848\n",
      "Done with the batch: 849\n",
      "Done with the batch: 850\n",
      "Done with the batch: 851\n",
      "Done with the batch: 852\n",
      "Done with the batch: 853\n",
      "Done with the batch: 854\n",
      "Done with the batch: 855\n",
      "Done with the batch: 856\n",
      "Done with the batch: 857\n",
      "Done with the batch: 858\n",
      "Done with the batch: 859\n",
      "Done with the batch: 860\n",
      "Done with the batch: 861\n",
      "Done with the batch: 862\n",
      "Done with the batch: 863\n",
      "Done with the batch: 864\n",
      "Done with the batch: 865\n",
      "Done with the batch: 866\n",
      "Done with the batch: 867\n",
      "Done with the batch: 868\n",
      "Done with the batch: 869\n",
      "Done with the batch: 870\n",
      "Done with the batch: 871\n",
      "Done with the batch: 872\n",
      "Done with the batch: 873\n",
      "Done with the batch: 874\n",
      "Done with the batch: 875\n",
      "Done with the batch: 876\n",
      "Done with the batch: 877\n",
      "Done with the batch: 878\n",
      "Done with the batch: 879\n",
      "Done with the batch: 880\n",
      "Done with the batch: 881\n",
      "Done with the batch: 882\n",
      "Done with the batch: 883\n",
      "Done with the batch: 884\n",
      "Done with the batch: 885\n",
      "Done with the batch: 886\n",
      "Done with the batch: 887\n",
      "Done with the batch: 888\n",
      "Done with the batch: 889\n",
      "Done with the batch: 890\n",
      "Done with the batch: 891\n",
      "Done with the batch: 892\n",
      "Done with the batch: 893\n",
      "Done with the batch: 894\n",
      "Done with the batch: 895\n",
      "Done with the batch: 896\n",
      "Done with the batch: 897\n",
      "Done with the batch: 898\n",
      "Done with the batch: 899\n",
      "Done with the batch: 900\n",
      "Done with the batch: 901\n",
      "Done with the batch: 902\n",
      "Done with the batch: 903\n",
      "Done with the batch: 904\n",
      "Done with the batch: 905\n",
      "Done with the batch: 906\n",
      "Done with the batch: 907\n",
      "Done with the batch: 908\n",
      "Done with the batch: 909\n",
      "Done with the batch: 910\n",
      "Done with the batch: 911\n",
      "Done with the batch: 912\n",
      "Done with the batch: 913\n",
      "Done with the batch: 914\n",
      "Done with the batch: 915\n",
      "Done with the batch: 916\n",
      "Done with the batch: 917\n",
      "Done with the batch: 918\n",
      "Done with the batch: 919\n",
      "Done with the batch: 920\n",
      "Done with the batch: 921\n",
      "Done with the batch: 922\n",
      "Done with the batch: 923\n",
      "Done with the batch: 924\n",
      "Done with the batch: 925\n",
      "Done with the batch: 926\n",
      "Done with the batch: 927\n",
      "Done with the batch: 928\n",
      "Done with the batch: 929\n",
      "Done with the batch: 930\n",
      "Done with the batch: 931\n",
      "Done with the batch: 932\n",
      "Done with the batch: 933\n",
      "Done with the batch: 934\n",
      "Done with the batch: 935\n",
      "Done with the batch: 936\n",
      "Done with the batch: 937\n",
      "Done with the batch: 938\n",
      "Done with the batch: 939\n",
      "Done with the batch: 940\n",
      "Done with the batch: 941\n",
      "Done with the batch: 942\n",
      "Done with the batch: 943\n",
      "Done with the batch: 944\n",
      "Done with the batch: 945\n",
      "Done with the batch: 946\n",
      "Done with the batch: 947\n",
      "Done with the batch: 948\n",
      "Done with the batch: 949\n",
      "Done with the batch: 950\n",
      "Done with the batch: 951\n",
      "Done with the batch: 952\n",
      "Done with the batch: 953\n",
      "Done with the batch: 954\n",
      "Done with the batch: 955\n",
      "Done with the batch: 956\n",
      "Done with the batch: 957\n",
      "Done with the batch: 958\n",
      "Done with the batch: 959\n",
      "Done with the batch: 960\n",
      "Done with the batch: 961\n",
      "Done with the batch: 962\n",
      "Done with the batch: 963\n",
      "Done with the batch: 964\n",
      "Done with the batch: 965\n",
      "Done with the batch: 966\n",
      "Done with the batch: 967\n",
      "Done with the batch: 968\n",
      "Done with the batch: 969\n",
      "Done with the batch: 970\n",
      "Done with the batch: 971\n",
      "Done with the batch: 972\n",
      "Done with the batch: 973\n",
      "Done with the batch: 974\n",
      "Done with the batch: 975\n",
      "Done with the batch: 976\n",
      "Done with the batch: 977\n",
      "Done with the batch: 978\n",
      "Done with the batch: 979\n",
      "Done with the batch: 980\n",
      "Done with the batch: 981\n",
      "Done with the batch: 982\n",
      "Done with the batch: 983\n",
      "Done with the batch: 984\n",
      "Done with the batch: 985\n",
      "Done with the batch: 986\n",
      "Done with the batch: 987\n",
      "Done with the batch: 988\n",
      "Done with the batch: 989\n",
      "Done with the batch: 990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 991\n",
      "Done with the batch: 992\n",
      "Done with the batch: 993\n",
      "Done with the batch: 994\n",
      "Done with the batch: 995\n",
      "Done with the batch: 996\n",
      "Done with the batch: 997\n",
      "Done with the batch: 998\n",
      "Done with the batch: 999\n",
      "Done with the batch: 1000\n",
      "Done with the batch: 1001\n",
      "Done with the batch: 1002\n",
      "Done with the batch: 1003\n",
      "Done with the batch: 1004\n",
      "Done with the batch: 1005\n",
      "Done with the batch: 1006\n",
      "Done with the batch: 1007\n",
      "Done with the batch: 1008\n",
      "Done with the batch: 1009\n",
      "Done with the batch: 1010\n",
      "Done with the batch: 1011\n",
      "Done with the batch: 1012\n",
      "Done with the batch: 1013\n",
      "Done with the batch: 1014\n",
      "Done with the batch: 1015\n",
      "Done with the batch: 1016\n",
      "Done with the batch: 1017\n",
      "Done with the batch: 1018\n",
      "Done with the batch: 1019\n",
      "Done with the batch: 1020\n",
      "Done with the batch: 1021\n",
      "Done with the batch: 1022\n",
      "Done with the batch: 1023\n",
      "Done with the batch: 1024\n",
      "Done with the batch: 1025\n",
      "Done with the batch: 1026\n",
      "Done with the batch: 1027\n",
      "Done with the batch: 1028\n",
      "Done with the batch: 1029\n",
      "Done with the batch: 1030\n",
      "Done with the batch: 1031\n",
      "Done with the batch: 1032\n",
      "Done with the batch: 1033\n",
      "Done with the batch: 1034\n",
      "Done with the batch: 1035\n",
      "Done with the batch: 1036\n",
      "Done with the batch: 1037\n",
      "Done with the batch: 1038\n",
      "Done with the batch: 1039\n",
      "Done with the batch: 1040\n",
      "Done with the batch: 1041\n",
      "Done with the batch: 1042\n",
      "Done with the batch: 1043\n",
      "Done with the batch: 1044\n",
      "Done with the batch: 1045\n",
      "Done with the batch: 1046\n",
      "Done with the batch: 1047\n",
      "Done with the batch: 1048\n",
      "Done with the batch: 1049\n",
      "Done with the batch: 1050\n",
      "Done with the batch: 1051\n",
      "Done with the batch: 1052\n",
      "Done with the batch: 1053\n",
      "Done with the batch: 1054\n",
      "Done with the batch: 1055\n",
      "Done with the batch: 1056\n",
      "Done with the batch: 1057\n",
      "Done with the batch: 1058\n",
      "Done with the batch: 1059\n",
      "Done with the batch: 1060\n",
      "Done with the batch: 1061\n",
      "Done with the batch: 1062\n",
      "Done with the batch: 1063\n",
      "Done with the batch: 1064\n",
      "Done with the batch: 1065\n",
      "Done with the batch: 1066\n",
      "Done with the batch: 1067\n",
      "Done with the batch: 1068\n",
      "Done with the batch: 1069\n",
      "Done with the batch: 1070\n",
      "Done with the batch: 1071\n",
      "Done with the batch: 1072\n",
      "Done with the batch: 1073\n",
      "Done with the batch: 1074\n",
      "Done with the batch: 1075\n",
      "Done with the batch: 1076\n",
      "Done with the batch: 1077\n",
      "Done with the batch: 1078\n",
      "Done with the batch: 1079\n",
      "Done with the batch: 1080\n",
      "Done with the batch: 1081\n",
      "Done with the batch: 1082\n",
      "Done with the batch: 1083\n",
      "Done with the batch: 1084\n",
      "Done with the batch: 1085\n",
      "Done with the batch: 1086\n",
      "Done with the batch: 1087\n",
      "Done with the batch: 1088\n",
      "Done with the batch: 1089\n",
      "Done with the batch: 1090\n",
      "Done with the batch: 1091\n",
      "Done with the batch: 1092\n",
      "Done with the batch: 1093\n",
      "Done with the batch: 1094\n",
      "Done with the batch: 1095\n",
      "Done with the batch: 1096\n",
      "Done with the batch: 1097\n",
      "Done with the batch: 1098\n",
      "Done with the batch: 1099\n",
      "Done with the batch: 1100\n",
      "Done with the batch: 1101\n",
      "Done with the batch: 1102\n",
      "Done with the batch: 1103\n",
      "Done with the batch: 1104\n",
      "Done with the batch: 1105\n",
      "Done with the batch: 1106\n",
      "Done with the batch: 1107\n",
      "Done with the batch: 1108\n",
      "Done with the batch: 1109\n",
      "Done with the batch: 1110\n",
      "Done with the batch: 1111\n",
      "Done with the batch: 1112\n",
      "Done with the batch: 1113\n",
      "Done with the batch: 1114\n",
      "Done with the batch: 1115\n",
      "Done with the batch: 1116\n",
      "Done with the batch: 1117\n",
      "Done with the batch: 1118\n",
      "Done with the batch: 1119\n",
      "Done with the batch: 1120\n",
      "Done with the batch: 1121\n",
      "Done with the batch: 1122\n",
      "Done with the batch: 1123\n",
      "Done with the batch: 1124\n",
      "Done with the batch: 1125\n",
      "Done with the batch: 1126\n",
      "Done with the batch: 1127\n",
      "Done with the batch: 1128\n",
      "Done with the batch: 1129\n",
      "Done with the batch: 1130\n",
      "Done with the batch: 1131\n",
      "Done with the batch: 1132\n",
      "Done with the batch: 1133\n",
      "Done with the batch: 1134\n",
      "Done with the batch: 1135\n",
      "Done with the batch: 1136\n",
      "Done with the batch: 1137\n",
      "Done with the batch: 1138\n",
      "Done with the batch: 1139\n",
      "Done with the batch: 1140\n",
      "Done with the batch: 1141\n",
      "Done with the batch: 1142\n",
      "Done with the batch: 1143\n",
      "Done with the batch: 1144\n",
      "Done with the batch: 1145\n",
      "Done with the batch: 1146\n",
      "Done with the batch: 1147\n",
      "Done with the batch: 1148\n",
      "Done with the batch: 1149\n",
      "Done with the batch: 1150\n",
      "Done with the batch: 1151\n",
      "Done with the batch: 1152\n",
      "Done with the batch: 1153\n",
      "Done with the batch: 1154\n",
      "Done with the batch: 1155\n",
      "Done with the batch: 1156\n",
      "Done with the batch: 1157\n",
      "Done with the batch: 1158\n",
      "Done with the batch: 1159\n",
      "Done with the batch: 1160\n",
      "Done with the batch: 1161\n",
      "Done with the batch: 1162\n",
      "Done with the batch: 1163\n",
      "Done with the batch: 1164\n",
      "Done with the batch: 1165\n",
      "Done with the batch: 1166\n",
      "Done with the batch: 1167\n",
      "Done with the batch: 1168\n",
      "Done with the batch: 1169\n",
      "Done with the batch: 1170\n",
      "Done with the batch: 1171\n",
      "Done with the batch: 1172\n",
      "Done with the batch: 1173\n",
      "Done with the batch: 1174\n",
      "Done with the batch: 1175\n",
      "Done with the batch: 1176\n",
      "Done with the batch: 1177\n",
      "Done with the batch: 1178\n",
      "Done with the batch: 1179\n",
      "Done with the batch: 1180\n",
      "Done with the batch: 1181\n",
      "Done with the batch: 1182\n",
      "Done with the batch: 1183\n",
      "Done with the batch: 1184\n",
      "Done with the batch: 1185\n",
      "Done with the batch: 1186\n",
      "Done with the batch: 1187\n",
      "Done with the batch: 1188\n",
      "Done with the batch: 1189\n",
      "Done with the batch: 1190\n",
      "Done with the batch: 1191\n",
      "Done with the batch: 1192\n",
      "Done with the batch: 1193\n",
      "Done with the batch: 1194\n",
      "Done with the batch: 1195\n",
      "Done with the batch: 1196\n",
      "Done with the batch: 1197\n",
      "Done with the batch: 1198\n",
      "Done with the batch: 1199\n",
      "Done with the batch: 1200\n",
      "Done with the batch: 1201\n",
      "Done with the batch: 1202\n",
      "Done with the batch: 1203\n",
      "Done with the batch: 1204\n",
      "Done with the batch: 1205\n",
      "Done with the batch: 1206\n",
      "Done with the batch: 1207\n",
      "Done with the batch: 1208\n",
      "Done with the batch: 1209\n",
      "Done with the batch: 1210\n",
      "Done with the batch: 1211\n",
      "Done with the batch: 1212\n",
      "Done with the batch: 1213\n",
      "Done with the batch: 1214\n",
      "Done with the batch: 1215\n",
      "Done with the batch: 1216\n",
      "Done with the batch: 1217\n",
      "Done with the batch: 1218\n",
      "Done with the batch: 1219\n",
      "Done with the batch: 1220\n",
      "Done with the batch: 1221\n",
      "Done with the batch: 1222\n",
      "Done with the batch: 1223\n",
      "Done with the batch: 1224\n",
      "Done with the batch: 1225\n",
      "Done with the batch: 1226\n",
      "Done with the batch: 1227\n",
      "Done with the batch: 1228\n",
      "Done with the batch: 1229\n",
      "Done with the batch: 1230\n",
      "Done with the batch: 1231\n",
      "Done with the batch: 1232\n",
      "Done with the batch: 1233\n",
      "Done with the batch: 1234\n",
      "Done with the batch: 1235\n",
      "Done with the batch: 1236\n",
      "Done with the batch: 1237\n",
      "Done with the batch: 1238\n",
      "Done with the batch: 1239\n",
      "Done with the batch: 1240\n",
      "Done with the batch: 1241\n",
      "Done with the batch: 1242\n",
      "Done with the batch: 1243\n",
      "Done with the batch: 1244\n",
      "Done with the batch: 1245\n",
      "Done with the batch: 1246\n",
      "Done with the batch: 1247\n",
      "Done with the batch: 1248\n",
      "Done with the batch: 1249\n",
      "Done with the batch: 1250\n",
      "Done with the batch: 1251\n",
      "Done with the batch: 1252\n",
      "Done with the batch: 1253\n",
      "Done with the batch: 1254\n",
      "Done with the batch: 1255\n",
      "Done with the batch: 1256\n",
      "Done with the batch: 1257\n",
      "Done with the batch: 1258\n",
      "Done with the batch: 1259\n",
      "Done with the batch: 1260\n",
      "Done with the batch: 1261\n",
      "Done with the batch: 1262\n",
      "Done with the batch: 1263\n",
      "Done with the batch: 1264\n",
      "Done with the batch: 1265\n",
      "Done with the batch: 1266\n",
      "Done with the batch: 1267\n",
      "Done with the batch: 1268\n",
      "Done with the batch: 1269\n",
      "Done with the batch: 1270\n",
      "Done with the batch: 1271\n",
      "Done with the batch: 1272\n",
      "Done with the batch: 1273\n",
      "Done with the batch: 1274\n",
      "Done with the batch: 1275\n",
      "Done with the batch: 1276\n",
      "Done with the batch: 1277\n",
      "Done with the batch: 1278\n",
      "Done with the batch: 1279\n",
      "Done with the batch: 1280\n",
      "Done with the batch: 1281\n",
      "Done with the batch: 1282\n",
      "Done with the batch: 1283\n",
      "Done with the batch: 1284\n",
      "Done with the batch: 1285\n",
      "Done with the batch: 1286\n",
      "Done with the batch: 1287\n",
      "Done with the batch: 1288\n",
      "Done with the batch: 1289\n",
      "Done with the batch: 1290\n",
      "Done with the batch: 1291\n",
      "Done with the batch: 1292\n",
      "Done with the batch: 1293\n",
      "Done with the batch: 1294\n",
      "Done with the batch: 1295\n",
      "Done with the batch: 1296\n",
      "Done with the batch: 1297\n",
      "Done with the batch: 1298\n",
      "Done with the batch: 1299\n",
      "Done with the batch: 1300\n",
      "Done with the batch: 1301\n",
      "Done with the batch: 1302\n",
      "Done with the batch: 1303\n",
      "Done with the batch: 1304\n",
      "Done with the batch: 1305\n",
      "Done with the batch: 1306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 1307\n",
      "Done with the batch: 1308\n",
      "Done with the batch: 1309\n",
      "Done with the batch: 1310\n",
      "Done with the batch: 1311\n",
      "Done with the batch: 1312\n",
      "Done with the batch: 1313\n",
      "Done with the batch: 1314\n",
      "Done with the batch: 1315\n",
      "Done with the batch: 1316\n",
      "Done with the batch: 1317\n",
      "Done with the batch: 1318\n",
      "Done with the batch: 1319\n",
      "Done with the batch: 1320\n",
      "Done with the batch: 1321\n",
      "Done with the batch: 1322\n",
      "Done with the batch: 1323\n",
      "Done with the batch: 1324\n",
      "Done with the batch: 1325\n",
      "Done with the batch: 1326\n",
      "Done with the batch: 1327\n",
      "Done with the batch: 1328\n",
      "Done with the batch: 1329\n",
      "Done with the batch: 1330\n",
      "Done with the batch: 1331\n",
      "Done with the batch: 1332\n",
      "Done with the batch: 1333\n",
      "Done with the batch: 1334\n",
      "Done with the batch: 1335\n",
      "Done with the batch: 1336\n",
      "Done with the batch: 1337\n",
      "Done with the batch: 1338\n",
      "Done with the batch: 1339\n",
      "Done with the batch: 1340\n",
      "Done with the batch: 1341\n",
      "Done with the batch: 1342\n",
      "Done with the batch: 1343\n",
      "Done with the batch: 1344\n",
      "Done with the batch: 1345\n",
      "Done with the batch: 1346\n",
      "Done with the batch: 1347\n",
      "Done with the batch: 1348\n",
      "Done with the batch: 1349\n",
      "Done with the batch: 1350\n",
      "Done with the batch: 1351\n",
      "Done with the batch: 1352\n",
      "Done with the batch: 1353\n",
      "Done with the batch: 1354\n",
      "Done with the batch: 1355\n",
      "Done with the batch: 1356\n",
      "Done with the batch: 1357\n",
      "Done with the batch: 1358\n",
      "Done with the batch: 1359\n",
      "Done with the batch: 1360\n",
      "Done with the batch: 1361\n",
      "Done with the batch: 1362\n",
      "Done with the batch: 1363\n",
      "Done with the batch: 1364\n",
      "Done with the batch: 1365\n",
      "Done with the batch: 1366\n",
      "Done with the batch: 1367\n",
      "Done with the batch: 1368\n",
      "Done with the batch: 1369\n",
      "Done with the batch: 1370\n",
      "Done with the batch: 1371\n",
      "Done with the batch: 1372\n",
      "Done with the batch: 1373\n",
      "Done with the batch: 1374\n",
      "Done with the batch: 1375\n",
      "Done with the batch: 1376\n",
      "Done with the batch: 1377\n",
      "Done with the batch: 1378\n",
      "Done with the batch: 1379\n",
      "Done with the batch: 1380\n",
      "Done with the batch: 1381\n",
      "Done with the batch: 1382\n",
      "Done with the batch: 1383\n",
      "Done with the batch: 1384\n",
      "Done with the batch: 1385\n",
      "Done with the batch: 1386\n",
      "Done with the batch: 1387\n",
      "Done with the batch: 1388\n",
      "Done with the batch: 1389\n",
      "Done with the batch: 1390\n",
      "Done with the batch: 1391\n",
      "Done with the batch: 1392\n",
      "Done with the batch: 1393\n",
      "Done with the batch: 1394\n",
      "Done with the batch: 1395\n",
      "Done with the batch: 1396\n",
      "Done with the batch: 1397\n",
      "Done with the batch: 1398\n",
      "Done with the batch: 1399\n",
      "Done with the batch: 1400\n",
      "Done with the batch: 1401\n",
      "Done with the batch: 1402\n",
      "Done with the batch: 1403\n",
      "Done with the batch: 1404\n",
      "Done with the batch: 1405\n",
      "Done with the batch: 1406\n",
      "Done with the batch: 1407\n",
      "Done with the batch: 1408\n",
      "Done with the batch: 1409\n",
      "Done with the batch: 1410\n",
      "Done with the batch: 1411\n",
      "Done with the batch: 1412\n",
      "Done with the batch: 1413\n",
      "Done with the batch: 1414\n",
      "Done with the batch: 1415\n",
      "Done with the batch: 1416\n",
      "Done with the batch: 1417\n",
      "Done with the batch: 1418\n",
      "Done with the batch: 1419\n",
      "Done with the batch: 1420\n",
      "Done with the batch: 1421\n",
      "Done with the batch: 1422\n",
      "Done with the batch: 1423\n",
      "Done with the batch: 1424\n",
      "Done with the batch: 1425\n",
      "Done with the batch: 1426\n",
      "Done with the batch: 1427\n",
      "Done with the batch: 1428\n",
      "Done with the batch: 1429\n",
      "Done with the batch: 1430\n",
      "Done with the batch: 1431\n",
      "Done with the batch: 1432\n",
      "Done with the batch: 1433\n",
      "Done with the batch: 1434\n",
      "Done with the batch: 1435\n",
      "Done with the batch: 1436\n",
      "Done with the batch: 1437\n",
      "Done with the batch: 1438\n",
      "Done with the batch: 1439\n",
      "Done with the batch: 1440\n",
      "Done with the batch: 1441\n",
      "Done with the batch: 1442\n",
      "Done with the batch: 1443\n",
      "Done with the batch: 1444\n",
      "Done with the batch: 1445\n",
      "Done with the batch: 1446\n",
      "Done with the batch: 1447\n",
      "Done with the batch: 1448\n",
      "Done with the batch: 1449\n",
      "Done with the batch: 1450\n",
      "Done with the batch: 1451\n",
      "Done with the batch: 1452\n",
      "Done with the batch: 1453\n",
      "Done with the batch: 1454\n",
      "Done with the batch: 1455\n",
      "Done with the batch: 1456\n",
      "Done with the batch: 1457\n",
      "Done with the batch: 1458\n",
      "Done with the batch: 1459\n",
      "Done with the batch: 1460\n",
      "Done with the batch: 1461\n",
      "Done with the batch: 1462\n",
      "Done with the batch: 1463\n",
      "Done with the batch: 1464\n",
      "Done with the batch: 1465\n",
      "Done with the batch: 1466\n",
      "Done with the batch: 1467\n",
      "Done with the batch: 1468\n",
      "Done with the batch: 1469\n",
      "Done with the batch: 1470\n",
      "Done with the batch: 1471\n",
      "Done with the batch: 1472\n",
      "Done with the batch: 1473\n",
      "Done with the batch: 1474\n",
      "Done with the batch: 1475\n",
      "Done with the batch: 1476\n",
      "Done with the batch: 1477\n",
      "Done with the batch: 1478\n",
      "Done with the batch: 1479\n",
      "Done with the batch: 1480\n",
      "Done with the batch: 1481\n",
      "Done with the batch: 1482\n",
      "Done with the batch: 1483\n",
      "Done with the batch: 1484\n",
      "Done with the batch: 1485\n",
      "Done with the batch: 1486\n",
      "Done with the batch: 1487\n",
      "Done with the batch: 1488\n",
      "Done with the batch: 1489\n",
      "Done with the batch: 1490\n",
      "Done with the batch: 1491\n",
      "Done with the batch: 1492\n",
      "Done with the batch: 1493\n",
      "Done with the batch: 1494\n",
      "Done with the batch: 1495\n",
      "Done with the batch: 1496\n",
      "Done with the batch: 1497\n",
      "Done with the batch: 1498\n",
      "Done with the batch: 1499\n",
      "Done with the batch: 1500\n",
      "Done with the batch: 1501\n",
      "Done with the batch: 1502\n",
      "Done with the batch: 1503\n",
      "Done with the batch: 1504\n",
      "Done with the batch: 1505\n",
      "Done with the batch: 1506\n",
      "Done with the batch: 1507\n",
      "Done with the batch: 1508\n",
      "Done with the batch: 1509\n",
      "Done with the batch: 1510\n",
      "Done with the batch: 1511\n",
      "Done with the batch: 1512\n",
      "Done with the batch: 1513\n",
      "Done with the batch: 1514\n",
      "Done with the batch: 1515\n",
      "Done with the batch: 1516\n",
      "Done with the batch: 1517\n",
      "Done with the batch: 1518\n",
      "Done with the batch: 1519\n",
      "Done with the batch: 1520\n",
      "Done with the batch: 1521\n",
      "Done with the batch: 1522\n",
      "Done with the batch: 1523\n",
      "Done with the batch: 1524\n",
      "Done with the batch: 1525\n",
      "Done with the batch: 1526\n",
      "Done with the batch: 1527\n",
      "Done with the batch: 1528\n",
      "Done with the batch: 1529\n",
      "Done with the batch: 1530\n",
      "Done with the batch: 1531\n",
      "Done with the batch: 1532\n",
      "Done with the batch: 1533\n",
      "Done with the batch: 1534\n",
      "Done with the batch: 1535\n",
      "Done with the batch: 1536\n",
      "Done with the batch: 1537\n",
      "Done with the batch: 1538\n",
      "Done with the batch: 1539\n",
      "Done with the batch: 1540\n",
      "Done with the batch: 1541\n",
      "Done with the batch: 1542\n",
      "Done with the batch: 1543\n",
      "Done with the batch: 1544\n",
      "Done with the batch: 1545\n",
      "Done with the batch: 1546\n",
      "Done with the batch: 1547\n",
      "Done with the batch: 1548\n",
      "Done with the batch: 1549\n",
      "Done with the batch: 1550\n",
      "Done with the batch: 1551\n",
      "Done with the batch: 1552\n",
      "Done with the batch: 1553\n",
      "Done with the batch: 1554\n",
      "Done with the batch: 1555\n",
      "Done with the batch: 1556\n",
      "Done with the batch: 1557\n",
      "Done with the batch: 1558\n",
      "Done with the batch: 1559\n",
      "Done with the batch: 1560\n",
      "Done with the batch: 1561\n",
      "Done with the batch: 1562\n",
      "Done with the batch: 1563\n",
      "Done with the batch: 1564\n",
      "Done with the batch: 1565\n",
      "Done with the batch: 1566\n",
      "Done with the batch: 1567\n",
      "Done with the batch: 1568\n",
      "Done with the batch: 1569\n",
      "Done with the batch: 1570\n",
      "Done with the batch: 1571\n",
      "Done with the batch: 1572\n",
      "Done with the batch: 1573\n",
      "Done with the batch: 1574\n",
      "Done with the batch: 1575\n",
      "Done with the batch: 1576\n",
      "Done with the batch: 1577\n",
      "Done with the batch: 1578\n",
      "Done with the batch: 1579\n",
      "Done with the batch: 1580\n",
      "Done with the batch: 1581\n",
      "Done with the batch: 1582\n",
      "Done with the batch: 1583\n",
      "Done with the batch: 1584\n",
      "Done with the batch: 1585\n",
      "Done with the batch: 1586\n",
      "Done with the batch: 1587\n",
      "Done with the batch: 1588\n",
      "Done with the batch: 1589\n",
      "Done with the batch: 1590\n",
      "Done with the batch: 1591\n",
      "Done with the batch: 1592\n",
      "Done with the batch: 1593\n",
      "Done with the batch: 1594\n",
      "Done with the batch: 1595\n",
      "Done with the batch: 1596\n",
      "Done with the batch: 1597\n",
      "Done with the batch: 1598\n",
      "Done with the batch: 1599\n",
      "Done with the batch: 1600\n",
      "Done with the batch: 1601\n",
      "Done with the batch: 1602\n",
      "Done with the batch: 1603\n",
      "Done with the batch: 1604\n",
      "Done with the batch: 1605\n",
      "Done with the batch: 1606\n",
      "Done with the batch: 1607\n",
      "Done with the batch: 1608\n",
      "Done with the batch: 1609\n",
      "Done with the batch: 1610\n",
      "Done with the batch: 1611\n",
      "Done with the batch: 1612\n",
      "Done with the batch: 1613\n",
      "Done with the batch: 1614\n",
      "Done with the batch: 1615\n",
      "Done with the batch: 1616\n",
      "Done with the batch: 1617\n",
      "(6470, 512) (6470,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"570dbdfe-9bdc-4278-bf68-381b733d82ef\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"570dbdfe-9bdc-4278-bf68-381b733d82ef\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "\n",
    "X_Train=np.empty((0,512))\n",
    "Y_Train=np.empty((0,batch_size))\n",
    "print(X_Train.shape)\n",
    "for i,data in enumerate(trainloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    FCLayer=net.get_first_FC_Layer(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Train=np.append(X_Train,FCLayer,axis=0)\n",
    "    Y_Train=np.append(Y_Train,labels.numpy())\n",
    "print(X_Train.shape,Y_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576bf8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 512)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "(1618, 512) (1618,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"af29e9e1-1510-475c-a67e-2eac5e204592\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"af29e9e1-1510-475c-a67e-2eac5e204592\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "X_Test=np.empty((0,512))\n",
    "Y_Test=np.empty((0,batch_size))\n",
    "print(X_Test.shape)\n",
    "for i,data in enumerate(testloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    FCLayer=net.get_first_FC_Layer(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Test=np.append(X_Test,FCLayer,axis=0)\n",
    "    Y_Test=np.append(Y_Test,labels.numpy())\n",
    "print(X_Test.shape,Y_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547984b0",
   "metadata": {},
   "source": [
    "## Getting the feature extraction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd95559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 18432)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n",
      "Done with the batch: 332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "Done with the batch: 405\n",
      "Done with the batch: 406\n",
      "Done with the batch: 407\n",
      "Done with the batch: 408\n",
      "Done with the batch: 409\n",
      "Done with the batch: 410\n",
      "Done with the batch: 411\n",
      "Done with the batch: 412\n",
      "Done with the batch: 413\n",
      "Done with the batch: 414\n",
      "Done with the batch: 415\n",
      "Done with the batch: 416\n",
      "Done with the batch: 417\n",
      "Done with the batch: 418\n",
      "Done with the batch: 419\n",
      "Done with the batch: 420\n",
      "Done with the batch: 421\n",
      "Done with the batch: 422\n",
      "Done with the batch: 423\n",
      "Done with the batch: 424\n",
      "Done with the batch: 425\n",
      "Done with the batch: 426\n",
      "Done with the batch: 427\n",
      "Done with the batch: 428\n",
      "Done with the batch: 429\n",
      "Done with the batch: 430\n",
      "Done with the batch: 431\n",
      "Done with the batch: 432\n",
      "Done with the batch: 433\n",
      "Done with the batch: 434\n",
      "Done with the batch: 435\n",
      "Done with the batch: 436\n",
      "Done with the batch: 437\n",
      "Done with the batch: 438\n",
      "Done with the batch: 439\n",
      "Done with the batch: 440\n",
      "Done with the batch: 441\n",
      "Done with the batch: 442\n",
      "Done with the batch: 443\n",
      "Done with the batch: 444\n",
      "Done with the batch: 445\n",
      "Done with the batch: 446\n",
      "Done with the batch: 447\n",
      "Done with the batch: 448\n",
      "Done with the batch: 449\n",
      "Done with the batch: 450\n",
      "Done with the batch: 451\n",
      "Done with the batch: 452\n",
      "Done with the batch: 453\n",
      "Done with the batch: 454\n",
      "Done with the batch: 455\n",
      "Done with the batch: 456\n",
      "Done with the batch: 457\n",
      "Done with the batch: 458\n",
      "Done with the batch: 459\n",
      "Done with the batch: 460\n",
      "Done with the batch: 461\n",
      "Done with the batch: 462\n",
      "Done with the batch: 463\n",
      "Done with the batch: 464\n",
      "Done with the batch: 465\n",
      "Done with the batch: 466\n",
      "Done with the batch: 467\n",
      "Done with the batch: 468\n",
      "Done with the batch: 469\n",
      "Done with the batch: 470\n",
      "Done with the batch: 471\n",
      "Done with the batch: 472\n",
      "Done with the batch: 473\n",
      "Done with the batch: 474\n",
      "Done with the batch: 475\n",
      "Done with the batch: 476\n",
      "Done with the batch: 477\n",
      "Done with the batch: 478\n",
      "Done with the batch: 479\n",
      "Done with the batch: 480\n",
      "Done with the batch: 481\n",
      "Done with the batch: 482\n",
      "Done with the batch: 483\n",
      "Done with the batch: 484\n",
      "Done with the batch: 485\n",
      "Done with the batch: 486\n",
      "Done with the batch: 487\n",
      "Done with the batch: 488\n",
      "Done with the batch: 489\n",
      "Done with the batch: 490\n",
      "Done with the batch: 491\n",
      "Done with the batch: 492\n",
      "Done with the batch: 493\n",
      "Done with the batch: 494\n",
      "Done with the batch: 495\n",
      "Done with the batch: 496\n",
      "Done with the batch: 497\n",
      "Done with the batch: 498\n",
      "Done with the batch: 499\n",
      "Done with the batch: 500\n",
      "Done with the batch: 501\n",
      "Done with the batch: 502\n",
      "Done with the batch: 503\n",
      "Done with the batch: 504\n",
      "Done with the batch: 505\n",
      "Done with the batch: 506\n",
      "Done with the batch: 507\n",
      "Done with the batch: 508\n",
      "Done with the batch: 509\n",
      "Done with the batch: 510\n",
      "Done with the batch: 511\n",
      "Done with the batch: 512\n",
      "Done with the batch: 513\n",
      "Done with the batch: 514\n",
      "Done with the batch: 515\n",
      "Done with the batch: 516\n",
      "Done with the batch: 517\n",
      "Done with the batch: 518\n",
      "Done with the batch: 519\n",
      "Done with the batch: 520\n",
      "Done with the batch: 521\n",
      "Done with the batch: 522\n",
      "Done with the batch: 523\n",
      "Done with the batch: 524\n",
      "Done with the batch: 525\n",
      "Done with the batch: 526\n",
      "Done with the batch: 527\n",
      "Done with the batch: 528\n",
      "Done with the batch: 529\n",
      "Done with the batch: 530\n",
      "Done with the batch: 531\n",
      "Done with the batch: 532\n",
      "Done with the batch: 533\n",
      "Done with the batch: 534\n",
      "Done with the batch: 535\n",
      "Done with the batch: 536\n",
      "Done with the batch: 537\n",
      "Done with the batch: 538\n",
      "Done with the batch: 539\n",
      "Done with the batch: 540\n",
      "Done with the batch: 541\n",
      "Done with the batch: 542\n",
      "Done with the batch: 543\n",
      "Done with the batch: 544\n",
      "Done with the batch: 545\n",
      "Done with the batch: 546\n",
      "Done with the batch: 547\n",
      "Done with the batch: 548\n",
      "Done with the batch: 549\n",
      "Done with the batch: 550\n",
      "Done with the batch: 551\n",
      "Done with the batch: 552\n",
      "Done with the batch: 553\n",
      "Done with the batch: 554\n",
      "Done with the batch: 555\n",
      "Done with the batch: 556\n",
      "Done with the batch: 557\n",
      "Done with the batch: 558\n",
      "Done with the batch: 559\n",
      "Done with the batch: 560\n",
      "Done with the batch: 561\n",
      "Done with the batch: 562\n",
      "Done with the batch: 563\n",
      "Done with the batch: 564\n",
      "Done with the batch: 565\n",
      "Done with the batch: 566\n",
      "Done with the batch: 567\n",
      "Done with the batch: 568\n",
      "Done with the batch: 569\n",
      "Done with the batch: 570\n",
      "Done with the batch: 571\n",
      "Done with the batch: 572\n",
      "Done with the batch: 573\n",
      "Done with the batch: 574\n",
      "Done with the batch: 575\n",
      "Done with the batch: 576\n",
      "Done with the batch: 577\n",
      "Done with the batch: 578\n",
      "Done with the batch: 579\n",
      "Done with the batch: 580\n",
      "Done with the batch: 581\n",
      "Done with the batch: 582\n",
      "Done with the batch: 583\n",
      "Done with the batch: 584\n",
      "Done with the batch: 585\n",
      "Done with the batch: 586\n",
      "Done with the batch: 587\n",
      "Done with the batch: 588\n",
      "Done with the batch: 589\n",
      "Done with the batch: 590\n",
      "Done with the batch: 591\n",
      "Done with the batch: 592\n",
      "Done with the batch: 593\n",
      "Done with the batch: 594\n",
      "Done with the batch: 595\n",
      "Done with the batch: 596\n",
      "Done with the batch: 597\n",
      "Done with the batch: 598\n",
      "Done with the batch: 599\n",
      "Done with the batch: 600\n",
      "Done with the batch: 601\n",
      "Done with the batch: 602\n",
      "Done with the batch: 603\n",
      "Done with the batch: 604\n",
      "Done with the batch: 605\n",
      "Done with the batch: 606\n",
      "Done with the batch: 607\n",
      "Done with the batch: 608\n",
      "Done with the batch: 609\n",
      "Done with the batch: 610\n",
      "Done with the batch: 611\n",
      "Done with the batch: 612\n",
      "Done with the batch: 613\n",
      "Done with the batch: 614\n",
      "Done with the batch: 615\n",
      "Done with the batch: 616\n",
      "Done with the batch: 617\n",
      "Done with the batch: 618\n",
      "Done with the batch: 619\n",
      "Done with the batch: 620\n",
      "Done with the batch: 621\n",
      "Done with the batch: 622\n",
      "Done with the batch: 623\n",
      "Done with the batch: 624\n",
      "Done with the batch: 625\n",
      "Done with the batch: 626\n",
      "Done with the batch: 627\n",
      "Done with the batch: 628\n",
      "Done with the batch: 629\n",
      "Done with the batch: 630\n",
      "Done with the batch: 631\n",
      "Done with the batch: 632\n",
      "Done with the batch: 633\n",
      "Done with the batch: 634\n",
      "Done with the batch: 635\n",
      "Done with the batch: 636\n",
      "Done with the batch: 637\n",
      "Done with the batch: 638\n",
      "Done with the batch: 639\n",
      "Done with the batch: 640\n",
      "Done with the batch: 641\n",
      "Done with the batch: 642\n",
      "Done with the batch: 643\n",
      "Done with the batch: 644\n",
      "Done with the batch: 645\n",
      "Done with the batch: 646\n",
      "Done with the batch: 647\n",
      "Done with the batch: 648\n",
      "Done with the batch: 649\n",
      "Done with the batch: 650\n",
      "Done with the batch: 651\n",
      "Done with the batch: 652\n",
      "Done with the batch: 653\n",
      "Done with the batch: 654\n",
      "Done with the batch: 655\n",
      "Done with the batch: 656\n",
      "Done with the batch: 657\n",
      "Done with the batch: 658\n",
      "Done with the batch: 659\n",
      "Done with the batch: 660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 661\n",
      "Done with the batch: 662\n",
      "Done with the batch: 663\n",
      "Done with the batch: 664\n",
      "Done with the batch: 665\n",
      "Done with the batch: 666\n",
      "Done with the batch: 667\n",
      "Done with the batch: 668\n",
      "Done with the batch: 669\n",
      "Done with the batch: 670\n",
      "Done with the batch: 671\n",
      "Done with the batch: 672\n",
      "Done with the batch: 673\n",
      "Done with the batch: 674\n",
      "Done with the batch: 675\n",
      "Done with the batch: 676\n",
      "Done with the batch: 677\n",
      "Done with the batch: 678\n",
      "Done with the batch: 679\n",
      "Done with the batch: 680\n",
      "Done with the batch: 681\n",
      "Done with the batch: 682\n",
      "Done with the batch: 683\n",
      "Done with the batch: 684\n",
      "Done with the batch: 685\n",
      "Done with the batch: 686\n",
      "Done with the batch: 687\n",
      "Done with the batch: 688\n",
      "Done with the batch: 689\n",
      "Done with the batch: 690\n",
      "Done with the batch: 691\n",
      "Done with the batch: 692\n",
      "Done with the batch: 693\n",
      "Done with the batch: 694\n",
      "Done with the batch: 695\n",
      "Done with the batch: 696\n",
      "Done with the batch: 697\n",
      "Done with the batch: 698\n",
      "Done with the batch: 699\n",
      "Done with the batch: 700\n",
      "Done with the batch: 701\n",
      "Done with the batch: 702\n",
      "Done with the batch: 703\n",
      "Done with the batch: 704\n",
      "Done with the batch: 705\n",
      "Done with the batch: 706\n",
      "Done with the batch: 707\n",
      "Done with the batch: 708\n",
      "Done with the batch: 709\n",
      "Done with the batch: 710\n",
      "Done with the batch: 711\n",
      "Done with the batch: 712\n",
      "Done with the batch: 713\n",
      "Done with the batch: 714\n",
      "Done with the batch: 715\n",
      "Done with the batch: 716\n",
      "Done with the batch: 717\n",
      "Done with the batch: 718\n",
      "Done with the batch: 719\n",
      "Done with the batch: 720\n",
      "Done with the batch: 721\n",
      "Done with the batch: 722\n",
      "Done with the batch: 723\n",
      "Done with the batch: 724\n",
      "Done with the batch: 725\n",
      "Done with the batch: 726\n",
      "Done with the batch: 727\n",
      "Done with the batch: 728\n",
      "Done with the batch: 729\n",
      "Done with the batch: 730\n",
      "Done with the batch: 731\n",
      "Done with the batch: 732\n",
      "Done with the batch: 733\n",
      "Done with the batch: 734\n",
      "Done with the batch: 735\n",
      "Done with the batch: 736\n",
      "Done with the batch: 737\n",
      "Done with the batch: 738\n",
      "Done with the batch: 739\n",
      "Done with the batch: 740\n",
      "Done with the batch: 741\n",
      "Done with the batch: 742\n",
      "Done with the batch: 743\n",
      "Done with the batch: 744\n",
      "Done with the batch: 745\n",
      "Done with the batch: 746\n",
      "Done with the batch: 747\n",
      "Done with the batch: 748\n",
      "Done with the batch: 749\n",
      "Done with the batch: 750\n",
      "Done with the batch: 751\n",
      "Done with the batch: 752\n",
      "Done with the batch: 753\n",
      "Done with the batch: 754\n",
      "Done with the batch: 755\n",
      "Done with the batch: 756\n",
      "Done with the batch: 757\n",
      "Done with the batch: 758\n",
      "Done with the batch: 759\n",
      "Done with the batch: 760\n",
      "Done with the batch: 761\n",
      "Done with the batch: 762\n",
      "Done with the batch: 763\n",
      "Done with the batch: 764\n",
      "Done with the batch: 765\n",
      "Done with the batch: 766\n",
      "Done with the batch: 767\n",
      "Done with the batch: 768\n",
      "Done with the batch: 769\n",
      "Done with the batch: 770\n",
      "Done with the batch: 771\n",
      "Done with the batch: 772\n",
      "Done with the batch: 773\n",
      "Done with the batch: 774\n",
      "Done with the batch: 775\n",
      "Done with the batch: 776\n",
      "Done with the batch: 777\n",
      "Done with the batch: 778\n",
      "Done with the batch: 779\n",
      "Done with the batch: 780\n",
      "Done with the batch: 781\n",
      "Done with the batch: 782\n",
      "Done with the batch: 783\n",
      "Done with the batch: 784\n",
      "Done with the batch: 785\n",
      "Done with the batch: 786\n",
      "Done with the batch: 787\n",
      "Done with the batch: 788\n",
      "Done with the batch: 789\n",
      "Done with the batch: 790\n",
      "Done with the batch: 791\n",
      "Done with the batch: 792\n",
      "Done with the batch: 793\n",
      "Done with the batch: 794\n",
      "Done with the batch: 795\n",
      "Done with the batch: 796\n",
      "Done with the batch: 797\n",
      "Done with the batch: 798\n",
      "Done with the batch: 799\n",
      "Done with the batch: 800\n",
      "Done with the batch: 801\n",
      "Done with the batch: 802\n",
      "Done with the batch: 803\n",
      "Done with the batch: 804\n",
      "Done with the batch: 805\n",
      "Done with the batch: 806\n",
      "Done with the batch: 807\n",
      "Done with the batch: 808\n",
      "Done with the batch: 809\n",
      "Done with the batch: 810\n",
      "Done with the batch: 811\n",
      "Done with the batch: 812\n",
      "Done with the batch: 813\n",
      "Done with the batch: 814\n",
      "Done with the batch: 815\n",
      "Done with the batch: 816\n",
      "Done with the batch: 817\n",
      "Done with the batch: 818\n",
      "Done with the batch: 819\n",
      "Done with the batch: 820\n",
      "Done with the batch: 821\n",
      "Done with the batch: 822\n",
      "Done with the batch: 823\n",
      "Done with the batch: 824\n",
      "Done with the batch: 825\n",
      "Done with the batch: 826\n",
      "Done with the batch: 827\n",
      "Done with the batch: 828\n",
      "Done with the batch: 829\n",
      "Done with the batch: 830\n",
      "Done with the batch: 831\n",
      "Done with the batch: 832\n",
      "Done with the batch: 833\n",
      "Done with the batch: 834\n",
      "Done with the batch: 835\n",
      "Done with the batch: 836\n",
      "Done with the batch: 837\n",
      "Done with the batch: 838\n",
      "Done with the batch: 839\n",
      "Done with the batch: 840\n",
      "Done with the batch: 841\n",
      "Done with the batch: 842\n",
      "Done with the batch: 843\n",
      "Done with the batch: 844\n",
      "Done with the batch: 845\n",
      "Done with the batch: 846\n",
      "Done with the batch: 847\n",
      "Done with the batch: 848\n",
      "Done with the batch: 849\n",
      "Done with the batch: 850\n",
      "Done with the batch: 851\n",
      "Done with the batch: 852\n",
      "Done with the batch: 853\n",
      "Done with the batch: 854\n",
      "Done with the batch: 855\n",
      "Done with the batch: 856\n",
      "Done with the batch: 857\n",
      "Done with the batch: 858\n",
      "Done with the batch: 859\n",
      "Done with the batch: 860\n",
      "Done with the batch: 861\n",
      "Done with the batch: 862\n",
      "Done with the batch: 863\n",
      "Done with the batch: 864\n",
      "Done with the batch: 865\n",
      "Done with the batch: 866\n",
      "Done with the batch: 867\n",
      "Done with the batch: 868\n",
      "Done with the batch: 869\n",
      "Done with the batch: 870\n",
      "Done with the batch: 871\n",
      "Done with the batch: 872\n",
      "Done with the batch: 873\n",
      "Done with the batch: 874\n",
      "Done with the batch: 875\n",
      "Done with the batch: 876\n",
      "Done with the batch: 877\n",
      "Done with the batch: 878\n",
      "Done with the batch: 879\n",
      "Done with the batch: 880\n",
      "Done with the batch: 881\n",
      "Done with the batch: 882\n",
      "Done with the batch: 883\n",
      "Done with the batch: 884\n",
      "Done with the batch: 885\n",
      "Done with the batch: 886\n",
      "Done with the batch: 887\n",
      "Done with the batch: 888\n",
      "Done with the batch: 889\n",
      "Done with the batch: 890\n",
      "Done with the batch: 891\n",
      "Done with the batch: 892\n",
      "Done with the batch: 893\n",
      "Done with the batch: 894\n",
      "Done with the batch: 895\n",
      "Done with the batch: 896\n",
      "Done with the batch: 897\n",
      "Done with the batch: 898\n",
      "Done with the batch: 899\n",
      "Done with the batch: 900\n",
      "Done with the batch: 901\n",
      "Done with the batch: 902\n",
      "Done with the batch: 903\n",
      "Done with the batch: 904\n",
      "Done with the batch: 905\n",
      "Done with the batch: 906\n",
      "Done with the batch: 907\n",
      "Done with the batch: 908\n",
      "Done with the batch: 909\n",
      "Done with the batch: 910\n",
      "Done with the batch: 911\n",
      "Done with the batch: 912\n",
      "Done with the batch: 913\n",
      "Done with the batch: 914\n",
      "Done with the batch: 915\n",
      "Done with the batch: 916\n",
      "Done with the batch: 917\n",
      "Done with the batch: 918\n",
      "Done with the batch: 919\n",
      "Done with the batch: 920\n",
      "Done with the batch: 921\n",
      "Done with the batch: 922\n",
      "Done with the batch: 923\n",
      "Done with the batch: 924\n",
      "Done with the batch: 925\n",
      "Done with the batch: 926\n",
      "Done with the batch: 927\n",
      "Done with the batch: 928\n",
      "Done with the batch: 929\n",
      "Done with the batch: 930\n",
      "Done with the batch: 931\n",
      "Done with the batch: 932\n",
      "Done with the batch: 933\n",
      "Done with the batch: 934\n",
      "Done with the batch: 935\n",
      "Done with the batch: 936\n",
      "Done with the batch: 937\n",
      "Done with the batch: 938\n",
      "Done with the batch: 939\n",
      "Done with the batch: 940\n",
      "Done with the batch: 941\n",
      "Done with the batch: 942\n",
      "Done with the batch: 943\n",
      "Done with the batch: 944\n",
      "Done with the batch: 945\n",
      "Done with the batch: 946\n",
      "Done with the batch: 947\n",
      "Done with the batch: 948\n",
      "Done with the batch: 949\n",
      "Done with the batch: 950\n",
      "Done with the batch: 951\n",
      "Done with the batch: 952\n",
      "Done with the batch: 953\n",
      "Done with the batch: 954\n",
      "Done with the batch: 955\n",
      "Done with the batch: 956\n",
      "Done with the batch: 957\n",
      "Done with the batch: 958\n",
      "Done with the batch: 959\n",
      "Done with the batch: 960\n",
      "Done with the batch: 961\n",
      "Done with the batch: 962\n",
      "Done with the batch: 963\n",
      "Done with the batch: 964\n",
      "Done with the batch: 965\n",
      "Done with the batch: 966\n",
      "Done with the batch: 967\n",
      "Done with the batch: 968\n",
      "Done with the batch: 969\n",
      "Done with the batch: 970\n",
      "Done with the batch: 971\n",
      "Done with the batch: 972\n",
      "Done with the batch: 973\n",
      "Done with the batch: 974\n",
      "Done with the batch: 975\n",
      "Done with the batch: 976\n",
      "Done with the batch: 977\n",
      "Done with the batch: 978\n",
      "Done with the batch: 979\n",
      "Done with the batch: 980\n",
      "Done with the batch: 981\n",
      "Done with the batch: 982\n",
      "Done with the batch: 983\n",
      "Done with the batch: 984\n",
      "Done with the batch: 985\n",
      "Done with the batch: 986\n",
      "Done with the batch: 987\n",
      "Done with the batch: 988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 989\n",
      "Done with the batch: 990\n",
      "Done with the batch: 991\n",
      "Done with the batch: 992\n",
      "Done with the batch: 993\n",
      "Done with the batch: 994\n",
      "Done with the batch: 995\n",
      "Done with the batch: 996\n",
      "Done with the batch: 997\n",
      "Done with the batch: 998\n",
      "Done with the batch: 999\n",
      "Done with the batch: 1000\n",
      "Done with the batch: 1001\n",
      "Done with the batch: 1002\n",
      "Done with the batch: 1003\n",
      "Done with the batch: 1004\n",
      "Done with the batch: 1005\n",
      "Done with the batch: 1006\n",
      "Done with the batch: 1007\n",
      "Done with the batch: 1008\n",
      "Done with the batch: 1009\n",
      "Done with the batch: 1010\n",
      "Done with the batch: 1011\n",
      "Done with the batch: 1012\n",
      "Done with the batch: 1013\n",
      "Done with the batch: 1014\n",
      "Done with the batch: 1015\n",
      "Done with the batch: 1016\n",
      "Done with the batch: 1017\n",
      "Done with the batch: 1018\n",
      "Done with the batch: 1019\n",
      "Done with the batch: 1020\n",
      "Done with the batch: 1021\n",
      "Done with the batch: 1022\n",
      "Done with the batch: 1023\n",
      "Done with the batch: 1024\n",
      "Done with the batch: 1025\n",
      "Done with the batch: 1026\n",
      "Done with the batch: 1027\n",
      "Done with the batch: 1028\n",
      "Done with the batch: 1029\n",
      "Done with the batch: 1030\n",
      "Done with the batch: 1031\n",
      "Done with the batch: 1032\n",
      "Done with the batch: 1033\n",
      "Done with the batch: 1034\n",
      "Done with the batch: 1035\n",
      "Done with the batch: 1036\n",
      "Done with the batch: 1037\n",
      "Done with the batch: 1038\n",
      "Done with the batch: 1039\n",
      "Done with the batch: 1040\n",
      "Done with the batch: 1041\n",
      "Done with the batch: 1042\n",
      "Done with the batch: 1043\n",
      "Done with the batch: 1044\n",
      "Done with the batch: 1045\n",
      "Done with the batch: 1046\n",
      "Done with the batch: 1047\n",
      "Done with the batch: 1048\n",
      "Done with the batch: 1049\n",
      "Done with the batch: 1050\n",
      "Done with the batch: 1051\n",
      "Done with the batch: 1052\n",
      "Done with the batch: 1053\n",
      "Done with the batch: 1054\n",
      "Done with the batch: 1055\n",
      "Done with the batch: 1056\n",
      "Done with the batch: 1057\n",
      "Done with the batch: 1058\n",
      "Done with the batch: 1059\n",
      "Done with the batch: 1060\n",
      "Done with the batch: 1061\n",
      "Done with the batch: 1062\n",
      "Done with the batch: 1063\n",
      "Done with the batch: 1064\n",
      "Done with the batch: 1065\n",
      "Done with the batch: 1066\n",
      "Done with the batch: 1067\n",
      "Done with the batch: 1068\n",
      "Done with the batch: 1069\n",
      "Done with the batch: 1070\n",
      "Done with the batch: 1071\n",
      "Done with the batch: 1072\n",
      "Done with the batch: 1073\n",
      "Done with the batch: 1074\n",
      "Done with the batch: 1075\n",
      "Done with the batch: 1076\n",
      "Done with the batch: 1077\n",
      "Done with the batch: 1078\n",
      "Done with the batch: 1079\n",
      "Done with the batch: 1080\n",
      "Done with the batch: 1081\n",
      "Done with the batch: 1082\n",
      "Done with the batch: 1083\n",
      "Done with the batch: 1084\n",
      "Done with the batch: 1085\n",
      "Done with the batch: 1086\n",
      "Done with the batch: 1087\n",
      "Done with the batch: 1088\n",
      "Done with the batch: 1089\n",
      "Done with the batch: 1090\n",
      "Done with the batch: 1091\n",
      "Done with the batch: 1092\n",
      "Done with the batch: 1093\n",
      "Done with the batch: 1094\n",
      "Done with the batch: 1095\n",
      "Done with the batch: 1096\n",
      "Done with the batch: 1097\n",
      "Done with the batch: 1098\n",
      "Done with the batch: 1099\n",
      "Done with the batch: 1100\n",
      "Done with the batch: 1101\n",
      "Done with the batch: 1102\n",
      "Done with the batch: 1103\n",
      "Done with the batch: 1104\n",
      "Done with the batch: 1105\n",
      "Done with the batch: 1106\n",
      "Done with the batch: 1107\n",
      "Done with the batch: 1108\n",
      "Done with the batch: 1109\n",
      "Done with the batch: 1110\n",
      "Done with the batch: 1111\n",
      "Done with the batch: 1112\n",
      "Done with the batch: 1113\n",
      "Done with the batch: 1114\n",
      "Done with the batch: 1115\n",
      "Done with the batch: 1116\n",
      "Done with the batch: 1117\n",
      "Done with the batch: 1118\n",
      "Done with the batch: 1119\n",
      "Done with the batch: 1120\n",
      "Done with the batch: 1121\n",
      "Done with the batch: 1122\n",
      "Done with the batch: 1123\n",
      "Done with the batch: 1124\n",
      "Done with the batch: 1125\n",
      "Done with the batch: 1126\n",
      "Done with the batch: 1127\n",
      "Done with the batch: 1128\n",
      "Done with the batch: 1129\n",
      "Done with the batch: 1130\n",
      "Done with the batch: 1131\n",
      "Done with the batch: 1132\n",
      "Done with the batch: 1133\n",
      "Done with the batch: 1134\n",
      "Done with the batch: 1135\n",
      "Done with the batch: 1136\n",
      "Done with the batch: 1137\n",
      "Done with the batch: 1138\n",
      "Done with the batch: 1139\n",
      "Done with the batch: 1140\n",
      "Done with the batch: 1141\n",
      "Done with the batch: 1142\n",
      "Done with the batch: 1143\n",
      "Done with the batch: 1144\n",
      "Done with the batch: 1145\n",
      "Done with the batch: 1146\n",
      "Done with the batch: 1147\n",
      "Done with the batch: 1148\n",
      "Done with the batch: 1149\n",
      "Done with the batch: 1150\n",
      "Done with the batch: 1151\n",
      "Done with the batch: 1152\n",
      "Done with the batch: 1153\n",
      "Done with the batch: 1154\n",
      "Done with the batch: 1155\n",
      "Done with the batch: 1156\n",
      "Done with the batch: 1157\n",
      "Done with the batch: 1158\n",
      "Done with the batch: 1159\n",
      "Done with the batch: 1160\n",
      "Done with the batch: 1161\n",
      "Done with the batch: 1162\n",
      "Done with the batch: 1163\n",
      "Done with the batch: 1164\n",
      "Done with the batch: 1165\n",
      "Done with the batch: 1166\n",
      "Done with the batch: 1167\n",
      "Done with the batch: 1168\n",
      "Done with the batch: 1169\n",
      "Done with the batch: 1170\n",
      "Done with the batch: 1171\n",
      "Done with the batch: 1172\n",
      "Done with the batch: 1173\n",
      "Done with the batch: 1174\n",
      "Done with the batch: 1175\n",
      "Done with the batch: 1176\n",
      "Done with the batch: 1177\n",
      "Done with the batch: 1178\n",
      "Done with the batch: 1179\n",
      "Done with the batch: 1180\n",
      "Done with the batch: 1181\n",
      "Done with the batch: 1182\n",
      "Done with the batch: 1183\n",
      "Done with the batch: 1184\n",
      "Done with the batch: 1185\n",
      "Done with the batch: 1186\n",
      "Done with the batch: 1187\n",
      "Done with the batch: 1188\n",
      "Done with the batch: 1189\n",
      "Done with the batch: 1190\n",
      "Done with the batch: 1191\n",
      "Done with the batch: 1192\n",
      "Done with the batch: 1193\n",
      "Done with the batch: 1194\n",
      "Done with the batch: 1195\n",
      "Done with the batch: 1196\n",
      "Done with the batch: 1197\n",
      "Done with the batch: 1198\n",
      "Done with the batch: 1199\n",
      "Done with the batch: 1200\n",
      "Done with the batch: 1201\n",
      "Done with the batch: 1202\n",
      "Done with the batch: 1203\n",
      "Done with the batch: 1204\n",
      "Done with the batch: 1205\n",
      "Done with the batch: 1206\n",
      "Done with the batch: 1207\n",
      "Done with the batch: 1208\n",
      "Done with the batch: 1209\n",
      "Done with the batch: 1210\n",
      "Done with the batch: 1211\n",
      "Done with the batch: 1212\n",
      "Done with the batch: 1213\n",
      "Done with the batch: 1214\n",
      "Done with the batch: 1215\n",
      "Done with the batch: 1216\n",
      "Done with the batch: 1217\n",
      "Done with the batch: 1218\n",
      "Done with the batch: 1219\n",
      "Done with the batch: 1220\n",
      "Done with the batch: 1221\n",
      "Done with the batch: 1222\n",
      "Done with the batch: 1223\n",
      "Done with the batch: 1224\n",
      "Done with the batch: 1225\n",
      "Done with the batch: 1226\n",
      "Done with the batch: 1227\n",
      "Done with the batch: 1228\n",
      "Done with the batch: 1229\n",
      "Done with the batch: 1230\n",
      "Done with the batch: 1231\n",
      "Done with the batch: 1232\n",
      "Done with the batch: 1233\n",
      "Done with the batch: 1234\n",
      "Done with the batch: 1235\n",
      "Done with the batch: 1236\n",
      "Done with the batch: 1237\n",
      "Done with the batch: 1238\n",
      "Done with the batch: 1239\n",
      "Done with the batch: 1240\n",
      "Done with the batch: 1241\n",
      "Done with the batch: 1242\n",
      "Done with the batch: 1243\n",
      "Done with the batch: 1244\n",
      "Done with the batch: 1245\n",
      "Done with the batch: 1246\n",
      "Done with the batch: 1247\n",
      "Done with the batch: 1248\n",
      "Done with the batch: 1249\n",
      "Done with the batch: 1250\n",
      "Done with the batch: 1251\n",
      "Done with the batch: 1252\n",
      "Done with the batch: 1253\n",
      "Done with the batch: 1254\n",
      "Done with the batch: 1255\n",
      "Done with the batch: 1256\n",
      "Done with the batch: 1257\n",
      "Done with the batch: 1258\n",
      "Done with the batch: 1259\n",
      "Done with the batch: 1260\n",
      "Done with the batch: 1261\n",
      "Done with the batch: 1262\n",
      "Done with the batch: 1263\n",
      "Done with the batch: 1264\n",
      "Done with the batch: 1265\n",
      "Done with the batch: 1266\n",
      "Done with the batch: 1267\n",
      "Done with the batch: 1268\n",
      "Done with the batch: 1269\n",
      "Done with the batch: 1270\n",
      "Done with the batch: 1271\n",
      "Done with the batch: 1272\n",
      "Done with the batch: 1273\n",
      "Done with the batch: 1274\n",
      "Done with the batch: 1275\n",
      "Done with the batch: 1276\n",
      "Done with the batch: 1277\n",
      "Done with the batch: 1278\n",
      "Done with the batch: 1279\n",
      "Done with the batch: 1280\n",
      "Done with the batch: 1281\n",
      "Done with the batch: 1282\n",
      "Done with the batch: 1283\n",
      "Done with the batch: 1284\n",
      "Done with the batch: 1285\n",
      "Done with the batch: 1286\n",
      "Done with the batch: 1287\n",
      "Done with the batch: 1288\n",
      "Done with the batch: 1289\n",
      "Done with the batch: 1290\n",
      "Done with the batch: 1291\n",
      "Done with the batch: 1292\n",
      "Done with the batch: 1293\n",
      "Done with the batch: 1294\n",
      "Done with the batch: 1295\n",
      "Done with the batch: 1296\n",
      "Done with the batch: 1297\n",
      "Done with the batch: 1298\n",
      "Done with the batch: 1299\n",
      "Done with the batch: 1300\n",
      "Done with the batch: 1301\n",
      "Done with the batch: 1302\n",
      "Done with the batch: 1303\n",
      "Done with the batch: 1304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 1305\n",
      "Done with the batch: 1306\n",
      "Done with the batch: 1307\n",
      "Done with the batch: 1308\n",
      "Done with the batch: 1309\n",
      "Done with the batch: 1310\n",
      "Done with the batch: 1311\n",
      "Done with the batch: 1312\n",
      "Done with the batch: 1313\n",
      "Done with the batch: 1314\n",
      "Done with the batch: 1315\n",
      "Done with the batch: 1316\n",
      "Done with the batch: 1317\n",
      "Done with the batch: 1318\n",
      "Done with the batch: 1319\n",
      "Done with the batch: 1320\n",
      "Done with the batch: 1321\n",
      "Done with the batch: 1322\n",
      "Done with the batch: 1323\n",
      "Done with the batch: 1324\n",
      "Done with the batch: 1325\n",
      "Done with the batch: 1326\n",
      "Done with the batch: 1327\n",
      "Done with the batch: 1328\n",
      "Done with the batch: 1329\n",
      "Done with the batch: 1330\n",
      "Done with the batch: 1331\n",
      "Done with the batch: 1332\n",
      "Done with the batch: 1333\n",
      "Done with the batch: 1334\n",
      "Done with the batch: 1335\n",
      "Done with the batch: 1336\n",
      "Done with the batch: 1337\n",
      "Done with the batch: 1338\n",
      "Done with the batch: 1339\n",
      "Done with the batch: 1340\n",
      "Done with the batch: 1341\n",
      "Done with the batch: 1342\n",
      "Done with the batch: 1343\n",
      "Done with the batch: 1344\n",
      "Done with the batch: 1345\n",
      "Done with the batch: 1346\n",
      "Done with the batch: 1347\n",
      "Done with the batch: 1348\n",
      "Done with the batch: 1349\n",
      "Done with the batch: 1350\n",
      "Done with the batch: 1351\n",
      "Done with the batch: 1352\n",
      "Done with the batch: 1353\n",
      "Done with the batch: 1354\n",
      "Done with the batch: 1355\n",
      "Done with the batch: 1356\n",
      "Done with the batch: 1357\n",
      "Done with the batch: 1358\n",
      "Done with the batch: 1359\n",
      "Done with the batch: 1360\n",
      "Done with the batch: 1361\n",
      "Done with the batch: 1362\n",
      "Done with the batch: 1363\n",
      "Done with the batch: 1364\n",
      "Done with the batch: 1365\n",
      "Done with the batch: 1366\n",
      "Done with the batch: 1367\n",
      "Done with the batch: 1368\n",
      "Done with the batch: 1369\n",
      "Done with the batch: 1370\n",
      "Done with the batch: 1371\n",
      "Done with the batch: 1372\n",
      "Done with the batch: 1373\n",
      "Done with the batch: 1374\n",
      "Done with the batch: 1375\n",
      "Done with the batch: 1376\n",
      "Done with the batch: 1377\n",
      "Done with the batch: 1378\n",
      "Done with the batch: 1379\n",
      "Done with the batch: 1380\n",
      "Done with the batch: 1381\n",
      "Done with the batch: 1382\n",
      "Done with the batch: 1383\n",
      "Done with the batch: 1384\n",
      "Done with the batch: 1385\n",
      "Done with the batch: 1386\n",
      "Done with the batch: 1387\n",
      "Done with the batch: 1388\n",
      "Done with the batch: 1389\n",
      "Done with the batch: 1390\n",
      "Done with the batch: 1391\n",
      "Done with the batch: 1392\n",
      "Done with the batch: 1393\n",
      "Done with the batch: 1394\n",
      "Done with the batch: 1395\n",
      "Done with the batch: 1396\n",
      "Done with the batch: 1397\n",
      "Done with the batch: 1398\n",
      "Done with the batch: 1399\n",
      "Done with the batch: 1400\n",
      "Done with the batch: 1401\n",
      "Done with the batch: 1402\n",
      "Done with the batch: 1403\n",
      "Done with the batch: 1404\n",
      "Done with the batch: 1405\n",
      "Done with the batch: 1406\n",
      "Done with the batch: 1407\n",
      "Done with the batch: 1408\n",
      "Done with the batch: 1409\n",
      "Done with the batch: 1410\n",
      "Done with the batch: 1411\n",
      "Done with the batch: 1412\n",
      "Done with the batch: 1413\n",
      "Done with the batch: 1414\n",
      "Done with the batch: 1415\n",
      "Done with the batch: 1416\n",
      "Done with the batch: 1417\n",
      "Done with the batch: 1418\n",
      "Done with the batch: 1419\n",
      "Done with the batch: 1420\n",
      "Done with the batch: 1421\n",
      "Done with the batch: 1422\n",
      "Done with the batch: 1423\n",
      "Done with the batch: 1424\n",
      "Done with the batch: 1425\n",
      "Done with the batch: 1426\n",
      "Done with the batch: 1427\n",
      "Done with the batch: 1428\n",
      "Done with the batch: 1429\n",
      "Done with the batch: 1430\n",
      "Done with the batch: 1431\n",
      "Done with the batch: 1432\n",
      "Done with the batch: 1433\n",
      "Done with the batch: 1434\n",
      "Done with the batch: 1435\n",
      "Done with the batch: 1436\n",
      "Done with the batch: 1437\n",
      "Done with the batch: 1438\n",
      "Done with the batch: 1439\n",
      "Done with the batch: 1440\n",
      "Done with the batch: 1441\n",
      "Done with the batch: 1442\n",
      "Done with the batch: 1443\n",
      "Done with the batch: 1444\n",
      "Done with the batch: 1445\n",
      "Done with the batch: 1446\n",
      "Done with the batch: 1447\n",
      "Done with the batch: 1448\n",
      "Done with the batch: 1449\n",
      "Done with the batch: 1450\n",
      "Done with the batch: 1451\n",
      "Done with the batch: 1452\n",
      "Done with the batch: 1453\n",
      "Done with the batch: 1454\n",
      "Done with the batch: 1455\n",
      "Done with the batch: 1456\n",
      "Done with the batch: 1457\n",
      "Done with the batch: 1458\n",
      "Done with the batch: 1459\n",
      "Done with the batch: 1460\n",
      "Done with the batch: 1461\n",
      "Done with the batch: 1462\n",
      "Done with the batch: 1463\n",
      "Done with the batch: 1464\n",
      "Done with the batch: 1465\n",
      "Done with the batch: 1466\n",
      "Done with the batch: 1467\n",
      "Done with the batch: 1468\n",
      "Done with the batch: 1469\n",
      "Done with the batch: 1470\n",
      "Done with the batch: 1471\n",
      "Done with the batch: 1472\n",
      "Done with the batch: 1473\n",
      "Done with the batch: 1474\n",
      "Done with the batch: 1475\n",
      "Done with the batch: 1476\n",
      "Done with the batch: 1477\n",
      "Done with the batch: 1478\n",
      "Done with the batch: 1479\n",
      "Done with the batch: 1480\n",
      "Done with the batch: 1481\n",
      "Done with the batch: 1482\n",
      "Done with the batch: 1483\n",
      "Done with the batch: 1484\n",
      "Done with the batch: 1485\n",
      "Done with the batch: 1486\n",
      "Done with the batch: 1487\n",
      "Done with the batch: 1488\n",
      "Done with the batch: 1489\n",
      "Done with the batch: 1490\n",
      "Done with the batch: 1491\n",
      "Done with the batch: 1492\n",
      "Done with the batch: 1493\n",
      "Done with the batch: 1494\n",
      "Done with the batch: 1495\n",
      "Done with the batch: 1496\n",
      "Done with the batch: 1497\n",
      "Done with the batch: 1498\n",
      "Done with the batch: 1499\n",
      "Done with the batch: 1500\n",
      "Done with the batch: 1501\n",
      "Done with the batch: 1502\n",
      "Done with the batch: 1503\n",
      "Done with the batch: 1504\n",
      "Done with the batch: 1505\n",
      "Done with the batch: 1506\n",
      "Done with the batch: 1507\n",
      "Done with the batch: 1508\n",
      "Done with the batch: 1509\n",
      "Done with the batch: 1510\n",
      "Done with the batch: 1511\n",
      "Done with the batch: 1512\n",
      "Done with the batch: 1513\n",
      "Done with the batch: 1514\n",
      "Done with the batch: 1515\n",
      "Done with the batch: 1516\n",
      "Done with the batch: 1517\n",
      "Done with the batch: 1518\n",
      "Done with the batch: 1519\n",
      "Done with the batch: 1520\n",
      "Done with the batch: 1521\n",
      "Done with the batch: 1522\n",
      "Done with the batch: 1523\n",
      "Done with the batch: 1524\n",
      "Done with the batch: 1525\n",
      "Done with the batch: 1526\n",
      "Done with the batch: 1527\n",
      "Done with the batch: 1528\n",
      "Done with the batch: 1529\n",
      "Done with the batch: 1530\n",
      "Done with the batch: 1531\n",
      "Done with the batch: 1532\n",
      "Done with the batch: 1533\n",
      "Done with the batch: 1534\n",
      "Done with the batch: 1535\n",
      "Done with the batch: 1536\n",
      "Done with the batch: 1537\n",
      "Done with the batch: 1538\n",
      "Done with the batch: 1539\n",
      "Done with the batch: 1540\n",
      "Done with the batch: 1541\n",
      "Done with the batch: 1542\n",
      "Done with the batch: 1543\n",
      "Done with the batch: 1544\n",
      "Done with the batch: 1545\n",
      "Done with the batch: 1546\n",
      "Done with the batch: 1547\n",
      "Done with the batch: 1548\n",
      "Done with the batch: 1549\n",
      "Done with the batch: 1550\n",
      "Done with the batch: 1551\n",
      "Done with the batch: 1552\n",
      "Done with the batch: 1553\n",
      "Done with the batch: 1554\n",
      "Done with the batch: 1555\n",
      "Done with the batch: 1556\n",
      "Done with the batch: 1557\n",
      "Done with the batch: 1558\n",
      "Done with the batch: 1559\n",
      "Done with the batch: 1560\n",
      "Done with the batch: 1561\n",
      "Done with the batch: 1562\n",
      "Done with the batch: 1563\n",
      "Done with the batch: 1564\n",
      "Done with the batch: 1565\n",
      "Done with the batch: 1566\n",
      "Done with the batch: 1567\n",
      "Done with the batch: 1568\n",
      "Done with the batch: 1569\n",
      "Done with the batch: 1570\n",
      "Done with the batch: 1571\n",
      "Done with the batch: 1572\n",
      "Done with the batch: 1573\n",
      "Done with the batch: 1574\n",
      "Done with the batch: 1575\n",
      "Done with the batch: 1576\n",
      "Done with the batch: 1577\n",
      "Done with the batch: 1578\n",
      "Done with the batch: 1579\n",
      "Done with the batch: 1580\n",
      "Done with the batch: 1581\n",
      "Done with the batch: 1582\n",
      "Done with the batch: 1583\n",
      "Done with the batch: 1584\n",
      "Done with the batch: 1585\n",
      "Done with the batch: 1586\n",
      "Done with the batch: 1587\n",
      "Done with the batch: 1588\n",
      "Done with the batch: 1589\n",
      "Done with the batch: 1590\n",
      "Done with the batch: 1591\n",
      "Done with the batch: 1592\n",
      "Done with the batch: 1593\n",
      "Done with the batch: 1594\n",
      "Done with the batch: 1595\n",
      "Done with the batch: 1596\n",
      "Done with the batch: 1597\n",
      "Done with the batch: 1598\n",
      "Done with the batch: 1599\n",
      "Done with the batch: 1600\n",
      "Done with the batch: 1601\n",
      "Done with the batch: 1602\n",
      "Done with the batch: 1603\n",
      "Done with the batch: 1604\n",
      "Done with the batch: 1605\n",
      "Done with the batch: 1606\n",
      "Done with the batch: 1607\n",
      "Done with the batch: 1608\n",
      "Done with the batch: 1609\n",
      "Done with the batch: 1610\n",
      "Done with the batch: 1611\n",
      "Done with the batch: 1612\n",
      "Done with the batch: 1613\n",
      "Done with the batch: 1614\n",
      "Done with the batch: 1615\n",
      "Done with the batch: 1616\n",
      "Done with the batch: 1617\n",
      "(6470, 18432) (6470,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"e2809f76-6c51-4626-b78a-716b3d16aa5b\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"e2809f76-6c51-4626-b78a-716b3d16aa5b\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "X_Train_FeatureMap=np.empty((0,18432))\n",
    "Y_Train_FeatureMap=np.empty((0,batch_size))\n",
    "print(X_Train_FeatureMap.shape)\n",
    "for i,data in enumerate(trainloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    featureMap=net.get_Representation_Net(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Train_FeatureMap=np.append(X_Train_FeatureMap,featureMap,axis=0)\n",
    "    Y_Train_FeatureMap=np.append(Y_Train_FeatureMap,labels.numpy())\n",
    "print(X_Train_FeatureMap.shape,Y_Train_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d381adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 18432)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "(1618, 18432) (1618,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"176e2e59-2baf-4220-8551-47b40c7495ea\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"176e2e59-2baf-4220-8551-47b40c7495ea\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "X_Test_FeatureMap=np.empty((0,18432))\n",
    "Y_Test_FeatureMap=np.empty((0,batch_size))\n",
    "print(X_Test_FeatureMap.shape)\n",
    "for i,data in enumerate(testloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    featuremap=net.get_Representation_Net(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Test_FeatureMap=np.append(X_Test_FeatureMap,featuremap,axis=0)\n",
    "    Y_Test_FeatureMap=np.append(Y_Test_FeatureMap,labels.numpy())\n",
    "print(X_Test_FeatureMap.shape,Y_Test_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba597221",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f58470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3e0a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV 1/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 1/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.501 total time=  11.3s\n",
      "[CV 2/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 2/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.502 total time=  11.5s\n",
      "[CV 3/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 3/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.502 total time=  10.3s\n",
      "[CV 4/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 4/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.502 total time=  10.3s\n",
      "[CV 5/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 5/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.502 total time=  10.2s\n",
      "[CV 1/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 1/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.501 total time=  10.0s\n",
      "[CV 2/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 2/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.502 total time=  10.0s\n",
      "[CV 3/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 3/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.502 total time=   9.9s\n",
      "[CV 4/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 4/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.502 total time=  10.1s\n",
      "[CV 5/5; 2/25] START C=0.001, gamma=0.01, kernel=rbf............................\n",
      "[CV 5/5; 2/25] END C=0.001, gamma=0.01, kernel=rbf;, score=0.502 total time=  10.0s\n",
      "[CV 1/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 1/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.501 total time=  10.7s\n",
      "[CV 2/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 2/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 3/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 3/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 4/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 4/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 5/5; 3/25] START C=0.001, gamma=0.1, kernel=rbf.............................\n",
      "[CV 5/5; 3/25] END C=0.001, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 1/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 1/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.501 total time=  11.0s\n",
      "[CV 2/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 2/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.502 total time=  11.2s\n",
      "[CV 3/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 3/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.502 total time=  11.1s\n",
      "[CV 4/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 4/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.502 total time=  10.9s\n",
      "[CV 5/5; 4/25] START C=0.001, gamma=1, kernel=rbf...............................\n",
      "[CV 5/5; 4/25] END C=0.001, gamma=1, kernel=rbf;, score=0.502 total time=  11.1s\n",
      "[CV 1/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 1/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.501 total time=  11.2s\n",
      "[CV 2/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 2/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.502 total time=  11.2s\n",
      "[CV 3/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 3/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.502 total time=  11.1s\n",
      "[CV 4/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 4/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.502 total time=  11.2s\n",
      "[CV 5/5; 5/25] START C=0.001, gamma=10, kernel=rbf..............................\n",
      "[CV 5/5; 5/25] END C=0.001, gamma=10, kernel=rbf;, score=0.502 total time=  11.2s\n",
      "[CV 1/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 1/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.923 total time=   6.6s\n",
      "[CV 2/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 2/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.922 total time=   6.6s\n",
      "[CV 3/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 3/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.918 total time=   6.7s\n",
      "[CV 4/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 4/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.937 total time=   6.7s\n",
      "[CV 5/5; 6/25] START C=0.1, gamma=0.001, kernel=rbf.............................\n",
      "[CV 5/5; 6/25] END C=0.1, gamma=0.001, kernel=rbf;, score=0.930 total time=   6.7s\n",
      "[CV 1/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 1/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.527 total time=   9.9s\n",
      "[CV 2/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 2/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.536 total time=   9.9s\n",
      "[CV 3/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 3/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.528 total time=   9.9s\n",
      "[CV 4/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 4/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.532 total time=   9.8s\n",
      "[CV 5/5; 7/25] START C=0.1, gamma=0.01, kernel=rbf..............................\n",
      "[CV 5/5; 7/25] END C=0.1, gamma=0.01, kernel=rbf;, score=0.532 total time=   9.9s\n",
      "[CV 1/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 1/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.501 total time=  11.0s\n",
      "[CV 2/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 2/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.9s\n",
      "[CV 3/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 3/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.9s\n",
      "[CV 4/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 4/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 5/5; 8/25] START C=0.1, gamma=0.1, kernel=rbf...............................\n",
      "[CV 5/5; 8/25] END C=0.1, gamma=0.1, kernel=rbf;, score=0.502 total time=  10.8s\n",
      "[CV 1/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 1/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.501 total time=  11.0s\n",
      "[CV 2/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 2/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=  11.1s\n",
      "[CV 3/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 3/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=  11.0s\n",
      "[CV 4/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 4/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=  11.0s\n",
      "[CV 5/5; 9/25] START C=0.1, gamma=1, kernel=rbf.................................\n",
      "[CV 5/5; 9/25] END ..C=0.1, gamma=1, kernel=rbf;, score=0.502 total time=  11.0s\n",
      "[CV 1/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 1/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.501 total time=  11.2s\n",
      "[CV 2/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 2/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.502 total time=  11.1s\n",
      "[CV 3/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 3/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.502 total time=  11.0s\n",
      "[CV 4/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 4/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.502 total time=  11.1s\n",
      "[CV 5/5; 10/25] START C=0.1, gamma=10, kernel=rbf...............................\n",
      "[CV 5/5; 10/25] END C=0.1, gamma=10, kernel=rbf;, score=0.502 total time=  11.1s\n",
      "[CV 1/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.954 total time=   4.6s\n",
      "[CV 2/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 2/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.949 total time=   4.7s\n",
      "[CV 3/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 3/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.945 total time=   4.5s\n",
      "[CV 4/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 4/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.959 total time=   4.7s\n",
      "[CV 5/5; 11/25] START C=1, gamma=0.001, kernel=rbf..............................\n",
      "[CV 5/5; 11/25] END C=1, gamma=0.001, kernel=rbf;, score=0.957 total time=   4.7s\n",
      "[CV 1/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 1/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.652 total time=  10.0s\n",
      "[CV 2/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 2/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.678 total time=  10.0s\n",
      "[CV 3/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 3/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.660 total time=  10.0s\n",
      "[CV 4/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 4/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.678 total time=   9.8s\n",
      "[CV 5/5; 12/25] START C=1, gamma=0.01, kernel=rbf...............................\n",
      "[CV 5/5; 12/25] END C=1, gamma=0.01, kernel=rbf;, score=0.671 total time=   9.6s\n",
      "[CV 1/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 1/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.591 total time=  10.9s\n",
      "[CV 2/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 2/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.607 total time=  11.0s\n",
      "[CV 3/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 3/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.607 total time=  10.6s\n",
      "[CV 4/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 4/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.610 total time=  10.7s\n",
      "[CV 5/5; 13/25] START C=1, gamma=0.1, kernel=rbf................................\n",
      "[CV 5/5; 13/25] END .C=1, gamma=0.1, kernel=rbf;, score=0.614 total time=  10.6s\n",
      "[CV 1/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 1/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.569 total time=  10.8s\n",
      "[CV 2/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 2/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.580 total time=  11.1s\n",
      "[CV 3/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 3/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.581 total time=  11.0s\n",
      "[CV 4/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 4/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.585 total time=  10.9s\n",
      "[CV 5/5; 14/25] START C=1, gamma=1, kernel=rbf..................................\n",
      "[CV 5/5; 14/25] END ...C=1, gamma=1, kernel=rbf;, score=0.574 total time=  10.9s\n",
      "[CV 1/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 1/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.545 total time=  11.0s\n",
      "[CV 2/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 2/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.549 total time=  11.0s\n",
      "[CV 3/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 3/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.556 total time=  11.1s\n",
      "[CV 4/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 4/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.561 total time=  10.9s\n",
      "[CV 5/5; 15/25] START C=1, gamma=10, kernel=rbf.................................\n",
      "[CV 5/5; 15/25] END ..C=1, gamma=10, kernel=rbf;, score=0.546 total time=  11.0s\n",
      "[CV 1/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 1/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.954 total time=   4.6s\n",
      "[CV 2/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 2/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.948 total time=   4.6s\n",
      "[CV 3/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 3/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.946 total time=   4.5s\n",
      "[CV 4/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 4/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.951 total time=   4.7s\n",
      "[CV 5/5; 16/25] START C=10, gamma=0.001, kernel=rbf.............................\n",
      "[CV 5/5; 16/25] END C=10, gamma=0.001, kernel=rbf;, score=0.957 total time=   4.5s\n",
      "[CV 1/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 1/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.658 total time=  10.1s\n",
      "[CV 2/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 2/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.685 total time=   9.8s\n",
      "[CV 3/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 3/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.662 total time=   9.6s\n",
      "[CV 4/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 4/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.685 total time=   9.8s\n",
      "[CV 5/5; 17/25] START C=10, gamma=0.01, kernel=rbf..............................\n",
      "[CV 5/5; 17/25] END C=10, gamma=0.01, kernel=rbf;, score=0.677 total time=   9.9s\n",
      "[CV 1/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 1/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.591 total time=  10.5s\n",
      "[CV 2/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 2/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.608 total time=  10.6s\n",
      "[CV 3/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 3/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.608 total time=  10.6s\n",
      "[CV 4/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 4/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.611 total time=  10.7s\n",
      "[CV 5/5; 18/25] START C=10, gamma=0.1, kernel=rbf...............................\n",
      "[CV 5/5; 18/25] END C=10, gamma=0.1, kernel=rbf;, score=0.617 total time=  10.7s\n",
      "[CV 1/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 1/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.570 total time=  10.7s\n",
      "[CV 2/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 2/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.582 total time=  10.9s\n",
      "[CV 3/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 3/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.584 total time=  10.8s\n",
      "[CV 4/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 4/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.589 total time=  10.8s\n",
      "[CV 5/5; 19/25] START C=10, gamma=1, kernel=rbf.................................\n",
      "[CV 5/5; 19/25] END ..C=10, gamma=1, kernel=rbf;, score=0.577 total time=  10.9s\n",
      "[CV 1/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 1/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.546 total time=  10.8s\n",
      "[CV 2/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 2/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.552 total time=  10.9s\n",
      "[CV 3/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 3/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.559 total time=  10.7s\n",
      "[CV 4/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 4/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.561 total time=  10.7s\n",
      "[CV 5/5; 20/25] START C=10, gamma=10, kernel=rbf................................\n",
      "[CV 5/5; 20/25] END .C=10, gamma=10, kernel=rbf;, score=0.548 total time=  10.7s\n",
      "[CV 1/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 1/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.948 total time=   4.4s\n",
      "[CV 2/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.946 total time=   4.5s\n",
      "[CV 3/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 3/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.944 total time=   4.3s\n",
      "[CV 4/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 4/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.951 total time=   4.6s\n",
      "[CV 5/5; 21/25] START C=100, gamma=0.001, kernel=rbf............................\n",
      "[CV 5/5; 21/25] END C=100, gamma=0.001, kernel=rbf;, score=0.958 total time=   4.4s\n",
      "[CV 1/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 1/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.657 total time=  10.1s\n",
      "[CV 2/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 2/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.685 total time=   9.9s\n",
      "[CV 3/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 3/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.662 total time=   9.9s\n",
      "[CV 4/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 4/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.683 total time=  10.2s\n",
      "[CV 5/5; 22/25] START C=100, gamma=0.01, kernel=rbf.............................\n",
      "[CV 5/5; 22/25] END C=100, gamma=0.01, kernel=rbf;, score=0.677 total time=   9.9s\n",
      "[CV 1/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 1/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.591 total time=  10.5s\n",
      "[CV 2/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 2/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.608 total time=  10.5s\n",
      "[CV 3/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 3/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.607 total time=  10.5s\n",
      "[CV 4/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 4/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.611 total time=  10.6s\n",
      "[CV 5/5; 23/25] START C=100, gamma=0.1, kernel=rbf..............................\n",
      "[CV 5/5; 23/25] END C=100, gamma=0.1, kernel=rbf;, score=0.617 total time=  10.4s\n",
      "[CV 1/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 1/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.570 total time=  10.9s\n",
      "[CV 2/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 2/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.582 total time=  10.8s\n",
      "[CV 3/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 3/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.584 total time=  10.8s\n",
      "[CV 4/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 4/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.589 total time=  10.8s\n",
      "[CV 5/5; 24/25] START C=100, gamma=1, kernel=rbf................................\n",
      "[CV 5/5; 24/25] END .C=100, gamma=1, kernel=rbf;, score=0.577 total time=  10.8s\n",
      "[CV 1/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 1/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.546 total time=  10.8s\n",
      "[CV 2/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 2/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.552 total time=  10.9s\n",
      "[CV 3/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 3/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.559 total time=  10.8s\n",
      "[CV 4/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 4/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.561 total time=  10.7s\n",
      "[CV 5/5; 25/25] START C=100, gamma=10, kernel=rbf...............................\n",
      "[CV 5/5; 25/25] END C=100, gamma=10, kernel=rbf;, score=0.548 total time=  10.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.001, 0.1, 1, 10, 100],\n",
       "                         'gamma': [0.001, 0.01, 0.1, 1, 10],\n",
       "                         'kernel': ['rbf']},\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_parameters = {'kernel': ['rbf'], 'gamma': [1e-3, 1e-2,0.1,1,10],\n",
    "                     'C': [0.001,0.1,1, 10, 100],\n",
    "}\n",
    "# tuned_parameters = {'kernel': ['rbf'], 'gamma': [1e-3],\n",
    "#                      'C': [0.001],\n",
    "#                    }\n",
    "clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring= 'accuracy',verbose=10\n",
    "    )\n",
    "clf.fit(X_Train, Y_Train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15b74644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_svm(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "#         clf=SVC(C=10,kernel='rbf',gamma=0.0001)\n",
    "        clf=SVC(C=10,kernel='rbf',gamma=10)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del clf\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f8f7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 96.29057187017001\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 96.44513137557959\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 95.90417310664606\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 96.36785162287481\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 97.2952086553323\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 96.29057187017001 %\n",
      "Fold 1: 96.44513137557959 %\n",
      "Fold 2: 95.90417310664606 %\n",
      "Fold 3: 96.36785162287481 %\n",
      "Fold 4: 97.2952086553323 %\n",
      "Average: 96.46058732612056 %\n",
      "Accuracy:  0.9320148331273177\n",
      "F1-Score:  0.9334140435835352\n",
      "Precision:  0.9244604316546763\n",
      "Recall:  0.9425427872860636\n",
      "AUC:  0.9318963936430317\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_svm(X_Train,Y_Train.ravel())\n",
    "clf=SVC(C=10,kernel='rbf',gamma=1e-5)\n",
    "clf.fit(X_Train,Y_Train.ravel())\n",
    "y_pred = clf.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6730c",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cfd5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7bc865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_dtree(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        decision_tree = DecisionTreeClassifier(random_state=102)\n",
    "        decision_tree = decision_tree.fit(X_train, y_train.ravel())\n",
    "        y_pred = decision_tree.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del decision_tree\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28879968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 93.58578052550232\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 93.89489953632149\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 92.42658423493046\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 92.50386398763524\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 93.35394126738794\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 93.58578052550232 %\n",
      "Fold 1: 93.89489953632149 %\n",
      "Fold 2: 92.42658423493046 %\n",
      "Fold 3: 92.50386398763524 %\n",
      "Fold 4: 93.35394126738794 %\n",
      "Average: 93.1530139103555 %\n",
      "Accuracy:  0.896168108776267\n",
      "F1-Score:  0.8992805755395683\n",
      "Precision:  0.8823529411764706\n",
      "Recall:  0.9168704156479217\n",
      "AUC:  0.8959352078239609\n"
     ]
    }
   ],
   "source": [
    "dtree=k_fold_cv_dtree(X_Train,Y_Train.ravel())\n",
    "dtree = DecisionTreeClassifier(random_state=102)\n",
    "dtree = dtree.fit(X_Train, Y_Train.ravel())\n",
    "y_pred=dtree.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af007b78",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58168511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "195c8a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_rforest(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        random_forest = RandomForestClassifier(n_estimators=100,criterion='gini',random_state=102)\n",
    "        random_forest = random_forest.fit(X_train, y_train.ravel())\n",
    "        y_pred = random_forest.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del random_forest\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b857b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 96.98608964451314\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 96.44513137557959\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 96.13601236476043\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 95.90417310664606\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 95.74961360123648\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 96.98608964451314 %\n",
      "Fold 1: 96.44513137557959 %\n",
      "Fold 2: 96.13601236476043 %\n",
      "Fold 3: 95.90417310664606 %\n",
      "Fold 4: 95.74961360123648 %\n",
      "Average: 96.24420401854714 %\n",
      "Accuracy:  0.9227441285537701\n",
      "F1-Score:  0.924653405666064\n",
      "Precision:  0.9120095124851367\n",
      "Recall:  0.9376528117359413\n",
      "AUC:  0.9225764058679706\n"
     ]
    }
   ],
   "source": [
    "random_forest=k_fold_cv_rforest(X_Train,Y_Train.ravel())\n",
    "random_forest = RandomForestClassifier(n_estimators=100,criterion='gini',random_state=102)\n",
    "random_forest = random_forest.fit(X_Train, Y_Train.ravel())\n",
    "y_pred=random_forest.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8528f0",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddd41414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4649d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_xgb(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        eval_set = [(X_train, y_train.ravel()), (X_test, y_test)]\n",
    "        xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=102,use_label_encoder=False)\n",
    "        xg_cl.fit(X_train,y_train.ravel())\n",
    "        y_pred = xg_cl.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del xg_cl\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return xg_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6861e6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "[16:14:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.44513137557959\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "[16:14:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.44513137557959\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "[16:15:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 97.06336939721793\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "[16:15:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.83153013910355\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "[16:15:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.83153013910355\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 96.44513137557959 %\n",
      "Fold 1: 96.44513137557959 %\n",
      "Fold 2: 97.06336939721793 %\n",
      "Fold 3: 96.83153013910355 %\n",
      "Fold 4: 96.83153013910355 %\n",
      "Average: 96.72333848531684 %\n",
      "[16:15:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy:  0.9208899876390606\n",
      "F1-Score:  0.9227985524728589\n",
      "Precision:  0.9107142857142857\n",
      "Recall:  0.9352078239608802\n",
      "AUC:  0.92072891198044\n"
     ]
    }
   ],
   "source": [
    "xg=k_fold_cv_xgb(X_Train,Y_Train.ravel())\n",
    "xg = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=102,use_label_encoder=False)\n",
    "xg.fit(X_Train,Y_Train.ravel())\n",
    "y_pred=xg.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d3c44",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40685fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94b7c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_mlp(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        clf = MLPClassifier(random_state=102, max_iter=3000, verbose=True).fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del clf\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c47ffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.54593410\n",
      "Iteration 2, loss = 0.22878559\n",
      "Iteration 3, loss = 0.16003129\n",
      "Iteration 4, loss = 0.13774859\n",
      "Iteration 5, loss = 0.12412354\n",
      "Iteration 6, loss = 0.11117233\n",
      "Iteration 7, loss = 0.10380421\n",
      "Iteration 8, loss = 0.10891287\n",
      "Iteration 9, loss = 0.10628621\n",
      "Iteration 10, loss = 0.09000050\n",
      "Iteration 11, loss = 0.08255127\n",
      "Iteration 12, loss = 0.08004466\n",
      "Iteration 13, loss = 0.07846185\n",
      "Iteration 14, loss = 0.07312209\n",
      "Iteration 15, loss = 0.07290612\n",
      "Iteration 16, loss = 0.06944455\n",
      "Iteration 17, loss = 0.07156597\n",
      "Iteration 18, loss = 0.06997977\n",
      "Iteration 19, loss = 0.07172111\n",
      "Iteration 20, loss = 0.06553321\n",
      "Iteration 21, loss = 0.06365331\n",
      "Iteration 22, loss = 0.06350726\n",
      "Iteration 23, loss = 0.06173899\n",
      "Iteration 24, loss = 0.06023843\n",
      "Iteration 25, loss = 0.05996533\n",
      "Iteration 26, loss = 0.05569646\n",
      "Iteration 27, loss = 0.05993054\n",
      "Iteration 28, loss = 0.05179110\n",
      "Iteration 29, loss = 0.05938255\n",
      "Iteration 30, loss = 0.05200380\n",
      "Iteration 31, loss = 0.05123529\n",
      "Iteration 32, loss = 0.04635628\n",
      "Iteration 33, loss = 0.04751289\n",
      "Iteration 34, loss = 0.04861269\n",
      "Iteration 35, loss = 0.05502320\n",
      "Iteration 36, loss = 0.04966135\n",
      "Iteration 37, loss = 0.05457727\n",
      "Iteration 38, loss = 0.06079502\n",
      "Iteration 39, loss = 0.06073271\n",
      "Iteration 40, loss = 0.05190459\n",
      "Iteration 41, loss = 0.04299665\n",
      "Iteration 42, loss = 0.04443495\n",
      "Iteration 43, loss = 0.04123250\n",
      "Iteration 44, loss = 0.04167143\n",
      "Iteration 45, loss = 0.04089418\n",
      "Iteration 46, loss = 0.04167418\n",
      "Iteration 47, loss = 0.05094879\n",
      "Iteration 48, loss = 0.04196237\n",
      "Iteration 49, loss = 0.04594179\n",
      "Iteration 50, loss = 0.04510111\n",
      "Iteration 51, loss = 0.04166881\n",
      "Iteration 52, loss = 0.04307982\n",
      "Iteration 53, loss = 0.03817642\n",
      "Iteration 54, loss = 0.03569250\n",
      "Iteration 55, loss = 0.03736923\n",
      "Iteration 56, loss = 0.03539247\n",
      "Iteration 57, loss = 0.04001644\n",
      "Iteration 58, loss = 0.05149446\n",
      "Iteration 59, loss = 0.03842915\n",
      "Iteration 60, loss = 0.04907535\n",
      "Iteration 61, loss = 0.04564660\n",
      "Iteration 62, loss = 0.03862578\n",
      "Iteration 63, loss = 0.03465281\n",
      "Iteration 64, loss = 0.04870210\n",
      "Iteration 65, loss = 0.03786909\n",
      "Iteration 66, loss = 0.03611706\n",
      "Iteration 67, loss = 0.03982608\n",
      "Iteration 68, loss = 0.03494694\n",
      "Iteration 69, loss = 0.03455514\n",
      "Iteration 70, loss = 0.03678584\n",
      "Iteration 71, loss = 0.03565882\n",
      "Iteration 72, loss = 0.03347940\n",
      "Iteration 73, loss = 0.03420816\n",
      "Iteration 74, loss = 0.02975048\n",
      "Iteration 75, loss = 0.03097468\n",
      "Iteration 76, loss = 0.02987614\n",
      "Iteration 77, loss = 0.02911973\n",
      "Iteration 78, loss = 0.03314059\n",
      "Iteration 79, loss = 0.03053606\n",
      "Iteration 80, loss = 0.03497191\n",
      "Iteration 81, loss = 0.03205450\n",
      "Iteration 82, loss = 0.03070887\n",
      "Iteration 83, loss = 0.03522317\n",
      "Iteration 84, loss = 0.03270000\n",
      "Iteration 85, loss = 0.02824947\n",
      "Iteration 86, loss = 0.03726259\n",
      "Iteration 87, loss = 0.03767262\n",
      "Iteration 88, loss = 0.03116729\n",
      "Iteration 89, loss = 0.02871311\n",
      "Iteration 90, loss = 0.02919254\n",
      "Iteration 91, loss = 0.03461391\n",
      "Iteration 92, loss = 0.03686909\n",
      "Iteration 93, loss = 0.02681294\n",
      "Iteration 94, loss = 0.03110658\n",
      "Iteration 95, loss = 0.03100576\n",
      "Iteration 96, loss = 0.03253415\n",
      "Iteration 97, loss = 0.02912073\n",
      "Iteration 98, loss = 0.02715847\n",
      "Iteration 99, loss = 0.02783241\n",
      "Iteration 100, loss = 0.02873539\n",
      "Iteration 101, loss = 0.03360708\n",
      "Iteration 102, loss = 0.03006770\n",
      "Iteration 103, loss = 0.02881104\n",
      "Iteration 104, loss = 0.02921869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 95.90417310664606\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.52402910\n",
      "Iteration 2, loss = 0.22207247\n",
      "Iteration 3, loss = 0.15958618\n",
      "Iteration 4, loss = 0.12879501\n",
      "Iteration 5, loss = 0.11951723\n",
      "Iteration 6, loss = 0.10936289\n",
      "Iteration 7, loss = 0.09990009\n",
      "Iteration 8, loss = 0.09460064\n",
      "Iteration 9, loss = 0.08864812\n",
      "Iteration 10, loss = 0.08946923\n",
      "Iteration 11, loss = 0.08364096\n",
      "Iteration 12, loss = 0.07562786\n",
      "Iteration 13, loss = 0.07223824\n",
      "Iteration 14, loss = 0.06883350\n",
      "Iteration 15, loss = 0.06866310\n",
      "Iteration 16, loss = 0.06514062\n",
      "Iteration 17, loss = 0.06688027\n",
      "Iteration 18, loss = 0.06311208\n",
      "Iteration 19, loss = 0.06070285\n",
      "Iteration 20, loss = 0.05680594\n",
      "Iteration 21, loss = 0.06282898\n",
      "Iteration 22, loss = 0.06809938\n",
      "Iteration 23, loss = 0.06174542\n",
      "Iteration 24, loss = 0.05692268\n",
      "Iteration 25, loss = 0.05069868\n",
      "Iteration 26, loss = 0.05530685\n",
      "Iteration 27, loss = 0.05250182\n",
      "Iteration 28, loss = 0.04918254\n",
      "Iteration 29, loss = 0.06281970\n",
      "Iteration 30, loss = 0.06170956\n",
      "Iteration 31, loss = 0.05076184\n",
      "Iteration 32, loss = 0.04983279\n",
      "Iteration 33, loss = 0.04970583\n",
      "Iteration 34, loss = 0.04385041\n",
      "Iteration 35, loss = 0.04517053\n",
      "Iteration 36, loss = 0.05720316\n",
      "Iteration 37, loss = 0.04536020\n",
      "Iteration 38, loss = 0.03919661\n",
      "Iteration 39, loss = 0.04128848\n",
      "Iteration 40, loss = 0.03796651\n",
      "Iteration 41, loss = 0.03943598\n",
      "Iteration 42, loss = 0.03705631\n",
      "Iteration 43, loss = 0.04432297\n",
      "Iteration 44, loss = 0.03789432\n",
      "Iteration 45, loss = 0.03450664\n",
      "Iteration 46, loss = 0.03590240\n",
      "Iteration 47, loss = 0.03550090\n",
      "Iteration 48, loss = 0.03824816\n",
      "Iteration 49, loss = 0.03560008\n",
      "Iteration 50, loss = 0.03979999\n",
      "Iteration 51, loss = 0.04507038\n",
      "Iteration 52, loss = 0.03640058\n",
      "Iteration 53, loss = 0.03815114\n",
      "Iteration 54, loss = 0.03976638\n",
      "Iteration 55, loss = 0.03640870\n",
      "Iteration 56, loss = 0.03515816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 95.82689335394127\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.52358791\n",
      "Iteration 2, loss = 0.22532814\n",
      "Iteration 3, loss = 0.15320196\n",
      "Iteration 4, loss = 0.12591412\n",
      "Iteration 5, loss = 0.11322922\n",
      "Iteration 6, loss = 0.10789743\n",
      "Iteration 7, loss = 0.09548208\n",
      "Iteration 8, loss = 0.09992568\n",
      "Iteration 9, loss = 0.08742777\n",
      "Iteration 10, loss = 0.08338596\n",
      "Iteration 11, loss = 0.07765461\n",
      "Iteration 12, loss = 0.07596527\n",
      "Iteration 13, loss = 0.07994299\n",
      "Iteration 14, loss = 0.07106402\n",
      "Iteration 15, loss = 0.06935801\n",
      "Iteration 16, loss = 0.06168658\n",
      "Iteration 17, loss = 0.05990426\n",
      "Iteration 18, loss = 0.05940930\n",
      "Iteration 19, loss = 0.05652893\n",
      "Iteration 20, loss = 0.05768436\n",
      "Iteration 21, loss = 0.05251056\n",
      "Iteration 22, loss = 0.05503828\n",
      "Iteration 23, loss = 0.05560110\n",
      "Iteration 24, loss = 0.05450517\n",
      "Iteration 25, loss = 0.05573679\n",
      "Iteration 26, loss = 0.04844896\n",
      "Iteration 27, loss = 0.04535694\n",
      "Iteration 28, loss = 0.04815158\n",
      "Iteration 29, loss = 0.04582082\n",
      "Iteration 30, loss = 0.04634646\n",
      "Iteration 31, loss = 0.04559259\n",
      "Iteration 32, loss = 0.04736655\n",
      "Iteration 33, loss = 0.04122916\n",
      "Iteration 34, loss = 0.04061554\n",
      "Iteration 35, loss = 0.05169162\n",
      "Iteration 36, loss = 0.04275469\n",
      "Iteration 37, loss = 0.04686433\n",
      "Iteration 38, loss = 0.04632002\n",
      "Iteration 39, loss = 0.03955546\n",
      "Iteration 40, loss = 0.03734112\n",
      "Iteration 41, loss = 0.04112900\n",
      "Iteration 42, loss = 0.04590281\n",
      "Iteration 43, loss = 0.04453692\n",
      "Iteration 44, loss = 0.03738068\n",
      "Iteration 45, loss = 0.03658947\n",
      "Iteration 46, loss = 0.03408366\n",
      "Iteration 47, loss = 0.03418791\n",
      "Iteration 48, loss = 0.03587907\n",
      "Iteration 49, loss = 0.03336153\n",
      "Iteration 50, loss = 0.03096190\n",
      "Iteration 51, loss = 0.04040512\n",
      "Iteration 52, loss = 0.03884063\n",
      "Iteration 53, loss = 0.03409538\n",
      "Iteration 54, loss = 0.03937502\n",
      "Iteration 55, loss = 0.03617923\n",
      "Iteration 56, loss = 0.03600064\n",
      "Iteration 57, loss = 0.03075487\n",
      "Iteration 58, loss = 0.02861371\n",
      "Iteration 59, loss = 0.02987129\n",
      "Iteration 60, loss = 0.02958850\n",
      "Iteration 61, loss = 0.03251993\n",
      "Iteration 62, loss = 0.03133616\n",
      "Iteration 63, loss = 0.03012143\n",
      "Iteration 64, loss = 0.03630873\n",
      "Iteration 65, loss = 0.04178456\n",
      "Iteration 66, loss = 0.02942613\n",
      "Iteration 67, loss = 0.02820541\n",
      "Iteration 68, loss = 0.03508665\n",
      "Iteration 69, loss = 0.02888860\n",
      "Iteration 70, loss = 0.02607382\n",
      "Iteration 71, loss = 0.02988957\n",
      "Iteration 72, loss = 0.02555138\n",
      "Iteration 73, loss = 0.02619191\n",
      "Iteration 74, loss = 0.02966125\n",
      "Iteration 75, loss = 0.02556939\n",
      "Iteration 76, loss = 0.02599521\n",
      "Iteration 77, loss = 0.02663943\n",
      "Iteration 78, loss = 0.02510191\n",
      "Iteration 79, loss = 0.02545574\n",
      "Iteration 80, loss = 0.03078014\n",
      "Iteration 81, loss = 0.02350481\n",
      "Iteration 82, loss = 0.02673396\n",
      "Iteration 83, loss = 0.02549320\n",
      "Iteration 84, loss = 0.02437987\n",
      "Iteration 85, loss = 0.03201973\n",
      "Iteration 86, loss = 0.02114196\n",
      "Iteration 87, loss = 0.02490608\n",
      "Iteration 88, loss = 0.02808406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89, loss = 0.02431730\n",
      "Iteration 90, loss = 0.02562028\n",
      "Iteration 91, loss = 0.02713848\n",
      "Iteration 92, loss = 0.02512891\n",
      "Iteration 93, loss = 0.02196572\n",
      "Iteration 94, loss = 0.02998886\n",
      "Iteration 95, loss = 0.02626398\n",
      "Iteration 96, loss = 0.02511460\n",
      "Iteration 97, loss = 0.02113427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 95.74961360123648\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.46657772\n",
      "Iteration 2, loss = 0.20149834\n",
      "Iteration 3, loss = 0.14948906\n",
      "Iteration 4, loss = 0.12914764\n",
      "Iteration 5, loss = 0.11472844\n",
      "Iteration 6, loss = 0.11221789\n",
      "Iteration 7, loss = 0.10406361\n",
      "Iteration 8, loss = 0.09732821\n",
      "Iteration 9, loss = 0.09127905\n",
      "Iteration 10, loss = 0.08372990\n",
      "Iteration 11, loss = 0.09139940\n",
      "Iteration 12, loss = 0.07860140\n",
      "Iteration 13, loss = 0.07291668\n",
      "Iteration 14, loss = 0.07835053\n",
      "Iteration 15, loss = 0.07425021\n",
      "Iteration 16, loss = 0.07028730\n",
      "Iteration 17, loss = 0.06646935\n",
      "Iteration 18, loss = 0.06378191\n",
      "Iteration 19, loss = 0.06631174\n",
      "Iteration 20, loss = 0.06182192\n",
      "Iteration 21, loss = 0.07042430\n",
      "Iteration 22, loss = 0.06161565\n",
      "Iteration 23, loss = 0.05884951\n",
      "Iteration 24, loss = 0.05993783\n",
      "Iteration 25, loss = 0.06405669\n",
      "Iteration 26, loss = 0.06709970\n",
      "Iteration 27, loss = 0.06672425\n",
      "Iteration 28, loss = 0.06118279\n",
      "Iteration 29, loss = 0.05691741\n",
      "Iteration 30, loss = 0.06224547\n",
      "Iteration 31, loss = 0.05219861\n",
      "Iteration 32, loss = 0.05241528\n",
      "Iteration 33, loss = 0.05739515\n",
      "Iteration 34, loss = 0.05218460\n",
      "Iteration 35, loss = 0.04624505\n",
      "Iteration 36, loss = 0.04218105\n",
      "Iteration 37, loss = 0.04941957\n",
      "Iteration 38, loss = 0.04504264\n",
      "Iteration 39, loss = 0.05303243\n",
      "Iteration 40, loss = 0.04373629\n",
      "Iteration 41, loss = 0.04882330\n",
      "Iteration 42, loss = 0.04429143\n",
      "Iteration 43, loss = 0.04422644\n",
      "Iteration 44, loss = 0.04369039\n",
      "Iteration 45, loss = 0.03980348\n",
      "Iteration 46, loss = 0.04140500\n",
      "Iteration 47, loss = 0.03546541\n",
      "Iteration 48, loss = 0.04100900\n",
      "Iteration 49, loss = 0.05032707\n",
      "Iteration 50, loss = 0.03968114\n",
      "Iteration 51, loss = 0.03725225\n",
      "Iteration 52, loss = 0.03463594\n",
      "Iteration 53, loss = 0.03292519\n",
      "Iteration 54, loss = 0.03334058\n",
      "Iteration 55, loss = 0.04987043\n",
      "Iteration 56, loss = 0.03883566\n",
      "Iteration 57, loss = 0.03446744\n",
      "Iteration 58, loss = 0.04157864\n",
      "Iteration 59, loss = 0.04933322\n",
      "Iteration 60, loss = 0.03961345\n",
      "Iteration 61, loss = 0.03691222\n",
      "Iteration 62, loss = 0.03314492\n",
      "Iteration 63, loss = 0.03750864\n",
      "Iteration 64, loss = 0.03366533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 95.82689335394127\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.51331967\n",
      "Iteration 2, loss = 0.21287841\n",
      "Iteration 3, loss = 0.15309072\n",
      "Iteration 4, loss = 0.14398318\n",
      "Iteration 5, loss = 0.13910631\n",
      "Iteration 6, loss = 0.11000780\n",
      "Iteration 7, loss = 0.10924513\n",
      "Iteration 8, loss = 0.10593517\n",
      "Iteration 9, loss = 0.10994875\n",
      "Iteration 10, loss = 0.09729439\n",
      "Iteration 11, loss = 0.08681031\n",
      "Iteration 12, loss = 0.07933679\n",
      "Iteration 13, loss = 0.07888754\n",
      "Iteration 14, loss = 0.07426619\n",
      "Iteration 15, loss = 0.07002749\n",
      "Iteration 16, loss = 0.07281546\n",
      "Iteration 17, loss = 0.06509766\n",
      "Iteration 18, loss = 0.06283823\n",
      "Iteration 19, loss = 0.06598554\n",
      "Iteration 20, loss = 0.06293745\n",
      "Iteration 21, loss = 0.06569253\n",
      "Iteration 22, loss = 0.06022358\n",
      "Iteration 23, loss = 0.05582042\n",
      "Iteration 24, loss = 0.07836686\n",
      "Iteration 25, loss = 0.06359949\n",
      "Iteration 26, loss = 0.05777781\n",
      "Iteration 27, loss = 0.06129522\n",
      "Iteration 28, loss = 0.05895332\n",
      "Iteration 29, loss = 0.05174043\n",
      "Iteration 30, loss = 0.05094418\n",
      "Iteration 31, loss = 0.04722619\n",
      "Iteration 32, loss = 0.05079493\n",
      "Iteration 33, loss = 0.04736530\n",
      "Iteration 34, loss = 0.05452909\n",
      "Iteration 35, loss = 0.04799958\n",
      "Iteration 36, loss = 0.05110582\n",
      "Iteration 37, loss = 0.04350668\n",
      "Iteration 38, loss = 0.04200527\n",
      "Iteration 39, loss = 0.04423831\n",
      "Iteration 40, loss = 0.04707089\n",
      "Iteration 41, loss = 0.05270008\n",
      "Iteration 42, loss = 0.05312012\n",
      "Iteration 43, loss = 0.04396303\n",
      "Iteration 44, loss = 0.03987230\n",
      "Iteration 45, loss = 0.03876002\n",
      "Iteration 46, loss = 0.04355489\n",
      "Iteration 47, loss = 0.04623884\n",
      "Iteration 48, loss = 0.04160489\n",
      "Iteration 49, loss = 0.03824971\n",
      "Iteration 50, loss = 0.03762876\n",
      "Iteration 51, loss = 0.03612671\n",
      "Iteration 52, loss = 0.04036839\n",
      "Iteration 53, loss = 0.03882614\n",
      "Iteration 54, loss = 0.04237902\n",
      "Iteration 55, loss = 0.04290521\n",
      "Iteration 56, loss = 0.03544664\n",
      "Iteration 57, loss = 0.03674496\n",
      "Iteration 58, loss = 0.03560333\n",
      "Iteration 59, loss = 0.03448411\n",
      "Iteration 60, loss = 0.03633566\n",
      "Iteration 61, loss = 0.04088866\n",
      "Iteration 62, loss = 0.03547359\n",
      "Iteration 63, loss = 0.03434482\n",
      "Iteration 64, loss = 0.03674856\n",
      "Iteration 65, loss = 0.03407016\n",
      "Iteration 66, loss = 0.03390822\n",
      "Iteration 67, loss = 0.03505793\n",
      "Iteration 68, loss = 0.03592316\n",
      "Iteration 69, loss = 0.04353144\n",
      "Iteration 70, loss = 0.03169315\n",
      "Iteration 71, loss = 0.03318292\n",
      "Iteration 72, loss = 0.03477367\n",
      "Iteration 73, loss = 0.03200265\n",
      "Iteration 74, loss = 0.02781492\n",
      "Iteration 75, loss = 0.03255485\n",
      "Iteration 76, loss = 0.03176250\n",
      "Iteration 77, loss = 0.03168660\n",
      "Iteration 78, loss = 0.04170680\n",
      "Iteration 79, loss = 0.04064150\n",
      "Iteration 80, loss = 0.03597984\n",
      "Iteration 81, loss = 0.03190136\n",
      "Iteration 82, loss = 0.02836155\n",
      "Iteration 83, loss = 0.03227031\n",
      "Iteration 84, loss = 0.02984259\n",
      "Iteration 85, loss = 0.02566690\n",
      "Iteration 86, loss = 0.02752859\n",
      "Iteration 87, loss = 0.02744493\n",
      "Iteration 88, loss = 0.02630636\n",
      "Iteration 89, loss = 0.02650198\n",
      "Iteration 90, loss = 0.03002633\n",
      "Iteration 91, loss = 0.03104641\n",
      "Iteration 92, loss = 0.03301543\n",
      "Iteration 93, loss = 0.03124058\n",
      "Iteration 94, loss = 0.03133813\n",
      "Iteration 95, loss = 0.02924297\n",
      "Iteration 96, loss = 0.03206179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 96.67697063369397\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 95.90417310664606 %\n",
      "Fold 1: 95.82689335394127 %\n",
      "Fold 2: 95.74961360123648 %\n",
      "Fold 3: 95.82689335394127 %\n",
      "Fold 4: 96.67697063369397 %\n",
      "Average: 95.9969088098918 %\n",
      "Iteration 1, loss = 0.52037849\n",
      "Iteration 2, loss = 0.20714346\n",
      "Iteration 3, loss = 0.14507311\n",
      "Iteration 4, loss = 0.12857481\n",
      "Iteration 5, loss = 0.11479177\n",
      "Iteration 6, loss = 0.10703364\n",
      "Iteration 7, loss = 0.10084929\n",
      "Iteration 8, loss = 0.09536018\n",
      "Iteration 9, loss = 0.09377110\n",
      "Iteration 10, loss = 0.08631270\n",
      "Iteration 11, loss = 0.08314245\n",
      "Iteration 12, loss = 0.07921222\n",
      "Iteration 13, loss = 0.09375083\n",
      "Iteration 14, loss = 0.07505624\n",
      "Iteration 15, loss = 0.06970010\n",
      "Iteration 16, loss = 0.06871668\n",
      "Iteration 17, loss = 0.07001431\n",
      "Iteration 18, loss = 0.06943244\n",
      "Iteration 19, loss = 0.06991024\n",
      "Iteration 20, loss = 0.06897837\n",
      "Iteration 21, loss = 0.06187064\n",
      "Iteration 22, loss = 0.06903029\n",
      "Iteration 23, loss = 0.06029693\n",
      "Iteration 24, loss = 0.06412260\n",
      "Iteration 25, loss = 0.05781373\n",
      "Iteration 26, loss = 0.05574535\n",
      "Iteration 27, loss = 0.06058146\n",
      "Iteration 28, loss = 0.05554309\n",
      "Iteration 29, loss = 0.05771250\n",
      "Iteration 30, loss = 0.05312114\n",
      "Iteration 31, loss = 0.05753767\n",
      "Iteration 32, loss = 0.05525718\n",
      "Iteration 33, loss = 0.05841446\n",
      "Iteration 34, loss = 0.06029384\n",
      "Iteration 35, loss = 0.07148468\n",
      "Iteration 36, loss = 0.05915715\n",
      "Iteration 37, loss = 0.05576760\n",
      "Iteration 38, loss = 0.05225615\n",
      "Iteration 39, loss = 0.05037408\n",
      "Iteration 40, loss = 0.04882976\n",
      "Iteration 41, loss = 0.04969845\n",
      "Iteration 42, loss = 0.05927293\n",
      "Iteration 43, loss = 0.05089668\n",
      "Iteration 44, loss = 0.05317688\n",
      "Iteration 45, loss = 0.04635016\n",
      "Iteration 46, loss = 0.04892146\n",
      "Iteration 47, loss = 0.04763754\n",
      "Iteration 48, loss = 0.04232092\n",
      "Iteration 49, loss = 0.04160974\n",
      "Iteration 50, loss = 0.04217250\n",
      "Iteration 51, loss = 0.04176631\n",
      "Iteration 52, loss = 0.04215972\n",
      "Iteration 53, loss = 0.04891393\n",
      "Iteration 54, loss = 0.04043054\n",
      "Iteration 55, loss = 0.03852043\n",
      "Iteration 56, loss = 0.04140649\n",
      "Iteration 57, loss = 0.03965059\n",
      "Iteration 58, loss = 0.03958368\n",
      "Iteration 59, loss = 0.03813237\n",
      "Iteration 60, loss = 0.05731855\n",
      "Iteration 61, loss = 0.07439271\n",
      "Iteration 62, loss = 0.05419285\n",
      "Iteration 63, loss = 0.04978756\n",
      "Iteration 64, loss = 0.04479885\n",
      "Iteration 65, loss = 0.04131499\n",
      "Iteration 66, loss = 0.03813185\n",
      "Iteration 67, loss = 0.04067009\n",
      "Iteration 68, loss = 0.04047493\n",
      "Iteration 69, loss = 0.03802027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.04007921\n",
      "Iteration 71, loss = 0.03615046\n",
      "Iteration 72, loss = 0.03985481\n",
      "Iteration 73, loss = 0.03752480\n",
      "Iteration 74, loss = 0.03444422\n",
      "Iteration 75, loss = 0.03678000\n",
      "Iteration 76, loss = 0.03973426\n",
      "Iteration 77, loss = 0.03508824\n",
      "Iteration 78, loss = 0.03593198\n",
      "Iteration 79, loss = 0.03269950\n",
      "Iteration 80, loss = 0.03536098\n",
      "Iteration 81, loss = 0.03158061\n",
      "Iteration 82, loss = 0.03211974\n",
      "Iteration 83, loss = 0.02854969\n",
      "Iteration 84, loss = 0.03270376\n",
      "Iteration 85, loss = 0.03725369\n",
      "Iteration 86, loss = 0.03113977\n",
      "Iteration 87, loss = 0.03485723\n",
      "Iteration 88, loss = 0.03146723\n",
      "Iteration 89, loss = 0.03210284\n",
      "Iteration 90, loss = 0.02825686\n",
      "Iteration 91, loss = 0.02788985\n",
      "Iteration 92, loss = 0.02825631\n",
      "Iteration 93, loss = 0.03216566\n",
      "Iteration 94, loss = 0.03277241\n",
      "Iteration 95, loss = 0.02942129\n",
      "Iteration 96, loss = 0.03129932\n",
      "Iteration 97, loss = 0.04278163\n",
      "Iteration 98, loss = 0.03824175\n",
      "Iteration 99, loss = 0.03611954\n",
      "Iteration 100, loss = 0.03350470\n",
      "Iteration 101, loss = 0.03304335\n",
      "Iteration 102, loss = 0.02855624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy:  0.9239802224969098\n",
      "F1-Score:  0.9265671641791045\n",
      "Precision:  0.9054842473745625\n",
      "Recall:  0.9486552567237164\n",
      "AUC:  0.9237026283618582\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_mlp(X_Train,Y_Train.ravel())\n",
    "clf=MLPClassifier(random_state=102, max_iter=3000, verbose=True).fit(X_Train, Y_Train.ravel())\n",
    "y_pred=clf.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a8318",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4faa3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a074f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cumulative explained variance')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kpca = KernelPCA(kernel = 'rbf')\n",
    "kpca_transform = kpca.fit_transform(X_Train_FeatureMap)\n",
    "explained_variance = np.var(kpca_transform, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "plt.yticks([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "plt.plot(np.cumsum(explained_variance_ratio))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5309636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6470, 3500)\n"
     ]
    }
   ],
   "source": [
    "kpca = KernelPCA(kernel = 'rbf',n_components=3500)\n",
    "X_Train_Transformed_FeatureMap = kpca.fit_transform(X_Train_FeatureMap)\n",
    "print(X_Train_Transformed_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d3a29fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1618, 18432) (1618, 3500)\n"
     ]
    }
   ],
   "source": [
    "X_Test_Transformed_FeatureMap = kpca.transform(X_Test_FeatureMap)\n",
    "print(X_Test_FeatureMap.shape,X_Test_Transformed_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11bb137",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "899c36d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 1/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.942 total time=  33.6s\n",
      "[CV 2/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 2/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.933 total time=  33.2s\n",
      "[CV 3/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 3/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.928 total time=  29.0s\n",
      "[CV 4/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 4/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.927 total time=  33.1s\n",
      "[CV 5/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 5/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.923 total time=  32.1s\n",
      "[CV 1/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 1/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.941 total time=  38.4s\n",
      "[CV 2/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 2/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.926 total time=  32.5s\n",
      "[CV 3/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 3/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.920 total time=  33.7s\n",
      "[CV 4/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 4/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.921 total time=  33.2s\n",
      "[CV 5/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 5/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.919 total time=  33.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [10], 'gamma': [10, 100], 'kernel': ['rbf']},\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% notify -m \"GridCV\"\n",
    "tuned_parameters = {'kernel': ['rbf'], 'gamma': [10],\n",
    "                     'C': [10],\n",
    "}\n",
    "clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring= 'accuracy',verbose=10\n",
    "    )\n",
    "clf.fit(X_Train_Transformed_FeatureMap, Y_Train_FeatureMap.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "926f405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 93.43122102009274\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 92.65842349304482\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 92.8129829984544\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 93.43122102009274\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 92.42658423493046\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 93.43122102009274 %\n",
      "Fold 1: 92.65842349304482 %\n",
      "Fold 2: 92.8129829984544 %\n",
      "Fold 3: 93.43122102009274 %\n",
      "Fold 4: 92.42658423493046 %\n",
      "Average: 92.95208655332303 %\n",
      "Accuracy:  0.9258343634116193\n",
      "F1-Score:  0.9283154121863798\n",
      "Precision:  0.9077102803738317\n",
      "Recall:  0.9498777506112469\n",
      "AUC:  0.9255638753056233\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_svm(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "clf=SVC(C=10,kernel='rbf',gamma=10)\n",
    "clf.fit(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=clf.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a866da0",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2873330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 81.14374034003092\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 80.91190108191654\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 81.60741885625966\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 82.22565687789799\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 80.83462132921174\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 81.14374034003092 %\n",
      "Fold 1: 80.91190108191654 %\n",
      "Fold 2: 81.60741885625966 %\n",
      "Fold 3: 82.22565687789799 %\n",
      "Fold 4: 80.83462132921174 %\n",
      "Average: 81.34466769706339 %\n",
      "Accuracy:  0.7707045735475896\n",
      "F1-Score:  0.7766405779650812\n",
      "Precision:  0.7651245551601423\n",
      "Recall:  0.7885085574572127\n",
      "AUC:  0.7705042787286063\n"
     ]
    }
   ],
   "source": [
    "dtree=k_fold_cv_dtree(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "dtree = DecisionTreeClassifier(random_state=102)\n",
    "dtree = dtree.fit(X_Train_Transformed_FeatureMap, Y_Train_FeatureMap.ravel())\n",
    "y_pred=dtree.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a57dd",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30139066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "[16:26:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 89.10355486862443\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "[16:28:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 90.26275115919628\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "[16:29:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 89.41267387944359\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "[16:31:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 90.03091190108191\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "[16:33:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 90.34003091190108\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 89.10355486862443 %\n",
      "Fold 1: 90.26275115919628 %\n",
      "Fold 2: 89.41267387944359 %\n",
      "Fold 3: 90.03091190108191 %\n",
      "Fold 4: 90.34003091190108 %\n",
      "Average: 89.82998454404947 %\n",
      "[16:35:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy:  0.8992583436341162\n",
      "F1-Score:  0.902686567164179\n",
      "Precision:  0.882147024504084\n",
      "Recall:  0.9242053789731052\n",
      "AUC:  0.8989776894865525\n"
     ]
    }
   ],
   "source": [
    "xg=k_fold_cv_xgb(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "xg = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=102,use_label_encoder=False)\n",
    "xg.fit(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=xg.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14828e1",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e78ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69215904\n",
      "Iteration 2, loss = 0.68694829\n",
      "Iteration 3, loss = 0.68024130\n",
      "Iteration 4, loss = 0.67020181\n",
      "Iteration 5, loss = 0.65678640\n",
      "Iteration 6, loss = 0.64063067\n",
      "Iteration 7, loss = 0.62079675\n",
      "Iteration 8, loss = 0.59857379\n",
      "Iteration 9, loss = 0.57444140\n",
      "Iteration 10, loss = 0.54911267\n",
      "Iteration 11, loss = 0.52306119\n",
      "Iteration 12, loss = 0.49691893\n",
      "Iteration 13, loss = 0.47023067\n",
      "Iteration 14, loss = 0.44512251\n",
      "Iteration 15, loss = 0.42011858\n",
      "Iteration 16, loss = 0.39691895\n",
      "Iteration 17, loss = 0.37484915\n",
      "Iteration 18, loss = 0.35394805\n",
      "Iteration 19, loss = 0.33441979\n",
      "Iteration 20, loss = 0.31618121\n",
      "Iteration 21, loss = 0.29945088\n",
      "Iteration 22, loss = 0.28415109\n",
      "Iteration 23, loss = 0.26934807\n",
      "Iteration 24, loss = 0.25637502\n",
      "Iteration 25, loss = 0.24457242\n",
      "Iteration 26, loss = 0.23313428\n",
      "Iteration 27, loss = 0.22285306\n",
      "Iteration 28, loss = 0.21280646\n",
      "Iteration 29, loss = 0.20385161\n",
      "Iteration 30, loss = 0.19576282\n",
      "Iteration 31, loss = 0.18827199\n",
      "Iteration 32, loss = 0.18128519\n",
      "Iteration 33, loss = 0.17448553\n",
      "Iteration 34, loss = 0.16848278\n",
      "Iteration 35, loss = 0.16254191\n",
      "Iteration 36, loss = 0.15720732\n",
      "Iteration 37, loss = 0.15213716\n",
      "Iteration 38, loss = 0.14748251\n",
      "Iteration 39, loss = 0.14308335\n",
      "Iteration 40, loss = 0.13896458\n",
      "Iteration 41, loss = 0.13508612\n",
      "Iteration 42, loss = 0.13139603\n",
      "Iteration 43, loss = 0.12789952\n",
      "Iteration 44, loss = 0.12467133\n",
      "Iteration 45, loss = 0.12137700\n",
      "Iteration 46, loss = 0.11838747\n",
      "Iteration 47, loss = 0.11596318\n",
      "Iteration 48, loss = 0.11361062\n",
      "Iteration 49, loss = 0.11110741\n",
      "Iteration 50, loss = 0.10799482\n",
      "Iteration 51, loss = 0.10565008\n",
      "Iteration 52, loss = 0.10346626\n",
      "Iteration 53, loss = 0.10127844\n",
      "Iteration 54, loss = 0.09930251\n",
      "Iteration 55, loss = 0.09735665\n",
      "Iteration 56, loss = 0.09567140\n",
      "Iteration 57, loss = 0.09365513\n",
      "Iteration 58, loss = 0.09213765\n",
      "Iteration 59, loss = 0.09030003\n",
      "Iteration 60, loss = 0.08870466\n",
      "Iteration 61, loss = 0.08714615\n",
      "Iteration 62, loss = 0.08569384\n",
      "Iteration 63, loss = 0.08418848\n",
      "Iteration 64, loss = 0.08334164\n",
      "Iteration 65, loss = 0.08158719\n",
      "Iteration 66, loss = 0.08059933\n",
      "Iteration 67, loss = 0.07939163\n",
      "Iteration 68, loss = 0.07782227\n",
      "Iteration 69, loss = 0.07660655\n",
      "Iteration 70, loss = 0.07602132\n",
      "Iteration 71, loss = 0.07467522\n",
      "Iteration 72, loss = 0.07342067\n",
      "Iteration 73, loss = 0.07260319\n",
      "Iteration 74, loss = 0.07146541\n",
      "Iteration 75, loss = 0.07103594\n",
      "Iteration 76, loss = 0.06965354\n",
      "Iteration 77, loss = 0.06876497\n",
      "Iteration 78, loss = 0.06781587\n",
      "Iteration 79, loss = 0.06701316\n",
      "Iteration 80, loss = 0.06623712\n",
      "Iteration 81, loss = 0.06548787\n",
      "Iteration 82, loss = 0.06477459\n",
      "Iteration 83, loss = 0.06402326\n",
      "Iteration 84, loss = 0.06315187\n",
      "Iteration 85, loss = 0.06250064\n",
      "Iteration 86, loss = 0.06180309\n",
      "Iteration 87, loss = 0.06133544\n",
      "Iteration 88, loss = 0.06063909\n",
      "Iteration 89, loss = 0.06012075\n",
      "Iteration 90, loss = 0.05925971\n",
      "Iteration 91, loss = 0.05867829\n",
      "Iteration 92, loss = 0.05803601\n",
      "Iteration 93, loss = 0.05749262\n",
      "Iteration 94, loss = 0.05709320\n",
      "Iteration 95, loss = 0.05646879\n",
      "Iteration 96, loss = 0.05639235\n",
      "Iteration 97, loss = 0.05537350\n",
      "Iteration 98, loss = 0.05491792\n",
      "Iteration 99, loss = 0.05436257\n",
      "Iteration 100, loss = 0.05383150\n",
      "Iteration 101, loss = 0.05340221\n",
      "Iteration 102, loss = 0.05289459\n",
      "Iteration 103, loss = 0.05255104\n",
      "Iteration 104, loss = 0.05202475\n",
      "Iteration 105, loss = 0.05175576\n",
      "Iteration 106, loss = 0.05127163\n",
      "Iteration 107, loss = 0.05060339\n",
      "Iteration 108, loss = 0.05051390\n",
      "Iteration 109, loss = 0.05005133\n",
      "Iteration 110, loss = 0.04962055\n",
      "Iteration 111, loss = 0.04914590\n",
      "Iteration 112, loss = 0.04885890\n",
      "Iteration 113, loss = 0.04841129\n",
      "Iteration 114, loss = 0.04815282\n",
      "Iteration 115, loss = 0.04793778\n",
      "Iteration 116, loss = 0.04771881\n",
      "Iteration 117, loss = 0.04709697\n",
      "Iteration 118, loss = 0.04695209\n",
      "Iteration 119, loss = 0.04651053\n",
      "Iteration 120, loss = 0.04611073\n",
      "Iteration 121, loss = 0.04629846\n",
      "Iteration 122, loss = 0.04580969\n",
      "Iteration 123, loss = 0.04537208\n",
      "Iteration 124, loss = 0.04496180\n",
      "Iteration 125, loss = 0.04485636\n",
      "Iteration 126, loss = 0.04449121\n",
      "Iteration 127, loss = 0.04406894\n",
      "Iteration 128, loss = 0.04417569\n",
      "Iteration 129, loss = 0.04373336\n",
      "Iteration 130, loss = 0.04324996\n",
      "Iteration 131, loss = 0.04307721\n",
      "Iteration 132, loss = 0.04266847\n",
      "Iteration 133, loss = 0.04274966\n",
      "Iteration 134, loss = 0.04238167\n",
      "Iteration 135, loss = 0.04213016\n",
      "Iteration 136, loss = 0.04183309\n",
      "Iteration 137, loss = 0.04153105\n",
      "Iteration 138, loss = 0.04122889\n",
      "Iteration 139, loss = 0.04138015\n",
      "Iteration 140, loss = 0.04091454\n",
      "Iteration 141, loss = 0.04086318\n",
      "Iteration 142, loss = 0.04045217\n",
      "Iteration 143, loss = 0.04031197\n",
      "Iteration 144, loss = 0.04020880\n",
      "Iteration 145, loss = 0.03979283\n",
      "Iteration 146, loss = 0.03977271\n",
      "Iteration 147, loss = 0.03955676\n",
      "Iteration 148, loss = 0.03936184\n",
      "Iteration 149, loss = 0.03900805\n",
      "Iteration 150, loss = 0.03892708\n",
      "Iteration 151, loss = 0.03875734\n",
      "Iteration 152, loss = 0.03862531\n",
      "Iteration 153, loss = 0.03845562\n",
      "Iteration 154, loss = 0.03821806\n",
      "Iteration 155, loss = 0.03847646\n",
      "Iteration 156, loss = 0.03793133\n",
      "Iteration 157, loss = 0.03763085\n",
      "Iteration 158, loss = 0.03754805\n",
      "Iteration 159, loss = 0.03736420\n",
      "Iteration 160, loss = 0.03777646\n",
      "Iteration 161, loss = 0.03707739\n",
      "Iteration 162, loss = 0.03690467\n",
      "Iteration 163, loss = 0.03680089\n",
      "Iteration 164, loss = 0.03658259\n",
      "Iteration 165, loss = 0.03648738\n",
      "Iteration 166, loss = 0.03636646\n",
      "Iteration 167, loss = 0.03641980\n",
      "Iteration 168, loss = 0.03622658\n",
      "Iteration 169, loss = 0.03585894\n",
      "Iteration 170, loss = 0.03596312\n",
      "Iteration 171, loss = 0.03546310\n",
      "Iteration 172, loss = 0.03551413\n",
      "Iteration 173, loss = 0.03531998\n",
      "Iteration 174, loss = 0.03513692\n",
      "Iteration 175, loss = 0.03525139\n",
      "Iteration 176, loss = 0.03482979\n",
      "Iteration 177, loss = 0.03533138\n",
      "Iteration 178, loss = 0.03467378\n",
      "Iteration 179, loss = 0.03457752\n",
      "Iteration 180, loss = 0.03462081\n",
      "Iteration 181, loss = 0.03443954\n",
      "Iteration 182, loss = 0.03433395\n",
      "Iteration 183, loss = 0.03414448\n",
      "Iteration 184, loss = 0.03400659\n",
      "Iteration 185, loss = 0.03374948\n",
      "Iteration 186, loss = 0.03383474\n",
      "Iteration 187, loss = 0.03383236\n",
      "Iteration 188, loss = 0.03363043\n",
      "Iteration 189, loss = 0.03383076\n",
      "Iteration 190, loss = 0.03343823\n",
      "Iteration 191, loss = 0.03323019\n",
      "Iteration 192, loss = 0.03330571\n",
      "Iteration 193, loss = 0.03316055\n",
      "Iteration 194, loss = 0.03300578\n",
      "Iteration 195, loss = 0.03284499\n",
      "Iteration 196, loss = 0.03271791\n",
      "Iteration 197, loss = 0.03282002\n",
      "Iteration 198, loss = 0.03251884\n",
      "Iteration 199, loss = 0.03288720\n",
      "Iteration 200, loss = 0.03252863\n",
      "Iteration 201, loss = 0.03239464\n",
      "Iteration 202, loss = 0.03212564\n",
      "Iteration 203, loss = 0.03200750\n",
      "Iteration 204, loss = 0.03197161\n",
      "Iteration 205, loss = 0.03233814\n",
      "Iteration 206, loss = 0.03176944\n",
      "Iteration 207, loss = 0.03162234\n",
      "Iteration 208, loss = 0.03177772\n",
      "Iteration 209, loss = 0.03179882\n",
      "Iteration 210, loss = 0.03145933\n",
      "Iteration 211, loss = 0.03138524\n",
      "Iteration 212, loss = 0.03131953\n",
      "Iteration 213, loss = 0.03185809\n",
      "Iteration 214, loss = 0.03128717\n",
      "Iteration 215, loss = 0.03117987\n",
      "Iteration 216, loss = 0.03114311\n",
      "Iteration 217, loss = 0.03110587\n",
      "Iteration 218, loss = 0.03080357\n",
      "Iteration 219, loss = 0.03087154\n",
      "Iteration 220, loss = 0.03062714\n",
      "Iteration 221, loss = 0.03064573\n",
      "Iteration 222, loss = 0.03059347\n",
      "Iteration 223, loss = 0.03052421\n",
      "Iteration 224, loss = 0.03035955\n",
      "Iteration 225, loss = 0.03057697\n",
      "Iteration 226, loss = 0.03018853\n",
      "Iteration 227, loss = 0.03007325\n",
      "Iteration 228, loss = 0.03018886\n",
      "Iteration 229, loss = 0.02999392\n",
      "Iteration 230, loss = 0.02998994\n",
      "Iteration 231, loss = 0.03014219\n",
      "Iteration 232, loss = 0.02989876\n",
      "Iteration 233, loss = 0.02966143\n",
      "Iteration 234, loss = 0.02977457\n",
      "Iteration 235, loss = 0.02983255\n",
      "Iteration 236, loss = 0.02968837\n",
      "Iteration 237, loss = 0.02985639\n",
      "Iteration 238, loss = 0.02936199\n",
      "Iteration 239, loss = 0.02943128\n",
      "Iteration 240, loss = 0.02979479\n",
      "Iteration 241, loss = 0.02924169\n",
      "Iteration 242, loss = 0.02929300\n",
      "Iteration 243, loss = 0.02925191\n",
      "Iteration 244, loss = 0.02965049\n",
      "Iteration 245, loss = 0.02944002\n",
      "Iteration 246, loss = 0.02923659\n",
      "Iteration 247, loss = 0.02928705\n",
      "Iteration 248, loss = 0.02875806\n",
      "Iteration 249, loss = 0.02891585\n",
      "Iteration 250, loss = 0.02866957\n",
      "Iteration 251, loss = 0.02869350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.02870582\n",
      "Iteration 253, loss = 0.02909925\n",
      "Iteration 254, loss = 0.02878971\n",
      "Iteration 255, loss = 0.02847538\n",
      "Iteration 256, loss = 0.02859247\n",
      "Iteration 257, loss = 0.02842463\n",
      "Iteration 258, loss = 0.02866242\n",
      "Iteration 259, loss = 0.02836003\n",
      "Iteration 260, loss = 0.02841000\n",
      "Iteration 261, loss = 0.02858764\n",
      "Iteration 262, loss = 0.02820903\n",
      "Iteration 263, loss = 0.02806907\n",
      "Iteration 264, loss = 0.02802816\n",
      "Iteration 265, loss = 0.02812450\n",
      "Iteration 266, loss = 0.02833978\n",
      "Iteration 267, loss = 0.02789063\n",
      "Iteration 268, loss = 0.02767712\n",
      "Iteration 269, loss = 0.02787500\n",
      "Iteration 270, loss = 0.02813822\n",
      "Iteration 271, loss = 0.02788252\n",
      "Iteration 272, loss = 0.02808189\n",
      "Iteration 273, loss = 0.02745597\n",
      "Iteration 274, loss = 0.02755038\n",
      "Iteration 275, loss = 0.02752551\n",
      "Iteration 276, loss = 0.02757229\n",
      "Iteration 277, loss = 0.02757013\n",
      "Iteration 278, loss = 0.02732269\n",
      "Iteration 279, loss = 0.02750715\n",
      "Iteration 280, loss = 0.02734302\n",
      "Iteration 281, loss = 0.02731295\n",
      "Iteration 282, loss = 0.02772798\n",
      "Iteration 283, loss = 0.02756344\n",
      "Iteration 284, loss = 0.02716152\n",
      "Iteration 285, loss = 0.02758107\n",
      "Iteration 286, loss = 0.02708556\n",
      "Iteration 287, loss = 0.02711427\n",
      "Iteration 288, loss = 0.02694558\n",
      "Iteration 289, loss = 0.02719768\n",
      "Iteration 290, loss = 0.02692047\n",
      "Iteration 291, loss = 0.02696111\n",
      "Iteration 292, loss = 0.02675267\n",
      "Iteration 293, loss = 0.02685930\n",
      "Iteration 294, loss = 0.02665329\n",
      "Iteration 295, loss = 0.02711742\n",
      "Iteration 296, loss = 0.02702472\n",
      "Iteration 297, loss = 0.02666072\n",
      "Iteration 298, loss = 0.02667792\n",
      "Iteration 299, loss = 0.02665382\n",
      "Iteration 300, loss = 0.02643567\n",
      "Iteration 301, loss = 0.02653876\n",
      "Iteration 302, loss = 0.02646869\n",
      "Iteration 303, loss = 0.02660410\n",
      "Iteration 304, loss = 0.02677415\n",
      "Iteration 305, loss = 0.02644209\n",
      "Iteration 306, loss = 0.02626633\n",
      "Iteration 307, loss = 0.02638134\n",
      "Iteration 308, loss = 0.02633041\n",
      "Iteration 309, loss = 0.02677908\n",
      "Iteration 310, loss = 0.02618413\n",
      "Iteration 311, loss = 0.02623294\n",
      "Iteration 312, loss = 0.02595440\n",
      "Iteration 313, loss = 0.02607521\n",
      "Iteration 314, loss = 0.02609189\n",
      "Iteration 315, loss = 0.02592912\n",
      "Iteration 316, loss = 0.02614391\n",
      "Iteration 317, loss = 0.02598013\n",
      "Iteration 318, loss = 0.02599980\n",
      "Iteration 319, loss = 0.02642256\n",
      "Iteration 320, loss = 0.02588688\n",
      "Iteration 321, loss = 0.02602084\n",
      "Iteration 322, loss = 0.02560921\n",
      "Iteration 323, loss = 0.02585079\n",
      "Iteration 324, loss = 0.02591374\n",
      "Iteration 325, loss = 0.02568158\n",
      "Iteration 326, loss = 0.02591561\n",
      "Iteration 327, loss = 0.02565471\n",
      "Iteration 328, loss = 0.02573881\n",
      "Iteration 329, loss = 0.02546599\n",
      "Iteration 330, loss = 0.02549985\n",
      "Iteration 331, loss = 0.02551666\n",
      "Iteration 332, loss = 0.02558591\n",
      "Iteration 333, loss = 0.02568017\n",
      "Iteration 334, loss = 0.02546894\n",
      "Iteration 335, loss = 0.02538083\n",
      "Iteration 336, loss = 0.02538921\n",
      "Iteration 337, loss = 0.02522949\n",
      "Iteration 338, loss = 0.02515792\n",
      "Iteration 339, loss = 0.02546283\n",
      "Iteration 340, loss = 0.02524855\n",
      "Iteration 341, loss = 0.02537415\n",
      "Iteration 342, loss = 0.02547982\n",
      "Iteration 343, loss = 0.02523567\n",
      "Iteration 344, loss = 0.02500393\n",
      "Iteration 345, loss = 0.02522839\n",
      "Iteration 346, loss = 0.02501655\n",
      "Iteration 347, loss = 0.02495355\n",
      "Iteration 348, loss = 0.02524505\n",
      "Iteration 349, loss = 0.02525582\n",
      "Iteration 350, loss = 0.02486760\n",
      "Iteration 351, loss = 0.02546431\n",
      "Iteration 352, loss = 0.02486185\n",
      "Iteration 353, loss = 0.02524248\n",
      "Iteration 354, loss = 0.02487454\n",
      "Iteration 355, loss = 0.02506707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 91.49922720247295\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69233909\n",
      "Iteration 2, loss = 0.68727255\n",
      "Iteration 3, loss = 0.68074002\n",
      "Iteration 4, loss = 0.67117739\n",
      "Iteration 5, loss = 0.65826573\n",
      "Iteration 6, loss = 0.64196674\n",
      "Iteration 7, loss = 0.62274401\n",
      "Iteration 8, loss = 0.60044416\n",
      "Iteration 9, loss = 0.57654856\n",
      "Iteration 10, loss = 0.55108960\n",
      "Iteration 11, loss = 0.52456128\n",
      "Iteration 12, loss = 0.49794935\n",
      "Iteration 13, loss = 0.47152191\n",
      "Iteration 14, loss = 0.44565382\n",
      "Iteration 15, loss = 0.42067826\n",
      "Iteration 16, loss = 0.39730379\n",
      "Iteration 17, loss = 0.37456315\n",
      "Iteration 18, loss = 0.35395858\n",
      "Iteration 19, loss = 0.33399050\n",
      "Iteration 20, loss = 0.31574078\n",
      "Iteration 21, loss = 0.29901805\n",
      "Iteration 22, loss = 0.28324894\n",
      "Iteration 23, loss = 0.26861625\n",
      "Iteration 24, loss = 0.25546600\n",
      "Iteration 25, loss = 0.24310112\n",
      "Iteration 26, loss = 0.23180800\n",
      "Iteration 27, loss = 0.22140733\n",
      "Iteration 28, loss = 0.21229166\n",
      "Iteration 29, loss = 0.20304315\n",
      "Iteration 30, loss = 0.19457949\n",
      "Iteration 31, loss = 0.18733430\n",
      "Iteration 32, loss = 0.18001185\n",
      "Iteration 33, loss = 0.17341387\n",
      "Iteration 34, loss = 0.16730557\n",
      "Iteration 35, loss = 0.16166944\n",
      "Iteration 36, loss = 0.15625180\n",
      "Iteration 37, loss = 0.15148667\n",
      "Iteration 38, loss = 0.14696423\n",
      "Iteration 39, loss = 0.14268789\n",
      "Iteration 40, loss = 0.13850529\n",
      "Iteration 41, loss = 0.13428201\n",
      "Iteration 42, loss = 0.13052792\n",
      "Iteration 43, loss = 0.12743302\n",
      "Iteration 44, loss = 0.12398318\n",
      "Iteration 45, loss = 0.12069275\n",
      "Iteration 46, loss = 0.11773028\n",
      "Iteration 47, loss = 0.11496947\n",
      "Iteration 48, loss = 0.11246074\n",
      "Iteration 49, loss = 0.10967180\n",
      "Iteration 50, loss = 0.10745796\n",
      "Iteration 51, loss = 0.10530823\n",
      "Iteration 52, loss = 0.10291544\n",
      "Iteration 53, loss = 0.10089251\n",
      "Iteration 54, loss = 0.09860738\n",
      "Iteration 55, loss = 0.09675052\n",
      "Iteration 56, loss = 0.09475687\n",
      "Iteration 57, loss = 0.09291178\n",
      "Iteration 58, loss = 0.09133934\n",
      "Iteration 59, loss = 0.08970627\n",
      "Iteration 60, loss = 0.08799781\n",
      "Iteration 61, loss = 0.08645290\n",
      "Iteration 62, loss = 0.08503947\n",
      "Iteration 63, loss = 0.08389885\n",
      "Iteration 64, loss = 0.08239920\n",
      "Iteration 65, loss = 0.08094973\n",
      "Iteration 66, loss = 0.07977040\n",
      "Iteration 67, loss = 0.07851243\n",
      "Iteration 68, loss = 0.07735230\n",
      "Iteration 69, loss = 0.07612228\n",
      "Iteration 70, loss = 0.07495309\n",
      "Iteration 71, loss = 0.07383401\n",
      "Iteration 72, loss = 0.07278485\n",
      "Iteration 73, loss = 0.07180918\n",
      "Iteration 74, loss = 0.07100580\n",
      "Iteration 75, loss = 0.07001719\n",
      "Iteration 76, loss = 0.06897530\n",
      "Iteration 77, loss = 0.06819081\n",
      "Iteration 78, loss = 0.06721529\n",
      "Iteration 79, loss = 0.06643051\n",
      "Iteration 80, loss = 0.06562253\n",
      "Iteration 81, loss = 0.06462728\n",
      "Iteration 82, loss = 0.06446489\n",
      "Iteration 83, loss = 0.06357065\n",
      "Iteration 84, loss = 0.06251763\n",
      "Iteration 85, loss = 0.06198113\n",
      "Iteration 86, loss = 0.06126319\n",
      "Iteration 87, loss = 0.06052873\n",
      "Iteration 88, loss = 0.06012904\n",
      "Iteration 89, loss = 0.05936064\n",
      "Iteration 90, loss = 0.05869693\n",
      "Iteration 91, loss = 0.05805024\n",
      "Iteration 92, loss = 0.05740494\n",
      "Iteration 93, loss = 0.05685996\n",
      "Iteration 94, loss = 0.05639966\n",
      "Iteration 95, loss = 0.05594915\n",
      "Iteration 96, loss = 0.05550058\n",
      "Iteration 97, loss = 0.05480179\n",
      "Iteration 98, loss = 0.05424984\n",
      "Iteration 99, loss = 0.05383366\n",
      "Iteration 100, loss = 0.05359725\n",
      "Iteration 101, loss = 0.05319651\n",
      "Iteration 102, loss = 0.05255694\n",
      "Iteration 103, loss = 0.05204205\n",
      "Iteration 104, loss = 0.05151158\n",
      "Iteration 105, loss = 0.05119761\n",
      "Iteration 106, loss = 0.05067651\n",
      "Iteration 107, loss = 0.05019581\n",
      "Iteration 108, loss = 0.04979667\n",
      "Iteration 109, loss = 0.04972598\n",
      "Iteration 110, loss = 0.04907366\n",
      "Iteration 111, loss = 0.04877216\n",
      "Iteration 112, loss = 0.04840034\n",
      "Iteration 113, loss = 0.04816227\n",
      "Iteration 114, loss = 0.04787707\n",
      "Iteration 115, loss = 0.04739076\n",
      "Iteration 116, loss = 0.04702255\n",
      "Iteration 117, loss = 0.04702230\n",
      "Iteration 118, loss = 0.04649903\n",
      "Iteration 119, loss = 0.04639952\n",
      "Iteration 120, loss = 0.04651769\n",
      "Iteration 121, loss = 0.04562905\n",
      "Iteration 122, loss = 0.04541192\n",
      "Iteration 123, loss = 0.04495285\n",
      "Iteration 124, loss = 0.04464063\n",
      "Iteration 125, loss = 0.04435440\n",
      "Iteration 126, loss = 0.04408271\n",
      "Iteration 127, loss = 0.04387882\n",
      "Iteration 128, loss = 0.04352031\n",
      "Iteration 129, loss = 0.04356496\n",
      "Iteration 130, loss = 0.04303137\n",
      "Iteration 131, loss = 0.04279520\n",
      "Iteration 132, loss = 0.04265258\n",
      "Iteration 133, loss = 0.04228948\n",
      "Iteration 134, loss = 0.04221071\n",
      "Iteration 135, loss = 0.04193861\n",
      "Iteration 136, loss = 0.04183833\n",
      "Iteration 137, loss = 0.04136802\n",
      "Iteration 138, loss = 0.04126949\n",
      "Iteration 139, loss = 0.04110178\n",
      "Iteration 140, loss = 0.04102632\n",
      "Iteration 141, loss = 0.04058047\n",
      "Iteration 142, loss = 0.04090086\n",
      "Iteration 143, loss = 0.04017738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 144, loss = 0.04014493\n",
      "Iteration 145, loss = 0.03980739\n",
      "Iteration 146, loss = 0.04024172\n",
      "Iteration 147, loss = 0.03954605\n",
      "Iteration 148, loss = 0.03929497\n",
      "Iteration 149, loss = 0.03911170\n",
      "Iteration 150, loss = 0.03897824\n",
      "Iteration 151, loss = 0.03915074\n",
      "Iteration 152, loss = 0.03878622\n",
      "Iteration 153, loss = 0.03855574\n",
      "Iteration 154, loss = 0.03822550\n",
      "Iteration 155, loss = 0.03846441\n",
      "Iteration 156, loss = 0.03781829\n",
      "Iteration 157, loss = 0.03792134\n",
      "Iteration 158, loss = 0.03779072\n",
      "Iteration 159, loss = 0.03747263\n",
      "Iteration 160, loss = 0.03740940\n",
      "Iteration 161, loss = 0.03722516\n",
      "Iteration 162, loss = 0.03721270\n",
      "Iteration 163, loss = 0.03697021\n",
      "Iteration 164, loss = 0.03696436\n",
      "Iteration 165, loss = 0.03739308\n",
      "Iteration 166, loss = 0.03657522\n",
      "Iteration 167, loss = 0.03740825\n",
      "Iteration 168, loss = 0.03634073\n",
      "Iteration 169, loss = 0.03612997\n",
      "Iteration 170, loss = 0.03593399\n",
      "Iteration 171, loss = 0.03581571\n",
      "Iteration 172, loss = 0.03613692\n",
      "Iteration 173, loss = 0.03562391\n",
      "Iteration 174, loss = 0.03550188\n",
      "Iteration 175, loss = 0.03542266\n",
      "Iteration 176, loss = 0.03538553\n",
      "Iteration 177, loss = 0.03513295\n",
      "Iteration 178, loss = 0.03506697\n",
      "Iteration 179, loss = 0.03510743\n",
      "Iteration 180, loss = 0.03478631\n",
      "Iteration 181, loss = 0.03478377\n",
      "Iteration 182, loss = 0.03477795\n",
      "Iteration 183, loss = 0.03452515\n",
      "Iteration 184, loss = 0.03450474\n",
      "Iteration 185, loss = 0.03454179\n",
      "Iteration 186, loss = 0.03421378\n",
      "Iteration 187, loss = 0.03413015\n",
      "Iteration 188, loss = 0.03431829\n",
      "Iteration 189, loss = 0.03414534\n",
      "Iteration 190, loss = 0.03393303\n",
      "Iteration 191, loss = 0.03371525\n",
      "Iteration 192, loss = 0.03394947\n",
      "Iteration 193, loss = 0.03354784\n",
      "Iteration 194, loss = 0.03331288\n",
      "Iteration 195, loss = 0.03355976\n",
      "Iteration 196, loss = 0.03373563\n",
      "Iteration 197, loss = 0.03339502\n",
      "Iteration 198, loss = 0.03348025\n",
      "Iteration 199, loss = 0.03304860\n",
      "Iteration 200, loss = 0.03298949\n",
      "Iteration 201, loss = 0.03296280\n",
      "Iteration 202, loss = 0.03286165\n",
      "Iteration 203, loss = 0.03288312\n",
      "Iteration 204, loss = 0.03311024\n",
      "Iteration 205, loss = 0.03252110\n",
      "Iteration 206, loss = 0.03258551\n",
      "Iteration 207, loss = 0.03225735\n",
      "Iteration 208, loss = 0.03239105\n",
      "Iteration 209, loss = 0.03242767\n",
      "Iteration 210, loss = 0.03279984\n",
      "Iteration 211, loss = 0.03223225\n",
      "Iteration 212, loss = 0.03212889\n",
      "Iteration 213, loss = 0.03211307\n",
      "Iteration 214, loss = 0.03177797\n",
      "Iteration 215, loss = 0.03181974\n",
      "Iteration 216, loss = 0.03192003\n",
      "Iteration 217, loss = 0.03166427\n",
      "Iteration 218, loss = 0.03152692\n",
      "Iteration 219, loss = 0.03146042\n",
      "Iteration 220, loss = 0.03160328\n",
      "Iteration 221, loss = 0.03182183\n",
      "Iteration 222, loss = 0.03144849\n",
      "Iteration 223, loss = 0.03129686\n",
      "Iteration 224, loss = 0.03108759\n",
      "Iteration 225, loss = 0.03128633\n",
      "Iteration 226, loss = 0.03126481\n",
      "Iteration 227, loss = 0.03112295\n",
      "Iteration 228, loss = 0.03109105\n",
      "Iteration 229, loss = 0.03091897\n",
      "Iteration 230, loss = 0.03091149\n",
      "Iteration 231, loss = 0.03106581\n",
      "Iteration 232, loss = 0.03072692\n",
      "Iteration 233, loss = 0.03115390\n",
      "Iteration 234, loss = 0.03110444\n",
      "Iteration 235, loss = 0.03051292\n",
      "Iteration 236, loss = 0.03029825\n",
      "Iteration 237, loss = 0.03025165\n",
      "Iteration 238, loss = 0.03050405\n",
      "Iteration 239, loss = 0.03031609\n",
      "Iteration 240, loss = 0.03025400\n",
      "Iteration 241, loss = 0.03025637\n",
      "Iteration 242, loss = 0.03028825\n",
      "Iteration 243, loss = 0.03000319\n",
      "Iteration 244, loss = 0.03004244\n",
      "Iteration 245, loss = 0.03003037\n",
      "Iteration 246, loss = 0.03004239\n",
      "Iteration 247, loss = 0.02969163\n",
      "Iteration 248, loss = 0.02991591\n",
      "Iteration 249, loss = 0.02978486\n",
      "Iteration 250, loss = 0.02954768\n",
      "Iteration 251, loss = 0.03023347\n",
      "Iteration 252, loss = 0.02969800\n",
      "Iteration 253, loss = 0.02971285\n",
      "Iteration 254, loss = 0.02962286\n",
      "Iteration 255, loss = 0.02937561\n",
      "Iteration 256, loss = 0.02965411\n",
      "Iteration 257, loss = 0.02943798\n",
      "Iteration 258, loss = 0.02935276\n",
      "Iteration 259, loss = 0.02954730\n",
      "Iteration 260, loss = 0.02915941\n",
      "Iteration 261, loss = 0.02920437\n",
      "Iteration 262, loss = 0.02928683\n",
      "Iteration 263, loss = 0.02916848\n",
      "Iteration 264, loss = 0.02903025\n",
      "Iteration 265, loss = 0.02902005\n",
      "Iteration 266, loss = 0.02900851\n",
      "Iteration 267, loss = 0.02893348\n",
      "Iteration 268, loss = 0.02895456\n",
      "Iteration 269, loss = 0.02907449\n",
      "Iteration 270, loss = 0.02854032\n",
      "Iteration 271, loss = 0.02871881\n",
      "Iteration 272, loss = 0.02858959\n",
      "Iteration 273, loss = 0.02878956\n",
      "Iteration 274, loss = 0.02885180\n",
      "Iteration 275, loss = 0.02837877\n",
      "Iteration 276, loss = 0.02854615\n",
      "Iteration 277, loss = 0.02833100\n",
      "Iteration 278, loss = 0.02838619\n",
      "Iteration 279, loss = 0.02847198\n",
      "Iteration 280, loss = 0.02854689\n",
      "Iteration 281, loss = 0.02859781\n",
      "Iteration 282, loss = 0.02823948\n",
      "Iteration 283, loss = 0.02856166\n",
      "Iteration 284, loss = 0.02833544\n",
      "Iteration 285, loss = 0.02841591\n",
      "Iteration 286, loss = 0.02784007\n",
      "Iteration 287, loss = 0.02798398\n",
      "Iteration 288, loss = 0.02811784\n",
      "Iteration 289, loss = 0.02844146\n",
      "Iteration 290, loss = 0.02816099\n",
      "Iteration 291, loss = 0.02759438\n",
      "Iteration 292, loss = 0.02827905\n",
      "Iteration 293, loss = 0.02786043\n",
      "Iteration 294, loss = 0.02837427\n",
      "Iteration 295, loss = 0.02819134\n",
      "Iteration 296, loss = 0.02766357\n",
      "Iteration 297, loss = 0.02775870\n",
      "Iteration 298, loss = 0.02784524\n",
      "Iteration 299, loss = 0.02759715\n",
      "Iteration 300, loss = 0.02784573\n",
      "Iteration 301, loss = 0.02807634\n",
      "Iteration 302, loss = 0.02757096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 91.73106646058733\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69221935\n",
      "Iteration 2, loss = 0.68692496\n",
      "Iteration 3, loss = 0.68009145\n",
      "Iteration 4, loss = 0.67024524\n",
      "Iteration 5, loss = 0.65695160\n",
      "Iteration 6, loss = 0.64049086\n",
      "Iteration 7, loss = 0.62090817\n",
      "Iteration 8, loss = 0.59909091\n",
      "Iteration 9, loss = 0.57455396\n",
      "Iteration 10, loss = 0.54859708\n",
      "Iteration 11, loss = 0.52197617\n",
      "Iteration 12, loss = 0.49502488\n",
      "Iteration 13, loss = 0.46858867\n",
      "Iteration 14, loss = 0.44277711\n",
      "Iteration 15, loss = 0.41761742\n",
      "Iteration 16, loss = 0.39386772\n",
      "Iteration 17, loss = 0.37158088\n",
      "Iteration 18, loss = 0.35042827\n",
      "Iteration 19, loss = 0.33087695\n",
      "Iteration 20, loss = 0.31263187\n",
      "Iteration 21, loss = 0.29598012\n",
      "Iteration 22, loss = 0.27987105\n",
      "Iteration 23, loss = 0.26542372\n",
      "Iteration 24, loss = 0.25208800\n",
      "Iteration 25, loss = 0.24004061\n",
      "Iteration 26, loss = 0.22899179\n",
      "Iteration 27, loss = 0.21797454\n",
      "Iteration 28, loss = 0.20842625\n",
      "Iteration 29, loss = 0.19947269\n",
      "Iteration 30, loss = 0.19125443\n",
      "Iteration 31, loss = 0.18372949\n",
      "Iteration 32, loss = 0.17639645\n",
      "Iteration 33, loss = 0.17010946\n",
      "Iteration 34, loss = 0.16364605\n",
      "Iteration 35, loss = 0.15812359\n",
      "Iteration 36, loss = 0.15263218\n",
      "Iteration 37, loss = 0.14802235\n",
      "Iteration 38, loss = 0.14301274\n",
      "Iteration 39, loss = 0.13834462\n",
      "Iteration 40, loss = 0.13417791\n",
      "Iteration 41, loss = 0.13025687\n",
      "Iteration 42, loss = 0.12664508\n",
      "Iteration 43, loss = 0.12296589\n",
      "Iteration 44, loss = 0.11978997\n",
      "Iteration 45, loss = 0.11668027\n",
      "Iteration 46, loss = 0.11362189\n",
      "Iteration 47, loss = 0.11069887\n",
      "Iteration 48, loss = 0.10801300\n",
      "Iteration 49, loss = 0.10541606\n",
      "Iteration 50, loss = 0.10312084\n",
      "Iteration 51, loss = 0.10059281\n",
      "Iteration 52, loss = 0.09823742\n",
      "Iteration 53, loss = 0.09615694\n",
      "Iteration 54, loss = 0.09416881\n",
      "Iteration 55, loss = 0.09209697\n",
      "Iteration 56, loss = 0.09035211\n",
      "Iteration 57, loss = 0.08839337\n",
      "Iteration 58, loss = 0.08684909\n",
      "Iteration 59, loss = 0.08496377\n",
      "Iteration 60, loss = 0.08371094\n",
      "Iteration 61, loss = 0.08188391\n",
      "Iteration 62, loss = 0.08032105\n",
      "Iteration 63, loss = 0.07886965\n",
      "Iteration 64, loss = 0.07764099\n",
      "Iteration 65, loss = 0.07625169\n",
      "Iteration 66, loss = 0.07524769\n",
      "Iteration 67, loss = 0.07367138\n",
      "Iteration 68, loss = 0.07275804\n",
      "Iteration 69, loss = 0.07158248\n",
      "Iteration 70, loss = 0.07026011\n",
      "Iteration 71, loss = 0.06913863\n",
      "Iteration 72, loss = 0.06812026\n",
      "Iteration 73, loss = 0.06715822\n",
      "Iteration 74, loss = 0.06620110\n",
      "Iteration 75, loss = 0.06530251\n",
      "Iteration 76, loss = 0.06431385\n",
      "Iteration 77, loss = 0.06343687\n",
      "Iteration 78, loss = 0.06274103\n",
      "Iteration 79, loss = 0.06173963\n",
      "Iteration 80, loss = 0.06136286\n",
      "Iteration 81, loss = 0.06031028\n",
      "Iteration 82, loss = 0.05946308\n",
      "Iteration 83, loss = 0.05866150\n",
      "Iteration 84, loss = 0.05806979\n",
      "Iteration 85, loss = 0.05725191\n",
      "Iteration 86, loss = 0.05651874\n",
      "Iteration 87, loss = 0.05600489\n",
      "Iteration 88, loss = 0.05534321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89, loss = 0.05475090\n",
      "Iteration 90, loss = 0.05437452\n",
      "Iteration 91, loss = 0.05374947\n",
      "Iteration 92, loss = 0.05306567\n",
      "Iteration 93, loss = 0.05245773\n",
      "Iteration 94, loss = 0.05195200\n",
      "Iteration 95, loss = 0.05134474\n",
      "Iteration 96, loss = 0.05086078\n",
      "Iteration 97, loss = 0.05089874\n",
      "Iteration 98, loss = 0.04991278\n",
      "Iteration 99, loss = 0.04963924\n",
      "Iteration 100, loss = 0.04908679\n",
      "Iteration 101, loss = 0.04845442\n",
      "Iteration 102, loss = 0.04833447\n",
      "Iteration 103, loss = 0.04760931\n",
      "Iteration 104, loss = 0.04725281\n",
      "Iteration 105, loss = 0.04688893\n",
      "Iteration 106, loss = 0.04642666\n",
      "Iteration 107, loss = 0.04615937\n",
      "Iteration 108, loss = 0.04567286\n",
      "Iteration 109, loss = 0.04536752\n",
      "Iteration 110, loss = 0.04484022\n",
      "Iteration 111, loss = 0.04457771\n",
      "Iteration 112, loss = 0.04421880\n",
      "Iteration 113, loss = 0.04393461\n",
      "Iteration 114, loss = 0.04413674\n",
      "Iteration 115, loss = 0.04326770\n",
      "Iteration 116, loss = 0.04297766\n",
      "Iteration 117, loss = 0.04261580\n",
      "Iteration 118, loss = 0.04240936\n",
      "Iteration 119, loss = 0.04197702\n",
      "Iteration 120, loss = 0.04177078\n",
      "Iteration 121, loss = 0.04141560\n",
      "Iteration 122, loss = 0.04108330\n",
      "Iteration 123, loss = 0.04086827\n",
      "Iteration 124, loss = 0.04077378\n",
      "Iteration 125, loss = 0.04050855\n",
      "Iteration 126, loss = 0.04034574\n",
      "Iteration 127, loss = 0.03977738\n",
      "Iteration 128, loss = 0.03957237\n",
      "Iteration 129, loss = 0.03937329\n",
      "Iteration 130, loss = 0.03909167\n",
      "Iteration 131, loss = 0.03886936\n",
      "Iteration 132, loss = 0.03863113\n",
      "Iteration 133, loss = 0.03841397\n",
      "Iteration 134, loss = 0.03838971\n",
      "Iteration 135, loss = 0.03826977\n",
      "Iteration 136, loss = 0.03784266\n",
      "Iteration 137, loss = 0.03760667\n",
      "Iteration 138, loss = 0.03735350\n",
      "Iteration 139, loss = 0.03739386\n",
      "Iteration 140, loss = 0.03699190\n",
      "Iteration 141, loss = 0.03689503\n",
      "Iteration 142, loss = 0.03666450\n",
      "Iteration 143, loss = 0.03658257\n",
      "Iteration 144, loss = 0.03632128\n",
      "Iteration 145, loss = 0.03603179\n",
      "Iteration 146, loss = 0.03601000\n",
      "Iteration 147, loss = 0.03614579\n",
      "Iteration 148, loss = 0.03573541\n",
      "Iteration 149, loss = 0.03541852\n",
      "Iteration 150, loss = 0.03515404\n",
      "Iteration 151, loss = 0.03499690\n",
      "Iteration 152, loss = 0.03499097\n",
      "Iteration 153, loss = 0.03471596\n",
      "Iteration 154, loss = 0.03450182\n",
      "Iteration 155, loss = 0.03483429\n",
      "Iteration 156, loss = 0.03432997\n",
      "Iteration 157, loss = 0.03422858\n",
      "Iteration 158, loss = 0.03394823\n",
      "Iteration 159, loss = 0.03379552\n",
      "Iteration 160, loss = 0.03369281\n",
      "Iteration 161, loss = 0.03350317\n",
      "Iteration 162, loss = 0.03366382\n",
      "Iteration 163, loss = 0.03353228\n",
      "Iteration 164, loss = 0.03315791\n",
      "Iteration 165, loss = 0.03306483\n",
      "Iteration 166, loss = 0.03282038\n",
      "Iteration 167, loss = 0.03268127\n",
      "Iteration 168, loss = 0.03252818\n",
      "Iteration 169, loss = 0.03265308\n",
      "Iteration 170, loss = 0.03230580\n",
      "Iteration 171, loss = 0.03235905\n",
      "Iteration 172, loss = 0.03225461\n",
      "Iteration 173, loss = 0.03200923\n",
      "Iteration 174, loss = 0.03214497\n",
      "Iteration 175, loss = 0.03181690\n",
      "Iteration 176, loss = 0.03159345\n",
      "Iteration 177, loss = 0.03156973\n",
      "Iteration 178, loss = 0.03147010\n",
      "Iteration 179, loss = 0.03128521\n",
      "Iteration 180, loss = 0.03177203\n",
      "Iteration 181, loss = 0.03168385\n",
      "Iteration 182, loss = 0.03083401\n",
      "Iteration 183, loss = 0.03087758\n",
      "Iteration 184, loss = 0.03096277\n",
      "Iteration 185, loss = 0.03078082\n",
      "Iteration 186, loss = 0.03061014\n",
      "Iteration 187, loss = 0.03064429\n",
      "Iteration 188, loss = 0.03037744\n",
      "Iteration 189, loss = 0.03029515\n",
      "Iteration 190, loss = 0.03025174\n",
      "Iteration 191, loss = 0.03017968\n",
      "Iteration 192, loss = 0.03009032\n",
      "Iteration 193, loss = 0.02983922\n",
      "Iteration 194, loss = 0.02983087\n",
      "Iteration 195, loss = 0.03011407\n",
      "Iteration 196, loss = 0.02973823\n",
      "Iteration 197, loss = 0.02958238\n",
      "Iteration 198, loss = 0.02949934\n",
      "Iteration 199, loss = 0.03001273\n",
      "Iteration 200, loss = 0.02967841\n",
      "Iteration 201, loss = 0.02907057\n",
      "Iteration 202, loss = 0.02953638\n",
      "Iteration 203, loss = 0.02911899\n",
      "Iteration 204, loss = 0.02903554\n",
      "Iteration 205, loss = 0.02901678\n",
      "Iteration 206, loss = 0.02891600\n",
      "Iteration 207, loss = 0.02865903\n",
      "Iteration 208, loss = 0.02877656\n",
      "Iteration 209, loss = 0.02863259\n",
      "Iteration 210, loss = 0.02875571\n",
      "Iteration 211, loss = 0.02826316\n",
      "Iteration 212, loss = 0.02834471\n",
      "Iteration 213, loss = 0.02826545\n",
      "Iteration 214, loss = 0.02828733\n",
      "Iteration 215, loss = 0.02811414\n",
      "Iteration 216, loss = 0.02848279\n",
      "Iteration 217, loss = 0.02827250\n",
      "Iteration 218, loss = 0.02806820\n",
      "Iteration 219, loss = 0.02783466\n",
      "Iteration 220, loss = 0.02777064\n",
      "Iteration 221, loss = 0.02834910\n",
      "Iteration 222, loss = 0.02792273\n",
      "Iteration 223, loss = 0.02764609\n",
      "Iteration 224, loss = 0.02776731\n",
      "Iteration 225, loss = 0.02745039\n",
      "Iteration 226, loss = 0.02756822\n",
      "Iteration 227, loss = 0.02745099\n",
      "Iteration 228, loss = 0.02728719\n",
      "Iteration 229, loss = 0.02750291\n",
      "Iteration 230, loss = 0.02735017\n",
      "Iteration 231, loss = 0.02739400\n",
      "Iteration 232, loss = 0.02706746\n",
      "Iteration 233, loss = 0.02718128\n",
      "Iteration 234, loss = 0.02708543\n",
      "Iteration 235, loss = 0.02701282\n",
      "Iteration 236, loss = 0.02680016\n",
      "Iteration 237, loss = 0.02680489\n",
      "Iteration 238, loss = 0.02702127\n",
      "Iteration 239, loss = 0.02701185\n",
      "Iteration 240, loss = 0.02659391\n",
      "Iteration 241, loss = 0.02651682\n",
      "Iteration 242, loss = 0.02650968\n",
      "Iteration 243, loss = 0.02627204\n",
      "Iteration 244, loss = 0.02647402\n",
      "Iteration 245, loss = 0.02656601\n",
      "Iteration 246, loss = 0.02668856\n",
      "Iteration 247, loss = 0.02616327\n",
      "Iteration 248, loss = 0.02610673\n",
      "Iteration 249, loss = 0.02618035\n",
      "Iteration 250, loss = 0.02612369\n",
      "Iteration 251, loss = 0.02596309\n",
      "Iteration 252, loss = 0.02589719\n",
      "Iteration 253, loss = 0.02599641\n",
      "Iteration 254, loss = 0.02587305\n",
      "Iteration 255, loss = 0.02572110\n",
      "Iteration 256, loss = 0.02582307\n",
      "Iteration 257, loss = 0.02579999\n",
      "Iteration 258, loss = 0.02577277\n",
      "Iteration 259, loss = 0.02576623\n",
      "Iteration 260, loss = 0.02555919\n",
      "Iteration 261, loss = 0.02556986\n",
      "Iteration 262, loss = 0.02564429\n",
      "Iteration 263, loss = 0.02534792\n",
      "Iteration 264, loss = 0.02538690\n",
      "Iteration 265, loss = 0.02519918\n",
      "Iteration 266, loss = 0.02516306\n",
      "Iteration 267, loss = 0.02527939\n",
      "Iteration 268, loss = 0.02529088\n",
      "Iteration 269, loss = 0.02531516\n",
      "Iteration 270, loss = 0.02502704\n",
      "Iteration 271, loss = 0.02542962\n",
      "Iteration 272, loss = 0.02513836\n",
      "Iteration 273, loss = 0.02504823\n",
      "Iteration 274, loss = 0.02534414\n",
      "Iteration 275, loss = 0.02475432\n",
      "Iteration 276, loss = 0.02490730\n",
      "Iteration 277, loss = 0.02475170\n",
      "Iteration 278, loss = 0.02494477\n",
      "Iteration 279, loss = 0.02460852\n",
      "Iteration 280, loss = 0.02461282\n",
      "Iteration 281, loss = 0.02458146\n",
      "Iteration 282, loss = 0.02457172\n",
      "Iteration 283, loss = 0.02452729\n",
      "Iteration 284, loss = 0.02476657\n",
      "Iteration 285, loss = 0.02448859\n",
      "Iteration 286, loss = 0.02509433\n",
      "Iteration 287, loss = 0.02455262\n",
      "Iteration 288, loss = 0.02441659\n",
      "Iteration 289, loss = 0.02456785\n",
      "Iteration 290, loss = 0.02445062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 90.49459041731066\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69226891\n",
      "Iteration 2, loss = 0.68706884\n",
      "Iteration 3, loss = 0.68033396\n",
      "Iteration 4, loss = 0.67096350\n",
      "Iteration 5, loss = 0.65798550\n",
      "Iteration 6, loss = 0.64181500\n",
      "Iteration 7, loss = 0.62273200\n",
      "Iteration 8, loss = 0.60027412\n",
      "Iteration 9, loss = 0.57655013\n",
      "Iteration 10, loss = 0.55097010\n",
      "Iteration 11, loss = 0.52455475\n",
      "Iteration 12, loss = 0.49775924\n",
      "Iteration 13, loss = 0.47171672\n",
      "Iteration 14, loss = 0.44547763\n",
      "Iteration 15, loss = 0.42094470\n",
      "Iteration 16, loss = 0.39696060\n",
      "Iteration 17, loss = 0.37464295\n",
      "Iteration 18, loss = 0.35351331\n",
      "Iteration 19, loss = 0.33410669\n",
      "Iteration 20, loss = 0.31617601\n",
      "Iteration 21, loss = 0.29882195\n",
      "Iteration 22, loss = 0.28310386\n",
      "Iteration 23, loss = 0.26856591\n",
      "Iteration 24, loss = 0.25524526\n",
      "Iteration 25, loss = 0.24291013\n",
      "Iteration 26, loss = 0.23172340\n",
      "Iteration 27, loss = 0.22112703\n",
      "Iteration 28, loss = 0.21171379\n",
      "Iteration 29, loss = 0.20291236\n",
      "Iteration 30, loss = 0.19440338\n",
      "Iteration 31, loss = 0.18692278\n",
      "Iteration 32, loss = 0.17959709\n",
      "Iteration 33, loss = 0.17286135\n",
      "Iteration 34, loss = 0.16674050\n",
      "Iteration 35, loss = 0.16105517\n",
      "Iteration 36, loss = 0.15563993\n",
      "Iteration 37, loss = 0.15061980\n",
      "Iteration 38, loss = 0.14605640\n",
      "Iteration 39, loss = 0.14153376\n",
      "Iteration 40, loss = 0.13739627\n",
      "Iteration 41, loss = 0.13340049\n",
      "Iteration 42, loss = 0.12984225\n",
      "Iteration 43, loss = 0.12632614\n",
      "Iteration 44, loss = 0.12304109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.11999578\n",
      "Iteration 46, loss = 0.11704155\n",
      "Iteration 47, loss = 0.11399706\n",
      "Iteration 48, loss = 0.11151467\n",
      "Iteration 49, loss = 0.10888368\n",
      "Iteration 50, loss = 0.10648219\n",
      "Iteration 51, loss = 0.10406337\n",
      "Iteration 52, loss = 0.10216288\n",
      "Iteration 53, loss = 0.09975864\n",
      "Iteration 54, loss = 0.09774044\n",
      "Iteration 55, loss = 0.09586673\n",
      "Iteration 56, loss = 0.09399566\n",
      "Iteration 57, loss = 0.09214160\n",
      "Iteration 58, loss = 0.09021926\n",
      "Iteration 59, loss = 0.08891237\n",
      "Iteration 60, loss = 0.08733086\n",
      "Iteration 61, loss = 0.08576010\n",
      "Iteration 62, loss = 0.08408534\n",
      "Iteration 63, loss = 0.08257034\n",
      "Iteration 64, loss = 0.08129226\n",
      "Iteration 65, loss = 0.08001873\n",
      "Iteration 66, loss = 0.07886502\n",
      "Iteration 67, loss = 0.07745669\n",
      "Iteration 68, loss = 0.07634584\n",
      "Iteration 69, loss = 0.07516583\n",
      "Iteration 70, loss = 0.07403341\n",
      "Iteration 71, loss = 0.07299863\n",
      "Iteration 72, loss = 0.07191088\n",
      "Iteration 73, loss = 0.07106952\n",
      "Iteration 74, loss = 0.07005417\n",
      "Iteration 75, loss = 0.06916945\n",
      "Iteration 76, loss = 0.06811088\n",
      "Iteration 77, loss = 0.06737951\n",
      "Iteration 78, loss = 0.06657820\n",
      "Iteration 79, loss = 0.06556257\n",
      "Iteration 80, loss = 0.06474014\n",
      "Iteration 81, loss = 0.06432047\n",
      "Iteration 82, loss = 0.06349666\n",
      "Iteration 83, loss = 0.06277657\n",
      "Iteration 84, loss = 0.06181234\n",
      "Iteration 85, loss = 0.06112906\n",
      "Iteration 86, loss = 0.06069333\n",
      "Iteration 87, loss = 0.05980614\n",
      "Iteration 88, loss = 0.05936128\n",
      "Iteration 89, loss = 0.05846857\n",
      "Iteration 90, loss = 0.05812491\n",
      "Iteration 91, loss = 0.05740783\n",
      "Iteration 92, loss = 0.05677737\n",
      "Iteration 93, loss = 0.05626371\n",
      "Iteration 94, loss = 0.05570116\n",
      "Iteration 95, loss = 0.05523952\n",
      "Iteration 96, loss = 0.05471693\n",
      "Iteration 97, loss = 0.05416925\n",
      "Iteration 98, loss = 0.05362551\n",
      "Iteration 99, loss = 0.05342696\n",
      "Iteration 100, loss = 0.05267993\n",
      "Iteration 101, loss = 0.05263673\n",
      "Iteration 102, loss = 0.05183065\n",
      "Iteration 103, loss = 0.05168665\n",
      "Iteration 104, loss = 0.05105429\n",
      "Iteration 105, loss = 0.05078717\n",
      "Iteration 106, loss = 0.05018410\n",
      "Iteration 107, loss = 0.04978832\n",
      "Iteration 108, loss = 0.04948731\n",
      "Iteration 109, loss = 0.04892379\n",
      "Iteration 110, loss = 0.04903107\n",
      "Iteration 111, loss = 0.04836891\n",
      "Iteration 112, loss = 0.04789993\n",
      "Iteration 113, loss = 0.04769802\n",
      "Iteration 114, loss = 0.04748367\n",
      "Iteration 115, loss = 0.04713094\n",
      "Iteration 116, loss = 0.04684509\n",
      "Iteration 117, loss = 0.04632741\n",
      "Iteration 118, loss = 0.04607051\n",
      "Iteration 119, loss = 0.04586448\n",
      "Iteration 120, loss = 0.04548190\n",
      "Iteration 121, loss = 0.04522272\n",
      "Iteration 122, loss = 0.04491309\n",
      "Iteration 123, loss = 0.04454976\n",
      "Iteration 124, loss = 0.04437686\n",
      "Iteration 125, loss = 0.04400621\n",
      "Iteration 126, loss = 0.04384204\n",
      "Iteration 127, loss = 0.04363291\n",
      "Iteration 128, loss = 0.04323425\n",
      "Iteration 129, loss = 0.04337384\n",
      "Iteration 130, loss = 0.04311701\n",
      "Iteration 131, loss = 0.04283999\n",
      "Iteration 132, loss = 0.04251890\n",
      "Iteration 133, loss = 0.04205136\n",
      "Iteration 134, loss = 0.04185278\n",
      "Iteration 135, loss = 0.04159822\n",
      "Iteration 136, loss = 0.04148830\n",
      "Iteration 137, loss = 0.04125383\n",
      "Iteration 138, loss = 0.04079594\n",
      "Iteration 139, loss = 0.04099474\n",
      "Iteration 140, loss = 0.04085044\n",
      "Iteration 141, loss = 0.04028889\n",
      "Iteration 142, loss = 0.04039366\n",
      "Iteration 143, loss = 0.03997148\n",
      "Iteration 144, loss = 0.03979229\n",
      "Iteration 145, loss = 0.03962761\n",
      "Iteration 146, loss = 0.03938257\n",
      "Iteration 147, loss = 0.03905942\n",
      "Iteration 148, loss = 0.03947552\n",
      "Iteration 149, loss = 0.03889489\n",
      "Iteration 150, loss = 0.03921911\n",
      "Iteration 151, loss = 0.03853509\n",
      "Iteration 152, loss = 0.03847567\n",
      "Iteration 153, loss = 0.03831078\n",
      "Iteration 154, loss = 0.03837514\n",
      "Iteration 155, loss = 0.03795271\n",
      "Iteration 156, loss = 0.03798822\n",
      "Iteration 157, loss = 0.03815249\n",
      "Iteration 158, loss = 0.03749986\n",
      "Iteration 159, loss = 0.03726083\n",
      "Iteration 160, loss = 0.03708137\n",
      "Iteration 161, loss = 0.03696338\n",
      "Iteration 162, loss = 0.03683923\n",
      "Iteration 163, loss = 0.03671257\n",
      "Iteration 164, loss = 0.03680668\n",
      "Iteration 165, loss = 0.03627255\n",
      "Iteration 166, loss = 0.03637382\n",
      "Iteration 167, loss = 0.03629813\n",
      "Iteration 168, loss = 0.03620176\n",
      "Iteration 169, loss = 0.03597958\n",
      "Iteration 170, loss = 0.03582837\n",
      "Iteration 171, loss = 0.03586909\n",
      "Iteration 172, loss = 0.03562631\n",
      "Iteration 173, loss = 0.03535682\n",
      "Iteration 174, loss = 0.03534237\n",
      "Iteration 175, loss = 0.03594830\n",
      "Iteration 176, loss = 0.03568371\n",
      "Iteration 177, loss = 0.03535892\n",
      "Iteration 178, loss = 0.03483036\n",
      "Iteration 179, loss = 0.03469871\n",
      "Iteration 180, loss = 0.03463915\n",
      "Iteration 181, loss = 0.03458162\n",
      "Iteration 182, loss = 0.03447486\n",
      "Iteration 183, loss = 0.03415537\n",
      "Iteration 184, loss = 0.03466685\n",
      "Iteration 185, loss = 0.03462033\n",
      "Iteration 186, loss = 0.03422221\n",
      "Iteration 187, loss = 0.03400772\n",
      "Iteration 188, loss = 0.03371060\n",
      "Iteration 189, loss = 0.03371893\n",
      "Iteration 190, loss = 0.03351757\n",
      "Iteration 191, loss = 0.03370185\n",
      "Iteration 192, loss = 0.03334738\n",
      "Iteration 193, loss = 0.03343452\n",
      "Iteration 194, loss = 0.03320835\n",
      "Iteration 195, loss = 0.03316958\n",
      "Iteration 196, loss = 0.03302548\n",
      "Iteration 197, loss = 0.03301022\n",
      "Iteration 198, loss = 0.03284568\n",
      "Iteration 199, loss = 0.03309408\n",
      "Iteration 200, loss = 0.03281703\n",
      "Iteration 201, loss = 0.03270431\n",
      "Iteration 202, loss = 0.03256041\n",
      "Iteration 203, loss = 0.03260500\n",
      "Iteration 204, loss = 0.03261272\n",
      "Iteration 205, loss = 0.03248943\n",
      "Iteration 206, loss = 0.03220818\n",
      "Iteration 207, loss = 0.03213133\n",
      "Iteration 208, loss = 0.03207993\n",
      "Iteration 209, loss = 0.03211965\n",
      "Iteration 210, loss = 0.03264114\n",
      "Iteration 211, loss = 0.03190498\n",
      "Iteration 212, loss = 0.03229539\n",
      "Iteration 213, loss = 0.03166786\n",
      "Iteration 214, loss = 0.03186875\n",
      "Iteration 215, loss = 0.03156490\n",
      "Iteration 216, loss = 0.03168782\n",
      "Iteration 217, loss = 0.03135819\n",
      "Iteration 218, loss = 0.03142369\n",
      "Iteration 219, loss = 0.03133153\n",
      "Iteration 220, loss = 0.03113463\n",
      "Iteration 221, loss = 0.03119866\n",
      "Iteration 222, loss = 0.03109045\n",
      "Iteration 223, loss = 0.03097961\n",
      "Iteration 224, loss = 0.03079265\n",
      "Iteration 225, loss = 0.03075858\n",
      "Iteration 226, loss = 0.03091391\n",
      "Iteration 227, loss = 0.03083612\n",
      "Iteration 228, loss = 0.03062403\n",
      "Iteration 229, loss = 0.03095521\n",
      "Iteration 230, loss = 0.03043385\n",
      "Iteration 231, loss = 0.03059184\n",
      "Iteration 232, loss = 0.03064192\n",
      "Iteration 233, loss = 0.03035873\n",
      "Iteration 234, loss = 0.03027499\n",
      "Iteration 235, loss = 0.03050291\n",
      "Iteration 236, loss = 0.03063882\n",
      "Iteration 237, loss = 0.03055537\n",
      "Iteration 238, loss = 0.03010144\n",
      "Iteration 239, loss = 0.02986276\n",
      "Iteration 240, loss = 0.02990988\n",
      "Iteration 241, loss = 0.02989047\n",
      "Iteration 242, loss = 0.02985931\n",
      "Iteration 243, loss = 0.02998747\n",
      "Iteration 244, loss = 0.02971139\n",
      "Iteration 245, loss = 0.02962331\n",
      "Iteration 246, loss = 0.02968024\n",
      "Iteration 247, loss = 0.02943431\n",
      "Iteration 248, loss = 0.02951956\n",
      "Iteration 249, loss = 0.02954610\n",
      "Iteration 250, loss = 0.02952260\n",
      "Iteration 251, loss = 0.02939699\n",
      "Iteration 252, loss = 0.02933148\n",
      "Iteration 253, loss = 0.02921072\n",
      "Iteration 254, loss = 0.02911860\n",
      "Iteration 255, loss = 0.02940954\n",
      "Iteration 256, loss = 0.02905913\n",
      "Iteration 257, loss = 0.02901932\n",
      "Iteration 258, loss = 0.02928075\n",
      "Iteration 259, loss = 0.02882852\n",
      "Iteration 260, loss = 0.02896649\n",
      "Iteration 261, loss = 0.02894482\n",
      "Iteration 262, loss = 0.02888866\n",
      "Iteration 263, loss = 0.02868618\n",
      "Iteration 264, loss = 0.02898165\n",
      "Iteration 265, loss = 0.02870638\n",
      "Iteration 266, loss = 0.02884678\n",
      "Iteration 267, loss = 0.02848716\n",
      "Iteration 268, loss = 0.02874984\n",
      "Iteration 269, loss = 0.02865925\n",
      "Iteration 270, loss = 0.02841546\n",
      "Iteration 271, loss = 0.02846470\n",
      "Iteration 272, loss = 0.02827395\n",
      "Iteration 273, loss = 0.02817485\n",
      "Iteration 274, loss = 0.02863485\n",
      "Iteration 275, loss = 0.02836551\n",
      "Iteration 276, loss = 0.02820176\n",
      "Iteration 277, loss = 0.02859465\n",
      "Iteration 278, loss = 0.02799530\n",
      "Iteration 279, loss = 0.02817517\n",
      "Iteration 280, loss = 0.02841692\n",
      "Iteration 281, loss = 0.02807919\n",
      "Iteration 282, loss = 0.02791965\n",
      "Iteration 283, loss = 0.02792173\n",
      "Iteration 284, loss = 0.02782292\n",
      "Iteration 285, loss = 0.02796200\n",
      "Iteration 286, loss = 0.02760982\n",
      "Iteration 287, loss = 0.02775515\n",
      "Iteration 288, loss = 0.02803358\n",
      "Iteration 289, loss = 0.02788796\n",
      "Iteration 290, loss = 0.02759197\n",
      "Iteration 291, loss = 0.02783042\n",
      "Iteration 292, loss = 0.02808208\n",
      "Iteration 293, loss = 0.02794471\n",
      "Iteration 294, loss = 0.02760016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 295, loss = 0.02746722\n",
      "Iteration 296, loss = 0.02754854\n",
      "Iteration 297, loss = 0.02749754\n",
      "Iteration 298, loss = 0.02741624\n",
      "Iteration 299, loss = 0.02746214\n",
      "Iteration 300, loss = 0.02733934\n",
      "Iteration 301, loss = 0.02726688\n",
      "Iteration 302, loss = 0.02725981\n",
      "Iteration 303, loss = 0.02735287\n",
      "Iteration 304, loss = 0.02711124\n",
      "Iteration 305, loss = 0.02715416\n",
      "Iteration 306, loss = 0.02733482\n",
      "Iteration 307, loss = 0.02687339\n",
      "Iteration 308, loss = 0.02715780\n",
      "Iteration 309, loss = 0.02686581\n",
      "Iteration 310, loss = 0.02718477\n",
      "Iteration 311, loss = 0.02704610\n",
      "Iteration 312, loss = 0.02698338\n",
      "Iteration 313, loss = 0.02683147\n",
      "Iteration 314, loss = 0.02679193\n",
      "Iteration 315, loss = 0.02688337\n",
      "Iteration 316, loss = 0.02702309\n",
      "Iteration 317, loss = 0.02676473\n",
      "Iteration 318, loss = 0.02684801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 91.57650695517773\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69225011\n",
      "Iteration 2, loss = 0.68721607\n",
      "Iteration 3, loss = 0.68063253\n",
      "Iteration 4, loss = 0.67122557\n",
      "Iteration 5, loss = 0.65842293\n",
      "Iteration 6, loss = 0.64223406\n",
      "Iteration 7, loss = 0.62298912\n",
      "Iteration 8, loss = 0.60120817\n",
      "Iteration 9, loss = 0.57740045\n",
      "Iteration 10, loss = 0.55220785\n",
      "Iteration 11, loss = 0.52571993\n",
      "Iteration 12, loss = 0.49930493\n",
      "Iteration 13, loss = 0.47243004\n",
      "Iteration 14, loss = 0.44630391\n",
      "Iteration 15, loss = 0.42118833\n",
      "Iteration 16, loss = 0.39685850\n",
      "Iteration 17, loss = 0.37398347\n",
      "Iteration 18, loss = 0.35275778\n",
      "Iteration 19, loss = 0.33268639\n",
      "Iteration 20, loss = 0.31408697\n",
      "Iteration 21, loss = 0.29708526\n",
      "Iteration 22, loss = 0.28108586\n",
      "Iteration 23, loss = 0.26659328\n",
      "Iteration 24, loss = 0.25332959\n",
      "Iteration 25, loss = 0.24039899\n",
      "Iteration 26, loss = 0.22915113\n",
      "Iteration 27, loss = 0.21842489\n",
      "Iteration 28, loss = 0.20922801\n",
      "Iteration 29, loss = 0.19969560\n",
      "Iteration 30, loss = 0.19139805\n",
      "Iteration 31, loss = 0.18369276\n",
      "Iteration 32, loss = 0.17701241\n",
      "Iteration 33, loss = 0.17007133\n",
      "Iteration 34, loss = 0.16428231\n",
      "Iteration 35, loss = 0.15823557\n",
      "Iteration 36, loss = 0.15277120\n",
      "Iteration 37, loss = 0.14771457\n",
      "Iteration 38, loss = 0.14324664\n",
      "Iteration 39, loss = 0.13874825\n",
      "Iteration 40, loss = 0.13467798\n",
      "Iteration 41, loss = 0.13050692\n",
      "Iteration 42, loss = 0.12688975\n",
      "Iteration 43, loss = 0.12349369\n",
      "Iteration 44, loss = 0.12026109\n",
      "Iteration 45, loss = 0.11719135\n",
      "Iteration 46, loss = 0.11405093\n",
      "Iteration 47, loss = 0.11133522\n",
      "Iteration 48, loss = 0.10860834\n",
      "Iteration 49, loss = 0.10621536\n",
      "Iteration 50, loss = 0.10364859\n",
      "Iteration 51, loss = 0.10152591\n",
      "Iteration 52, loss = 0.09924389\n",
      "Iteration 53, loss = 0.09764815\n",
      "Iteration 54, loss = 0.09548422\n",
      "Iteration 55, loss = 0.09339868\n",
      "Iteration 56, loss = 0.09145629\n",
      "Iteration 57, loss = 0.08973790\n",
      "Iteration 58, loss = 0.08783157\n",
      "Iteration 59, loss = 0.08619529\n",
      "Iteration 60, loss = 0.08467917\n",
      "Iteration 61, loss = 0.08342651\n",
      "Iteration 62, loss = 0.08178972\n",
      "Iteration 63, loss = 0.08074428\n",
      "Iteration 64, loss = 0.07906004\n",
      "Iteration 65, loss = 0.07769688\n",
      "Iteration 66, loss = 0.07671908\n",
      "Iteration 67, loss = 0.07531130\n",
      "Iteration 68, loss = 0.07418230\n",
      "Iteration 69, loss = 0.07302875\n",
      "Iteration 70, loss = 0.07206893\n",
      "Iteration 71, loss = 0.07094058\n",
      "Iteration 72, loss = 0.06994353\n",
      "Iteration 73, loss = 0.06900655\n",
      "Iteration 74, loss = 0.06804485\n",
      "Iteration 75, loss = 0.06707177\n",
      "Iteration 76, loss = 0.06621259\n",
      "Iteration 77, loss = 0.06564242\n",
      "Iteration 78, loss = 0.06446437\n",
      "Iteration 79, loss = 0.06381371\n",
      "Iteration 80, loss = 0.06308436\n",
      "Iteration 81, loss = 0.06218322\n",
      "Iteration 82, loss = 0.06149495\n",
      "Iteration 83, loss = 0.06098752\n",
      "Iteration 84, loss = 0.06015130\n",
      "Iteration 85, loss = 0.05939997\n",
      "Iteration 86, loss = 0.05886078\n",
      "Iteration 87, loss = 0.05820457\n",
      "Iteration 88, loss = 0.05745053\n",
      "Iteration 89, loss = 0.05674252\n",
      "Iteration 90, loss = 0.05646317\n",
      "Iteration 91, loss = 0.05579460\n",
      "Iteration 92, loss = 0.05516123\n",
      "Iteration 93, loss = 0.05479742\n",
      "Iteration 94, loss = 0.05397486\n",
      "Iteration 95, loss = 0.05379150\n",
      "Iteration 96, loss = 0.05305790\n",
      "Iteration 97, loss = 0.05279659\n",
      "Iteration 98, loss = 0.05195812\n",
      "Iteration 99, loss = 0.05167733\n",
      "Iteration 100, loss = 0.05114700\n",
      "Iteration 101, loss = 0.05083177\n",
      "Iteration 102, loss = 0.05044538\n",
      "Iteration 103, loss = 0.05009547\n",
      "Iteration 104, loss = 0.04946155\n",
      "Iteration 105, loss = 0.04899578\n",
      "Iteration 106, loss = 0.04874941\n",
      "Iteration 107, loss = 0.04836425\n",
      "Iteration 108, loss = 0.04789716\n",
      "Iteration 109, loss = 0.04749181\n",
      "Iteration 110, loss = 0.04716519\n",
      "Iteration 111, loss = 0.04694882\n",
      "Iteration 112, loss = 0.04663908\n",
      "Iteration 113, loss = 0.04658842\n",
      "Iteration 114, loss = 0.04570059\n",
      "Iteration 115, loss = 0.04536067\n",
      "Iteration 116, loss = 0.04518635\n",
      "Iteration 117, loss = 0.04497972\n",
      "Iteration 118, loss = 0.04461751\n",
      "Iteration 119, loss = 0.04422965\n",
      "Iteration 120, loss = 0.04421427\n",
      "Iteration 121, loss = 0.04374304\n",
      "Iteration 122, loss = 0.04349077\n",
      "Iteration 123, loss = 0.04318510\n",
      "Iteration 124, loss = 0.04310738\n",
      "Iteration 125, loss = 0.04291604\n",
      "Iteration 126, loss = 0.04219473\n",
      "Iteration 127, loss = 0.04230492\n",
      "Iteration 128, loss = 0.04186072\n",
      "Iteration 129, loss = 0.04156326\n",
      "Iteration 130, loss = 0.04128947\n",
      "Iteration 131, loss = 0.04104277\n",
      "Iteration 132, loss = 0.04089367\n",
      "Iteration 133, loss = 0.04063911\n",
      "Iteration 134, loss = 0.04029491\n",
      "Iteration 135, loss = 0.04011431\n",
      "Iteration 136, loss = 0.04002294\n",
      "Iteration 137, loss = 0.03989814\n",
      "Iteration 138, loss = 0.03954053\n",
      "Iteration 139, loss = 0.03936291\n",
      "Iteration 140, loss = 0.03919129\n",
      "Iteration 141, loss = 0.03898564\n",
      "Iteration 142, loss = 0.03912330\n",
      "Iteration 143, loss = 0.03876216\n",
      "Iteration 144, loss = 0.03833831\n",
      "Iteration 145, loss = 0.03855401\n",
      "Iteration 146, loss = 0.03824610\n",
      "Iteration 147, loss = 0.03768694\n",
      "Iteration 148, loss = 0.03761887\n",
      "Iteration 149, loss = 0.03770252\n",
      "Iteration 150, loss = 0.03736622\n",
      "Iteration 151, loss = 0.03733186\n",
      "Iteration 152, loss = 0.03709894\n",
      "Iteration 153, loss = 0.03675608\n",
      "Iteration 154, loss = 0.03665613\n",
      "Iteration 155, loss = 0.03655902\n",
      "Iteration 156, loss = 0.03647568\n",
      "Iteration 157, loss = 0.03626698\n",
      "Iteration 158, loss = 0.03600140\n",
      "Iteration 159, loss = 0.03617053\n",
      "Iteration 160, loss = 0.03598122\n",
      "Iteration 161, loss = 0.03564656\n",
      "Iteration 162, loss = 0.03550811\n",
      "Iteration 163, loss = 0.03533966\n",
      "Iteration 164, loss = 0.03520334\n",
      "Iteration 165, loss = 0.03506544\n",
      "Iteration 166, loss = 0.03511535\n",
      "Iteration 167, loss = 0.03499358\n",
      "Iteration 168, loss = 0.03492620\n",
      "Iteration 169, loss = 0.03478862\n",
      "Iteration 170, loss = 0.03475760\n",
      "Iteration 171, loss = 0.03419022\n",
      "Iteration 172, loss = 0.03414908\n",
      "Iteration 173, loss = 0.03404457\n",
      "Iteration 174, loss = 0.03399953\n",
      "Iteration 175, loss = 0.03395843\n",
      "Iteration 176, loss = 0.03387810\n",
      "Iteration 177, loss = 0.03361972\n",
      "Iteration 178, loss = 0.03358138\n",
      "Iteration 179, loss = 0.03340505\n",
      "Iteration 180, loss = 0.03335743\n",
      "Iteration 181, loss = 0.03347730\n",
      "Iteration 182, loss = 0.03325046\n",
      "Iteration 183, loss = 0.03309419\n",
      "Iteration 184, loss = 0.03281909\n",
      "Iteration 185, loss = 0.03299531\n",
      "Iteration 186, loss = 0.03317345\n",
      "Iteration 187, loss = 0.03267248\n",
      "Iteration 188, loss = 0.03249345\n",
      "Iteration 189, loss = 0.03239641\n",
      "Iteration 190, loss = 0.03238791\n",
      "Iteration 191, loss = 0.03206971\n",
      "Iteration 192, loss = 0.03215217\n",
      "Iteration 193, loss = 0.03219078\n",
      "Iteration 194, loss = 0.03183866\n",
      "Iteration 195, loss = 0.03188006\n",
      "Iteration 196, loss = 0.03179704\n",
      "Iteration 197, loss = 0.03161066\n",
      "Iteration 198, loss = 0.03199592\n",
      "Iteration 199, loss = 0.03198502\n",
      "Iteration 200, loss = 0.03146532\n",
      "Iteration 201, loss = 0.03158173\n",
      "Iteration 202, loss = 0.03141887\n",
      "Iteration 203, loss = 0.03122168\n",
      "Iteration 204, loss = 0.03115636\n",
      "Iteration 205, loss = 0.03112918\n",
      "Iteration 206, loss = 0.03086089\n",
      "Iteration 207, loss = 0.03093434\n",
      "Iteration 208, loss = 0.03088543\n",
      "Iteration 209, loss = 0.03107748\n",
      "Iteration 210, loss = 0.03063003\n",
      "Iteration 211, loss = 0.03043453\n",
      "Iteration 212, loss = 0.03057098\n",
      "Iteration 213, loss = 0.03043173\n",
      "Iteration 214, loss = 0.03057273\n",
      "Iteration 215, loss = 0.03036239\n",
      "Iteration 216, loss = 0.03030986\n",
      "Iteration 217, loss = 0.03036048\n",
      "Iteration 218, loss = 0.03004618\n",
      "Iteration 219, loss = 0.02994609\n",
      "Iteration 220, loss = 0.02996491\n",
      "Iteration 221, loss = 0.03011215\n",
      "Iteration 222, loss = 0.02984296\n",
      "Iteration 223, loss = 0.03000944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 224, loss = 0.02961865\n",
      "Iteration 225, loss = 0.02966182\n",
      "Iteration 226, loss = 0.02966716\n",
      "Iteration 227, loss = 0.02980677\n",
      "Iteration 228, loss = 0.02932859\n",
      "Iteration 229, loss = 0.02932543\n",
      "Iteration 230, loss = 0.02919365\n",
      "Iteration 231, loss = 0.02909309\n",
      "Iteration 232, loss = 0.02926480\n",
      "Iteration 233, loss = 0.02961888\n",
      "Iteration 234, loss = 0.02995826\n",
      "Iteration 235, loss = 0.02935488\n",
      "Iteration 236, loss = 0.02925359\n",
      "Iteration 237, loss = 0.02895283\n",
      "Iteration 238, loss = 0.02893369\n",
      "Iteration 239, loss = 0.02882238\n",
      "Iteration 240, loss = 0.02866868\n",
      "Iteration 241, loss = 0.02867243\n",
      "Iteration 242, loss = 0.02865424\n",
      "Iteration 243, loss = 0.02866525\n",
      "Iteration 244, loss = 0.02844130\n",
      "Iteration 245, loss = 0.02862943\n",
      "Iteration 246, loss = 0.02839482\n",
      "Iteration 247, loss = 0.02832809\n",
      "Iteration 248, loss = 0.02821087\n",
      "Iteration 249, loss = 0.02821199\n",
      "Iteration 250, loss = 0.02812455\n",
      "Iteration 251, loss = 0.02830480\n",
      "Iteration 252, loss = 0.02814572\n",
      "Iteration 253, loss = 0.02831969\n",
      "Iteration 254, loss = 0.02835617\n",
      "Iteration 255, loss = 0.02841639\n",
      "Iteration 256, loss = 0.02802013\n",
      "Iteration 257, loss = 0.02787731\n",
      "Iteration 258, loss = 0.02793278\n",
      "Iteration 259, loss = 0.02773594\n",
      "Iteration 260, loss = 0.02768474\n",
      "Iteration 261, loss = 0.02762125\n",
      "Iteration 262, loss = 0.02773395\n",
      "Iteration 263, loss = 0.02784038\n",
      "Iteration 264, loss = 0.02751889\n",
      "Iteration 265, loss = 0.02743142\n",
      "Iteration 266, loss = 0.02765208\n",
      "Iteration 267, loss = 0.02727361\n",
      "Iteration 268, loss = 0.02734084\n",
      "Iteration 269, loss = 0.02770949\n",
      "Iteration 270, loss = 0.02716186\n",
      "Iteration 271, loss = 0.02720793\n",
      "Iteration 272, loss = 0.02751228\n",
      "Iteration 273, loss = 0.02720060\n",
      "Iteration 274, loss = 0.02715510\n",
      "Iteration 275, loss = 0.02709429\n",
      "Iteration 276, loss = 0.02705393\n",
      "Iteration 277, loss = 0.02679441\n",
      "Iteration 278, loss = 0.02723673\n",
      "Iteration 279, loss = 0.02689773\n",
      "Iteration 280, loss = 0.02674123\n",
      "Iteration 281, loss = 0.02722884\n",
      "Iteration 282, loss = 0.02720692\n",
      "Iteration 283, loss = 0.02693439\n",
      "Iteration 284, loss = 0.02677307\n",
      "Iteration 285, loss = 0.02660703\n",
      "Iteration 286, loss = 0.02670762\n",
      "Iteration 287, loss = 0.02676827\n",
      "Iteration 288, loss = 0.02711645\n",
      "Iteration 289, loss = 0.02682924\n",
      "Iteration 290, loss = 0.02677650\n",
      "Iteration 291, loss = 0.02676793\n",
      "Iteration 292, loss = 0.02657484\n",
      "Iteration 293, loss = 0.02632438\n",
      "Iteration 294, loss = 0.02652176\n",
      "Iteration 295, loss = 0.02630348\n",
      "Iteration 296, loss = 0.02613617\n",
      "Iteration 297, loss = 0.02644328\n",
      "Iteration 298, loss = 0.02616931\n",
      "Iteration 299, loss = 0.02624465\n",
      "Iteration 300, loss = 0.02616503\n",
      "Iteration 301, loss = 0.02635428\n",
      "Iteration 302, loss = 0.02617622\n",
      "Iteration 303, loss = 0.02593329\n",
      "Iteration 304, loss = 0.02638073\n",
      "Iteration 305, loss = 0.02602028\n",
      "Iteration 306, loss = 0.02587058\n",
      "Iteration 307, loss = 0.02582809\n",
      "Iteration 308, loss = 0.02594987\n",
      "Iteration 309, loss = 0.02579546\n",
      "Iteration 310, loss = 0.02571735\n",
      "Iteration 311, loss = 0.02585931\n",
      "Iteration 312, loss = 0.02559852\n",
      "Iteration 313, loss = 0.02555277\n",
      "Iteration 314, loss = 0.02586384\n",
      "Iteration 315, loss = 0.02574391\n",
      "Iteration 316, loss = 0.02594938\n",
      "Iteration 317, loss = 0.02571325\n",
      "Iteration 318, loss = 0.02555514\n",
      "Iteration 319, loss = 0.02608634\n",
      "Iteration 320, loss = 0.02568794\n",
      "Iteration 321, loss = 0.02538980\n",
      "Iteration 322, loss = 0.02523400\n",
      "Iteration 323, loss = 0.02588267\n",
      "Iteration 324, loss = 0.02548850\n",
      "Iteration 325, loss = 0.02525145\n",
      "Iteration 326, loss = 0.02530472\n",
      "Iteration 327, loss = 0.02553818\n",
      "Iteration 328, loss = 0.02525629\n",
      "Iteration 329, loss = 0.02515213\n",
      "Iteration 330, loss = 0.02566246\n",
      "Iteration 331, loss = 0.02536165\n",
      "Iteration 332, loss = 0.02510294\n",
      "Iteration 333, loss = 0.02504350\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 90.34003091190108\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 91.49922720247295 %\n",
      "Fold 1: 91.73106646058733 %\n",
      "Fold 2: 90.49459041731066 %\n",
      "Fold 3: 91.57650695517773 %\n",
      "Fold 4: 90.34003091190108 %\n",
      "Average: 91.12828438948995 %\n",
      "Iteration 1, loss = 0.69170245\n",
      "Iteration 2, loss = 0.68495078\n",
      "Iteration 3, loss = 0.67480799\n",
      "Iteration 4, loss = 0.65969835\n",
      "Iteration 5, loss = 0.63932403\n",
      "Iteration 6, loss = 0.61418474\n",
      "Iteration 7, loss = 0.58526252\n",
      "Iteration 8, loss = 0.55376397\n",
      "Iteration 9, loss = 0.52068655\n",
      "Iteration 10, loss = 0.48760757\n",
      "Iteration 11, loss = 0.45513920\n",
      "Iteration 12, loss = 0.42399517\n",
      "Iteration 13, loss = 0.39502168\n",
      "Iteration 14, loss = 0.36832775\n",
      "Iteration 15, loss = 0.34265244\n",
      "Iteration 16, loss = 0.31963357\n",
      "Iteration 17, loss = 0.29897705\n",
      "Iteration 18, loss = 0.27997010\n",
      "Iteration 19, loss = 0.26296391\n",
      "Iteration 20, loss = 0.24775220\n",
      "Iteration 21, loss = 0.23379697\n",
      "Iteration 22, loss = 0.22164179\n",
      "Iteration 23, loss = 0.20990695\n",
      "Iteration 24, loss = 0.19991808\n",
      "Iteration 25, loss = 0.19084372\n",
      "Iteration 26, loss = 0.18217800\n",
      "Iteration 27, loss = 0.17445995\n",
      "Iteration 28, loss = 0.16740769\n",
      "Iteration 29, loss = 0.16105571\n",
      "Iteration 30, loss = 0.15536910\n",
      "Iteration 31, loss = 0.14954163\n",
      "Iteration 32, loss = 0.14440358\n",
      "Iteration 33, loss = 0.13966069\n",
      "Iteration 34, loss = 0.13517043\n",
      "Iteration 35, loss = 0.13126638\n",
      "Iteration 36, loss = 0.12735864\n",
      "Iteration 37, loss = 0.12370212\n",
      "Iteration 38, loss = 0.12067160\n",
      "Iteration 39, loss = 0.11717968\n",
      "Iteration 40, loss = 0.11432468\n",
      "Iteration 41, loss = 0.11124165\n",
      "Iteration 42, loss = 0.10864019\n",
      "Iteration 43, loss = 0.10601060\n",
      "Iteration 44, loss = 0.10371518\n",
      "Iteration 45, loss = 0.10129331\n",
      "Iteration 46, loss = 0.09914019\n",
      "Iteration 47, loss = 0.09691248\n",
      "Iteration 48, loss = 0.09496552\n",
      "Iteration 49, loss = 0.09295751\n",
      "Iteration 50, loss = 0.09125074\n",
      "Iteration 51, loss = 0.08983205\n",
      "Iteration 52, loss = 0.08789095\n",
      "Iteration 53, loss = 0.08655003\n",
      "Iteration 54, loss = 0.08474924\n",
      "Iteration 55, loss = 0.08330707\n",
      "Iteration 56, loss = 0.08182590\n",
      "Iteration 57, loss = 0.08056842\n",
      "Iteration 58, loss = 0.07907061\n",
      "Iteration 59, loss = 0.07790630\n",
      "Iteration 60, loss = 0.07659805\n",
      "Iteration 61, loss = 0.07588699\n",
      "Iteration 62, loss = 0.07444372\n",
      "Iteration 63, loss = 0.07356254\n",
      "Iteration 64, loss = 0.07232758\n",
      "Iteration 65, loss = 0.07132161\n",
      "Iteration 66, loss = 0.07040665\n",
      "Iteration 67, loss = 0.07014638\n",
      "Iteration 68, loss = 0.06897932\n",
      "Iteration 69, loss = 0.06775549\n",
      "Iteration 70, loss = 0.06693405\n",
      "Iteration 71, loss = 0.06618636\n",
      "Iteration 72, loss = 0.06517732\n",
      "Iteration 73, loss = 0.06448756\n",
      "Iteration 74, loss = 0.06373287\n",
      "Iteration 75, loss = 0.06280087\n",
      "Iteration 76, loss = 0.06219810\n",
      "Iteration 77, loss = 0.06152237\n",
      "Iteration 78, loss = 0.06094158\n",
      "Iteration 79, loss = 0.06043767\n",
      "Iteration 80, loss = 0.05992356\n",
      "Iteration 81, loss = 0.05915210\n",
      "Iteration 82, loss = 0.05903257\n",
      "Iteration 83, loss = 0.05777612\n",
      "Iteration 84, loss = 0.05753178\n",
      "Iteration 85, loss = 0.05675104\n",
      "Iteration 86, loss = 0.05612961\n",
      "Iteration 87, loss = 0.05578657\n",
      "Iteration 88, loss = 0.05513655\n",
      "Iteration 89, loss = 0.05471695\n",
      "Iteration 90, loss = 0.05426733\n",
      "Iteration 91, loss = 0.05424746\n",
      "Iteration 92, loss = 0.05340150\n",
      "Iteration 93, loss = 0.05310171\n",
      "Iteration 94, loss = 0.05318089\n",
      "Iteration 95, loss = 0.05281725\n",
      "Iteration 96, loss = 0.05164095\n",
      "Iteration 97, loss = 0.05156627\n",
      "Iteration 98, loss = 0.05085139\n",
      "Iteration 99, loss = 0.05056760\n",
      "Iteration 100, loss = 0.05086622\n",
      "Iteration 101, loss = 0.04966871\n",
      "Iteration 102, loss = 0.04950573\n",
      "Iteration 103, loss = 0.04924121\n",
      "Iteration 104, loss = 0.04907248\n",
      "Iteration 105, loss = 0.04860205\n",
      "Iteration 106, loss = 0.04805090\n",
      "Iteration 107, loss = 0.04785420\n",
      "Iteration 108, loss = 0.04751140\n",
      "Iteration 109, loss = 0.04733103\n",
      "Iteration 110, loss = 0.04706463\n",
      "Iteration 111, loss = 0.04686781\n",
      "Iteration 112, loss = 0.04637026\n",
      "Iteration 113, loss = 0.04603985\n",
      "Iteration 114, loss = 0.04590378\n",
      "Iteration 115, loss = 0.04568756\n",
      "Iteration 116, loss = 0.04525208\n",
      "Iteration 117, loss = 0.04505909\n",
      "Iteration 118, loss = 0.04481723\n",
      "Iteration 119, loss = 0.04463821\n",
      "Iteration 120, loss = 0.04423602\n",
      "Iteration 121, loss = 0.04395057\n",
      "Iteration 122, loss = 0.04387196\n",
      "Iteration 123, loss = 0.04353129\n",
      "Iteration 124, loss = 0.04341361\n",
      "Iteration 125, loss = 0.04314967\n",
      "Iteration 126, loss = 0.04279519\n",
      "Iteration 127, loss = 0.04278100\n",
      "Iteration 128, loss = 0.04252575\n",
      "Iteration 129, loss = 0.04253679\n",
      "Iteration 130, loss = 0.04225517\n",
      "Iteration 131, loss = 0.04204553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 132, loss = 0.04178067\n",
      "Iteration 133, loss = 0.04164141\n",
      "Iteration 134, loss = 0.04126807\n",
      "Iteration 135, loss = 0.04116827\n",
      "Iteration 136, loss = 0.04091685\n",
      "Iteration 137, loss = 0.04072281\n",
      "Iteration 138, loss = 0.04049066\n",
      "Iteration 139, loss = 0.04027090\n",
      "Iteration 140, loss = 0.04024452\n",
      "Iteration 141, loss = 0.04009054\n",
      "Iteration 142, loss = 0.03995420\n",
      "Iteration 143, loss = 0.03968997\n",
      "Iteration 144, loss = 0.03990614\n",
      "Iteration 145, loss = 0.03928193\n",
      "Iteration 146, loss = 0.03933397\n",
      "Iteration 147, loss = 0.03924429\n",
      "Iteration 148, loss = 0.03904208\n",
      "Iteration 149, loss = 0.03878403\n",
      "Iteration 150, loss = 0.03860364\n",
      "Iteration 151, loss = 0.03843142\n",
      "Iteration 152, loss = 0.03833499\n",
      "Iteration 153, loss = 0.03815291\n",
      "Iteration 154, loss = 0.03800293\n",
      "Iteration 155, loss = 0.03839202\n",
      "Iteration 156, loss = 0.03790869\n",
      "Iteration 157, loss = 0.03829860\n",
      "Iteration 158, loss = 0.03779068\n",
      "Iteration 159, loss = 0.03747993\n",
      "Iteration 160, loss = 0.03736130\n",
      "Iteration 161, loss = 0.03715460\n",
      "Iteration 162, loss = 0.03702218\n",
      "Iteration 163, loss = 0.03718451\n",
      "Iteration 164, loss = 0.03699016\n",
      "Iteration 165, loss = 0.03712723\n",
      "Iteration 166, loss = 0.03687039\n",
      "Iteration 167, loss = 0.03662907\n",
      "Iteration 168, loss = 0.03666180\n",
      "Iteration 169, loss = 0.03649806\n",
      "Iteration 170, loss = 0.03612012\n",
      "Iteration 171, loss = 0.03616469\n",
      "Iteration 172, loss = 0.03591171\n",
      "Iteration 173, loss = 0.03648249\n",
      "Iteration 174, loss = 0.03652662\n",
      "Iteration 175, loss = 0.03594982\n",
      "Iteration 176, loss = 0.03583132\n",
      "Iteration 177, loss = 0.03551805\n",
      "Iteration 178, loss = 0.03535289\n",
      "Iteration 179, loss = 0.03540919\n",
      "Iteration 180, loss = 0.03549043\n",
      "Iteration 181, loss = 0.03504723\n",
      "Iteration 182, loss = 0.03490995\n",
      "Iteration 183, loss = 0.03475881\n",
      "Iteration 184, loss = 0.03465669\n",
      "Iteration 185, loss = 0.03467624\n",
      "Iteration 186, loss = 0.03462656\n",
      "Iteration 187, loss = 0.03463942\n",
      "Iteration 188, loss = 0.03502532\n",
      "Iteration 189, loss = 0.03433909\n",
      "Iteration 190, loss = 0.03427889\n",
      "Iteration 191, loss = 0.03407746\n",
      "Iteration 192, loss = 0.03443910\n",
      "Iteration 193, loss = 0.03408674\n",
      "Iteration 194, loss = 0.03425775\n",
      "Iteration 195, loss = 0.03405121\n",
      "Iteration 196, loss = 0.03381525\n",
      "Iteration 197, loss = 0.03391835\n",
      "Iteration 198, loss = 0.03364698\n",
      "Iteration 199, loss = 0.03346331\n",
      "Iteration 200, loss = 0.03359634\n",
      "Iteration 201, loss = 0.03348007\n",
      "Iteration 202, loss = 0.03378609\n",
      "Iteration 203, loss = 0.03336869\n",
      "Iteration 204, loss = 0.03318802\n",
      "Iteration 205, loss = 0.03310857\n",
      "Iteration 206, loss = 0.03317019\n",
      "Iteration 207, loss = 0.03284432\n",
      "Iteration 208, loss = 0.03286302\n",
      "Iteration 209, loss = 0.03311458\n",
      "Iteration 210, loss = 0.03291467\n",
      "Iteration 211, loss = 0.03298560\n",
      "Iteration 212, loss = 0.03262111\n",
      "Iteration 213, loss = 0.03273869\n",
      "Iteration 214, loss = 0.03234197\n",
      "Iteration 215, loss = 0.03269255\n",
      "Iteration 216, loss = 0.03232197\n",
      "Iteration 217, loss = 0.03252703\n",
      "Iteration 218, loss = 0.03237805\n",
      "Iteration 219, loss = 0.03213736\n",
      "Iteration 220, loss = 0.03245134\n",
      "Iteration 221, loss = 0.03213853\n",
      "Iteration 222, loss = 0.03208339\n",
      "Iteration 223, loss = 0.03238181\n",
      "Iteration 224, loss = 0.03184948\n",
      "Iteration 225, loss = 0.03212347\n",
      "Iteration 226, loss = 0.03183478\n",
      "Iteration 227, loss = 0.03185100\n",
      "Iteration 228, loss = 0.03159084\n",
      "Iteration 229, loss = 0.03167466\n",
      "Iteration 230, loss = 0.03165531\n",
      "Iteration 231, loss = 0.03160508\n",
      "Iteration 232, loss = 0.03147904\n",
      "Iteration 233, loss = 0.03174260\n",
      "Iteration 234, loss = 0.03168707\n",
      "Iteration 235, loss = 0.03129638\n",
      "Iteration 236, loss = 0.03146165\n",
      "Iteration 237, loss = 0.03123397\n",
      "Iteration 238, loss = 0.03146284\n",
      "Iteration 239, loss = 0.03132755\n",
      "Iteration 240, loss = 0.03086714\n",
      "Iteration 241, loss = 0.03106275\n",
      "Iteration 242, loss = 0.03131403\n",
      "Iteration 243, loss = 0.03094520\n",
      "Iteration 244, loss = 0.03058579\n",
      "Iteration 245, loss = 0.03111795\n",
      "Iteration 246, loss = 0.03073591\n",
      "Iteration 247, loss = 0.03067456\n",
      "Iteration 248, loss = 0.03065049\n",
      "Iteration 249, loss = 0.03054509\n",
      "Iteration 250, loss = 0.03078025\n",
      "Iteration 251, loss = 0.03107457\n",
      "Iteration 252, loss = 0.03075853\n",
      "Iteration 253, loss = 0.03064094\n",
      "Iteration 254, loss = 0.03117990\n",
      "Iteration 255, loss = 0.03029375\n",
      "Iteration 256, loss = 0.03043403\n",
      "Iteration 257, loss = 0.03061738\n",
      "Iteration 258, loss = 0.03022837\n",
      "Iteration 259, loss = 0.03037853\n",
      "Iteration 260, loss = 0.03047732\n",
      "Iteration 261, loss = 0.03027321\n",
      "Iteration 262, loss = 0.03027745\n",
      "Iteration 263, loss = 0.03015764\n",
      "Iteration 264, loss = 0.03008361\n",
      "Iteration 265, loss = 0.02978951\n",
      "Iteration 266, loss = 0.03036045\n",
      "Iteration 267, loss = 0.02993029\n",
      "Iteration 268, loss = 0.02988598\n",
      "Iteration 269, loss = 0.02989525\n",
      "Iteration 270, loss = 0.02979081\n",
      "Iteration 271, loss = 0.02982189\n",
      "Iteration 272, loss = 0.02955201\n",
      "Iteration 273, loss = 0.02978865\n",
      "Iteration 274, loss = 0.02980661\n",
      "Iteration 275, loss = 0.02968401\n",
      "Iteration 276, loss = 0.02958223\n",
      "Iteration 277, loss = 0.02984563\n",
      "Iteration 278, loss = 0.02974457\n",
      "Iteration 279, loss = 0.02953609\n",
      "Iteration 280, loss = 0.02931146\n",
      "Iteration 281, loss = 0.02973489\n",
      "Iteration 282, loss = 0.03026023\n",
      "Iteration 283, loss = 0.02943578\n",
      "Iteration 284, loss = 0.02958231\n",
      "Iteration 285, loss = 0.02954133\n",
      "Iteration 286, loss = 0.02930027\n",
      "Iteration 287, loss = 0.02934549\n",
      "Iteration 288, loss = 0.02939214\n",
      "Iteration 289, loss = 0.02913603\n",
      "Iteration 290, loss = 0.02917257\n",
      "Iteration 291, loss = 0.02914482\n",
      "Iteration 292, loss = 0.02905933\n",
      "Iteration 293, loss = 0.02893092\n",
      "Iteration 294, loss = 0.02879237\n",
      "Iteration 295, loss = 0.02868987\n",
      "Iteration 296, loss = 0.02967337\n",
      "Iteration 297, loss = 0.02912857\n",
      "Iteration 298, loss = 0.02881322\n",
      "Iteration 299, loss = 0.02872503\n",
      "Iteration 300, loss = 0.02896743\n",
      "Iteration 301, loss = 0.02947796\n",
      "Iteration 302, loss = 0.02884641\n",
      "Iteration 303, loss = 0.02866818\n",
      "Iteration 304, loss = 0.02917372\n",
      "Iteration 305, loss = 0.02889316\n",
      "Iteration 306, loss = 0.02884436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy:  0.9066749072929543\n",
      "F1-Score:  0.9096349491322562\n",
      "Precision:  0.8909730363423212\n",
      "Recall:  0.9290953545232273\n",
      "AUC:  0.9064226772616137\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_mlp(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "clf=MLPClassifier(random_state=102, max_iter=3000, verbose=True).fit(X_Train_Transformed_FeatureMap, Y_Train_FeatureMap.ravel())\n",
    "y_pred=clf.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f1296",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35c08122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 80.98918083462134\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 82.38021638330757\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 82.68933539412674\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 81.45285935085008\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 82.61205564142195\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 80.98918083462134 %\n",
      "Fold 1: 82.38021638330757 %\n",
      "Fold 2: 82.68933539412674 %\n",
      "Fold 3: 81.45285935085008 %\n",
      "Fold 4: 82.61205564142195 %\n",
      "Average: 82.02472952086553 %\n",
      "Accuracy:  0.8065512978986403\n",
      "F1-Score:  0.8101879927228622\n",
      "Precision:  0.8038507821901324\n",
      "Recall:  0.8166259168704156\n",
      "AUC:  0.8064379584352079\n"
     ]
    }
   ],
   "source": [
    "random_forest=k_fold_cv_rforest(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "random_forest = RandomForestClassifier(n_estimators=100,criterion='gini',random_state=102)\n",
    "random_forest = random_forest.fit(X_Train_Transformed_FeatureMap, Y_Train_FeatureMap.ravel())\n",
    "y_pred=random_forest.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

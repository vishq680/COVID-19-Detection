{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import copy\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device=torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['Covid','No Covid']\n",
    "num_classes=2\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape all images to 64x64 and apply tensor transformation\n",
    "dataset = torchvision.datasets.ImageFolder(root=\"./Full\",transform=transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Resize([227,227])\n",
    "                                                            # transforms.Grayscale(num_output_channels=1)\n",
    "                                                            ]))\n",
    "# testset = torchvision.datasets.ImageFolder(root=\"./xray\",train=False,transform=transforms.Compose([transforms.Resize([300,305]),transforms.ToTensor()]))\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8088\n",
      "1617.5 404.5\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "trainset,testset=torch.utils.data.random_split(dataset,[round(0.8*len(dataset)),round(0.2*len(dataset))],generator=torch.Generator().manual_seed(42))\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True)\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=4,shuffle=False)\n",
    "print(len(trainset)/batch_size,len(testset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def get_first_FC_Layer(self,x):\n",
    "            x=self.representation_network(x).flatten(1)\n",
    "            x=self.classification_network[0](x)\n",
    "            return x;\n",
    "    def get_Representation_Net(self,x):\n",
    "            x=self.representation_network(x).flatten(1)\n",
    "            return  x;\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.representation_network=nn.Sequential(\n",
    "            nn.Conv2d(3,32,3), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "            nn.Conv2d(32,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "        )\n",
    "        self.classification_network=nn.Sequential(\n",
    "            nn.Linear(3136,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        x=self.representation_network(x)\n",
    "#         print(x.shape)\n",
    "        # flattening of the vector=> same dimension of first index(batch size) , everythign else is flattened(-1)\n",
    "        x=x.view(x.size(0),-1)\n",
    "#         print(x.shape)\n",
    "        x=self.classification_network(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CNN()\n",
    "net.load_state_dict(torch.load(\"./CNN6.pth\").state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataloader,model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total,correct=0,0\n",
    "        for data in dataloader:\n",
    "            inputs,labels=data\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "            outputs=model(inputs)\n",
    "    #         print(outputs)\n",
    "    #         print(outputs,labels)\n",
    "            m = nn.Sigmoid()\n",
    "            outputs=m(outputs)\n",
    "            pred=outputs>=0.5\n",
    "            pred=pred.flatten()\n",
    "            total+=labels.size(0)\n",
    "            # labels=torch.add(labels,-1)\n",
    "            # print(pred,labels)\n",
    "    #         print(list(map(lambda a: classes[a],pred)),list(map(lambda a: classes[a],labels)))\n",
    "            correct+=(pred==labels).sum().item()\n",
    "    print(correct,total)\n",
    "    model.train()\n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 512)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "Done with the batch: 405\n",
      "Done with the batch: 406\n",
      "Done with the batch: 407\n",
      "Done with the batch: 408\n",
      "Done with the batch: 409\n",
      "Done with the batch: 410\n",
      "Done with the batch: 411\n",
      "Done with the batch: 412\n",
      "Done with the batch: 413\n",
      "Done with the batch: 414\n",
      "Done with the batch: 415\n",
      "Done with the batch: 416\n",
      "Done with the batch: 417\n",
      "Done with the batch: 418\n",
      "Done with the batch: 419\n",
      "Done with the batch: 420\n",
      "Done with the batch: 421\n",
      "Done with the batch: 422\n",
      "Done with the batch: 423\n",
      "Done with the batch: 424\n",
      "Done with the batch: 425\n",
      "Done with the batch: 426\n",
      "Done with the batch: 427\n",
      "Done with the batch: 428\n",
      "Done with the batch: 429\n",
      "Done with the batch: 430\n",
      "Done with the batch: 431\n",
      "Done with the batch: 432\n",
      "Done with the batch: 433\n",
      "Done with the batch: 434\n",
      "Done with the batch: 435\n",
      "Done with the batch: 436\n",
      "Done with the batch: 437\n",
      "Done with the batch: 438\n",
      "Done with the batch: 439\n",
      "Done with the batch: 440\n",
      "Done with the batch: 441\n",
      "Done with the batch: 442\n",
      "Done with the batch: 443\n",
      "Done with the batch: 444\n",
      "Done with the batch: 445\n",
      "Done with the batch: 446\n",
      "Done with the batch: 447\n",
      "Done with the batch: 448\n",
      "Done with the batch: 449\n",
      "Done with the batch: 450\n",
      "Done with the batch: 451\n",
      "Done with the batch: 452\n",
      "Done with the batch: 453\n",
      "Done with the batch: 454\n",
      "Done with the batch: 455\n",
      "Done with the batch: 456\n",
      "Done with the batch: 457\n",
      "Done with the batch: 458\n",
      "Done with the batch: 459\n",
      "Done with the batch: 460\n",
      "Done with the batch: 461\n",
      "Done with the batch: 462\n",
      "Done with the batch: 463\n",
      "Done with the batch: 464\n",
      "Done with the batch: 465\n",
      "Done with the batch: 466\n",
      "Done with the batch: 467\n",
      "Done with the batch: 468\n",
      "Done with the batch: 469\n",
      "Done with the batch: 470\n",
      "Done with the batch: 471\n",
      "Done with the batch: 472\n",
      "Done with the batch: 473\n",
      "Done with the batch: 474\n",
      "Done with the batch: 475\n",
      "Done with the batch: 476\n",
      "Done with the batch: 477\n",
      "Done with the batch: 478\n",
      "Done with the batch: 479\n",
      "Done with the batch: 480\n",
      "Done with the batch: 481\n",
      "Done with the batch: 482\n",
      "Done with the batch: 483\n",
      "Done with the batch: 484\n",
      "Done with the batch: 485\n",
      "Done with the batch: 486\n",
      "Done with the batch: 487\n",
      "Done with the batch: 488\n",
      "Done with the batch: 489\n",
      "Done with the batch: 490\n",
      "Done with the batch: 491\n",
      "Done with the batch: 492\n",
      "Done with the batch: 493\n",
      "Done with the batch: 494\n",
      "Done with the batch: 495\n",
      "Done with the batch: 496\n",
      "Done with the batch: 497\n",
      "Done with the batch: 498\n",
      "Done with the batch: 499\n",
      "Done with the batch: 500\n",
      "Done with the batch: 501\n",
      "Done with the batch: 502\n",
      "Done with the batch: 503\n",
      "Done with the batch: 504\n",
      "Done with the batch: 505\n",
      "Done with the batch: 506\n",
      "Done with the batch: 507\n",
      "Done with the batch: 508\n",
      "Done with the batch: 509\n",
      "Done with the batch: 510\n",
      "Done with the batch: 511\n",
      "Done with the batch: 512\n",
      "Done with the batch: 513\n",
      "Done with the batch: 514\n",
      "Done with the batch: 515\n",
      "Done with the batch: 516\n",
      "Done with the batch: 517\n",
      "Done with the batch: 518\n",
      "Done with the batch: 519\n",
      "Done with the batch: 520\n",
      "Done with the batch: 521\n",
      "Done with the batch: 522\n",
      "Done with the batch: 523\n",
      "Done with the batch: 524\n",
      "Done with the batch: 525\n",
      "Done with the batch: 526\n",
      "Done with the batch: 527\n",
      "Done with the batch: 528\n",
      "Done with the batch: 529\n",
      "Done with the batch: 530\n",
      "Done with the batch: 531\n",
      "Done with the batch: 532\n",
      "Done with the batch: 533\n",
      "Done with the batch: 534\n",
      "Done with the batch: 535\n",
      "Done with the batch: 536\n",
      "Done with the batch: 537\n",
      "Done with the batch: 538\n",
      "Done with the batch: 539\n",
      "Done with the batch: 540\n",
      "Done with the batch: 541\n",
      "Done with the batch: 542\n",
      "Done with the batch: 543\n",
      "Done with the batch: 544\n",
      "Done with the batch: 545\n",
      "Done with the batch: 546\n",
      "Done with the batch: 547\n",
      "Done with the batch: 548\n",
      "Done with the batch: 549\n",
      "Done with the batch: 550\n",
      "Done with the batch: 551\n",
      "Done with the batch: 552\n",
      "Done with the batch: 553\n",
      "Done with the batch: 554\n",
      "Done with the batch: 555\n",
      "Done with the batch: 556\n",
      "Done with the batch: 557\n",
      "Done with the batch: 558\n",
      "Done with the batch: 559\n",
      "Done with the batch: 560\n",
      "Done with the batch: 561\n",
      "Done with the batch: 562\n",
      "Done with the batch: 563\n",
      "Done with the batch: 564\n",
      "Done with the batch: 565\n",
      "Done with the batch: 566\n",
      "Done with the batch: 567\n",
      "Done with the batch: 568\n",
      "Done with the batch: 569\n",
      "Done with the batch: 570\n",
      "Done with the batch: 571\n",
      "Done with the batch: 572\n",
      "Done with the batch: 573\n",
      "Done with the batch: 574\n",
      "Done with the batch: 575\n",
      "Done with the batch: 576\n",
      "Done with the batch: 577\n",
      "Done with the batch: 578\n",
      "Done with the batch: 579\n",
      "Done with the batch: 580\n",
      "Done with the batch: 581\n",
      "Done with the batch: 582\n",
      "Done with the batch: 583\n",
      "Done with the batch: 584\n",
      "Done with the batch: 585\n",
      "Done with the batch: 586\n",
      "Done with the batch: 587\n",
      "Done with the batch: 588\n",
      "Done with the batch: 589\n",
      "Done with the batch: 590\n",
      "Done with the batch: 591\n",
      "Done with the batch: 592\n",
      "Done with the batch: 593\n",
      "Done with the batch: 594\n",
      "Done with the batch: 595\n",
      "Done with the batch: 596\n",
      "Done with the batch: 597\n",
      "Done with the batch: 598\n",
      "Done with the batch: 599\n",
      "Done with the batch: 600\n",
      "Done with the batch: 601\n",
      "Done with the batch: 602\n",
      "Done with the batch: 603\n",
      "Done with the batch: 604\n",
      "Done with the batch: 605\n",
      "Done with the batch: 606\n",
      "Done with the batch: 607\n",
      "Done with the batch: 608\n",
      "Done with the batch: 609\n",
      "Done with the batch: 610\n",
      "Done with the batch: 611\n",
      "Done with the batch: 612\n",
      "Done with the batch: 613\n",
      "Done with the batch: 614\n",
      "Done with the batch: 615\n",
      "Done with the batch: 616\n",
      "Done with the batch: 617\n",
      "Done with the batch: 618\n",
      "Done with the batch: 619\n",
      "Done with the batch: 620\n",
      "Done with the batch: 621\n",
      "Done with the batch: 622\n",
      "Done with the batch: 623\n",
      "Done with the batch: 624\n",
      "Done with the batch: 625\n",
      "Done with the batch: 626\n",
      "Done with the batch: 627\n",
      "Done with the batch: 628\n",
      "Done with the batch: 629\n",
      "Done with the batch: 630\n",
      "Done with the batch: 631\n",
      "Done with the batch: 632\n",
      "Done with the batch: 633\n",
      "Done with the batch: 634\n",
      "Done with the batch: 635\n",
      "Done with the batch: 636\n",
      "Done with the batch: 637\n",
      "Done with the batch: 638\n",
      "Done with the batch: 639\n",
      "Done with the batch: 640\n",
      "Done with the batch: 641\n",
      "Done with the batch: 642\n",
      "Done with the batch: 643\n",
      "Done with the batch: 644\n",
      "Done with the batch: 645\n",
      "Done with the batch: 646\n",
      "Done with the batch: 647\n",
      "Done with the batch: 648\n",
      "Done with the batch: 649\n",
      "Done with the batch: 650\n",
      "Done with the batch: 651\n",
      "Done with the batch: 652\n",
      "Done with the batch: 653\n",
      "Done with the batch: 654\n",
      "Done with the batch: 655\n",
      "Done with the batch: 656\n",
      "Done with the batch: 657\n",
      "Done with the batch: 658\n",
      "Done with the batch: 659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 660\n",
      "Done with the batch: 661\n",
      "Done with the batch: 662\n",
      "Done with the batch: 663\n",
      "Done with the batch: 664\n",
      "Done with the batch: 665\n",
      "Done with the batch: 666\n",
      "Done with the batch: 667\n",
      "Done with the batch: 668\n",
      "Done with the batch: 669\n",
      "Done with the batch: 670\n",
      "Done with the batch: 671\n",
      "Done with the batch: 672\n",
      "Done with the batch: 673\n",
      "Done with the batch: 674\n",
      "Done with the batch: 675\n",
      "Done with the batch: 676\n",
      "Done with the batch: 677\n",
      "Done with the batch: 678\n",
      "Done with the batch: 679\n",
      "Done with the batch: 680\n",
      "Done with the batch: 681\n",
      "Done with the batch: 682\n",
      "Done with the batch: 683\n",
      "Done with the batch: 684\n",
      "Done with the batch: 685\n",
      "Done with the batch: 686\n",
      "Done with the batch: 687\n",
      "Done with the batch: 688\n",
      "Done with the batch: 689\n",
      "Done with the batch: 690\n",
      "Done with the batch: 691\n",
      "Done with the batch: 692\n",
      "Done with the batch: 693\n",
      "Done with the batch: 694\n",
      "Done with the batch: 695\n",
      "Done with the batch: 696\n",
      "Done with the batch: 697\n",
      "Done with the batch: 698\n",
      "Done with the batch: 699\n",
      "Done with the batch: 700\n",
      "Done with the batch: 701\n",
      "Done with the batch: 702\n",
      "Done with the batch: 703\n",
      "Done with the batch: 704\n",
      "Done with the batch: 705\n",
      "Done with the batch: 706\n",
      "Done with the batch: 707\n",
      "Done with the batch: 708\n",
      "Done with the batch: 709\n",
      "Done with the batch: 710\n",
      "Done with the batch: 711\n",
      "Done with the batch: 712\n",
      "Done with the batch: 713\n",
      "Done with the batch: 714\n",
      "Done with the batch: 715\n",
      "Done with the batch: 716\n",
      "Done with the batch: 717\n",
      "Done with the batch: 718\n",
      "Done with the batch: 719\n",
      "Done with the batch: 720\n",
      "Done with the batch: 721\n",
      "Done with the batch: 722\n",
      "Done with the batch: 723\n",
      "Done with the batch: 724\n",
      "Done with the batch: 725\n",
      "Done with the batch: 726\n",
      "Done with the batch: 727\n",
      "Done with the batch: 728\n",
      "Done with the batch: 729\n",
      "Done with the batch: 730\n",
      "Done with the batch: 731\n",
      "Done with the batch: 732\n",
      "Done with the batch: 733\n",
      "Done with the batch: 734\n",
      "Done with the batch: 735\n",
      "Done with the batch: 736\n",
      "Done with the batch: 737\n",
      "Done with the batch: 738\n",
      "Done with the batch: 739\n",
      "Done with the batch: 740\n",
      "Done with the batch: 741\n",
      "Done with the batch: 742\n",
      "Done with the batch: 743\n",
      "Done with the batch: 744\n",
      "Done with the batch: 745\n",
      "Done with the batch: 746\n",
      "Done with the batch: 747\n",
      "Done with the batch: 748\n",
      "Done with the batch: 749\n",
      "Done with the batch: 750\n",
      "Done with the batch: 751\n",
      "Done with the batch: 752\n",
      "Done with the batch: 753\n",
      "Done with the batch: 754\n",
      "Done with the batch: 755\n",
      "Done with the batch: 756\n",
      "Done with the batch: 757\n",
      "Done with the batch: 758\n",
      "Done with the batch: 759\n",
      "Done with the batch: 760\n",
      "Done with the batch: 761\n",
      "Done with the batch: 762\n",
      "Done with the batch: 763\n",
      "Done with the batch: 764\n",
      "Done with the batch: 765\n",
      "Done with the batch: 766\n",
      "Done with the batch: 767\n",
      "Done with the batch: 768\n",
      "Done with the batch: 769\n",
      "Done with the batch: 770\n",
      "Done with the batch: 771\n",
      "Done with the batch: 772\n",
      "Done with the batch: 773\n",
      "Done with the batch: 774\n",
      "Done with the batch: 775\n",
      "Done with the batch: 776\n",
      "Done with the batch: 777\n",
      "Done with the batch: 778\n",
      "Done with the batch: 779\n",
      "Done with the batch: 780\n",
      "Done with the batch: 781\n",
      "Done with the batch: 782\n",
      "Done with the batch: 783\n",
      "Done with the batch: 784\n",
      "Done with the batch: 785\n",
      "Done with the batch: 786\n",
      "Done with the batch: 787\n",
      "Done with the batch: 788\n",
      "Done with the batch: 789\n",
      "Done with the batch: 790\n",
      "Done with the batch: 791\n",
      "Done with the batch: 792\n",
      "Done with the batch: 793\n",
      "Done with the batch: 794\n",
      "Done with the batch: 795\n",
      "Done with the batch: 796\n",
      "Done with the batch: 797\n",
      "Done with the batch: 798\n",
      "Done with the batch: 799\n",
      "Done with the batch: 800\n",
      "Done with the batch: 801\n",
      "Done with the batch: 802\n",
      "Done with the batch: 803\n",
      "Done with the batch: 804\n",
      "Done with the batch: 805\n",
      "Done with the batch: 806\n",
      "Done with the batch: 807\n",
      "Done with the batch: 808\n",
      "Done with the batch: 809\n",
      "Done with the batch: 810\n",
      "Done with the batch: 811\n",
      "Done with the batch: 812\n",
      "Done with the batch: 813\n",
      "Done with the batch: 814\n",
      "Done with the batch: 815\n",
      "Done with the batch: 816\n",
      "Done with the batch: 817\n",
      "Done with the batch: 818\n",
      "Done with the batch: 819\n",
      "Done with the batch: 820\n",
      "Done with the batch: 821\n",
      "Done with the batch: 822\n",
      "Done with the batch: 823\n",
      "Done with the batch: 824\n",
      "Done with the batch: 825\n",
      "Done with the batch: 826\n",
      "Done with the batch: 827\n",
      "Done with the batch: 828\n",
      "Done with the batch: 829\n",
      "Done with the batch: 830\n",
      "Done with the batch: 831\n",
      "Done with the batch: 832\n",
      "Done with the batch: 833\n",
      "Done with the batch: 834\n",
      "Done with the batch: 835\n",
      "Done with the batch: 836\n",
      "Done with the batch: 837\n",
      "Done with the batch: 838\n",
      "Done with the batch: 839\n",
      "Done with the batch: 840\n",
      "Done with the batch: 841\n",
      "Done with the batch: 842\n",
      "Done with the batch: 843\n",
      "Done with the batch: 844\n",
      "Done with the batch: 845\n",
      "Done with the batch: 846\n",
      "Done with the batch: 847\n",
      "Done with the batch: 848\n",
      "Done with the batch: 849\n",
      "Done with the batch: 850\n",
      "Done with the batch: 851\n",
      "Done with the batch: 852\n",
      "Done with the batch: 853\n",
      "Done with the batch: 854\n",
      "Done with the batch: 855\n",
      "Done with the batch: 856\n",
      "Done with the batch: 857\n",
      "Done with the batch: 858\n",
      "Done with the batch: 859\n",
      "Done with the batch: 860\n",
      "Done with the batch: 861\n",
      "Done with the batch: 862\n",
      "Done with the batch: 863\n",
      "Done with the batch: 864\n",
      "Done with the batch: 865\n",
      "Done with the batch: 866\n",
      "Done with the batch: 867\n",
      "Done with the batch: 868\n",
      "Done with the batch: 869\n",
      "Done with the batch: 870\n",
      "Done with the batch: 871\n",
      "Done with the batch: 872\n",
      "Done with the batch: 873\n",
      "Done with the batch: 874\n",
      "Done with the batch: 875\n",
      "Done with the batch: 876\n",
      "Done with the batch: 877\n",
      "Done with the batch: 878\n",
      "Done with the batch: 879\n",
      "Done with the batch: 880\n",
      "Done with the batch: 881\n",
      "Done with the batch: 882\n",
      "Done with the batch: 883\n",
      "Done with the batch: 884\n",
      "Done with the batch: 885\n",
      "Done with the batch: 886\n",
      "Done with the batch: 887\n",
      "Done with the batch: 888\n",
      "Done with the batch: 889\n",
      "Done with the batch: 890\n",
      "Done with the batch: 891\n",
      "Done with the batch: 892\n",
      "Done with the batch: 893\n",
      "Done with the batch: 894\n",
      "Done with the batch: 895\n",
      "Done with the batch: 896\n",
      "Done with the batch: 897\n",
      "Done with the batch: 898\n",
      "Done with the batch: 899\n",
      "Done with the batch: 900\n",
      "Done with the batch: 901\n",
      "Done with the batch: 902\n",
      "Done with the batch: 903\n",
      "Done with the batch: 904\n",
      "Done with the batch: 905\n",
      "Done with the batch: 906\n",
      "Done with the batch: 907\n",
      "Done with the batch: 908\n",
      "Done with the batch: 909\n",
      "Done with the batch: 910\n",
      "Done with the batch: 911\n",
      "Done with the batch: 912\n",
      "Done with the batch: 913\n",
      "Done with the batch: 914\n",
      "Done with the batch: 915\n",
      "Done with the batch: 916\n",
      "Done with the batch: 917\n",
      "Done with the batch: 918\n",
      "Done with the batch: 919\n",
      "Done with the batch: 920\n",
      "Done with the batch: 921\n",
      "Done with the batch: 922\n",
      "Done with the batch: 923\n",
      "Done with the batch: 924\n",
      "Done with the batch: 925\n",
      "Done with the batch: 926\n",
      "Done with the batch: 927\n",
      "Done with the batch: 928\n",
      "Done with the batch: 929\n",
      "Done with the batch: 930\n",
      "Done with the batch: 931\n",
      "Done with the batch: 932\n",
      "Done with the batch: 933\n",
      "Done with the batch: 934\n",
      "Done with the batch: 935\n",
      "Done with the batch: 936\n",
      "Done with the batch: 937\n",
      "Done with the batch: 938\n",
      "Done with the batch: 939\n",
      "Done with the batch: 940\n",
      "Done with the batch: 941\n",
      "Done with the batch: 942\n",
      "Done with the batch: 943\n",
      "Done with the batch: 944\n",
      "Done with the batch: 945\n",
      "Done with the batch: 946\n",
      "Done with the batch: 947\n",
      "Done with the batch: 948\n",
      "Done with the batch: 949\n",
      "Done with the batch: 950\n",
      "Done with the batch: 951\n",
      "Done with the batch: 952\n",
      "Done with the batch: 953\n",
      "Done with the batch: 954\n",
      "Done with the batch: 955\n",
      "Done with the batch: 956\n",
      "Done with the batch: 957\n",
      "Done with the batch: 958\n",
      "Done with the batch: 959\n",
      "Done with the batch: 960\n",
      "Done with the batch: 961\n",
      "Done with the batch: 962\n",
      "Done with the batch: 963\n",
      "Done with the batch: 964\n",
      "Done with the batch: 965\n",
      "Done with the batch: 966\n",
      "Done with the batch: 967\n",
      "Done with the batch: 968\n",
      "Done with the batch: 969\n",
      "Done with the batch: 970\n",
      "Done with the batch: 971\n",
      "Done with the batch: 972\n",
      "Done with the batch: 973\n",
      "Done with the batch: 974\n",
      "Done with the batch: 975\n",
      "Done with the batch: 976\n",
      "Done with the batch: 977\n",
      "Done with the batch: 978\n",
      "Done with the batch: 979\n",
      "Done with the batch: 980\n",
      "Done with the batch: 981\n",
      "Done with the batch: 982\n",
      "Done with the batch: 983\n",
      "Done with the batch: 984\n",
      "Done with the batch: 985\n",
      "Done with the batch: 986\n",
      "Done with the batch: 987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 988\n",
      "Done with the batch: 989\n",
      "Done with the batch: 990\n",
      "Done with the batch: 991\n",
      "Done with the batch: 992\n",
      "Done with the batch: 993\n",
      "Done with the batch: 994\n",
      "Done with the batch: 995\n",
      "Done with the batch: 996\n",
      "Done with the batch: 997\n",
      "Done with the batch: 998\n",
      "Done with the batch: 999\n",
      "Done with the batch: 1000\n",
      "Done with the batch: 1001\n",
      "Done with the batch: 1002\n",
      "Done with the batch: 1003\n",
      "Done with the batch: 1004\n",
      "Done with the batch: 1005\n",
      "Done with the batch: 1006\n",
      "Done with the batch: 1007\n",
      "Done with the batch: 1008\n",
      "Done with the batch: 1009\n",
      "Done with the batch: 1010\n",
      "Done with the batch: 1011\n",
      "Done with the batch: 1012\n",
      "Done with the batch: 1013\n",
      "Done with the batch: 1014\n",
      "Done with the batch: 1015\n",
      "Done with the batch: 1016\n",
      "Done with the batch: 1017\n",
      "Done with the batch: 1018\n",
      "Done with the batch: 1019\n",
      "Done with the batch: 1020\n",
      "Done with the batch: 1021\n",
      "Done with the batch: 1022\n",
      "Done with the batch: 1023\n",
      "Done with the batch: 1024\n",
      "Done with the batch: 1025\n",
      "Done with the batch: 1026\n",
      "Done with the batch: 1027\n",
      "Done with the batch: 1028\n",
      "Done with the batch: 1029\n",
      "Done with the batch: 1030\n",
      "Done with the batch: 1031\n",
      "Done with the batch: 1032\n",
      "Done with the batch: 1033\n",
      "Done with the batch: 1034\n",
      "Done with the batch: 1035\n",
      "Done with the batch: 1036\n",
      "Done with the batch: 1037\n",
      "Done with the batch: 1038\n",
      "Done with the batch: 1039\n",
      "Done with the batch: 1040\n",
      "Done with the batch: 1041\n",
      "Done with the batch: 1042\n",
      "Done with the batch: 1043\n",
      "Done with the batch: 1044\n",
      "Done with the batch: 1045\n",
      "Done with the batch: 1046\n",
      "Done with the batch: 1047\n",
      "Done with the batch: 1048\n",
      "Done with the batch: 1049\n",
      "Done with the batch: 1050\n",
      "Done with the batch: 1051\n",
      "Done with the batch: 1052\n",
      "Done with the batch: 1053\n",
      "Done with the batch: 1054\n",
      "Done with the batch: 1055\n",
      "Done with the batch: 1056\n",
      "Done with the batch: 1057\n",
      "Done with the batch: 1058\n",
      "Done with the batch: 1059\n",
      "Done with the batch: 1060\n",
      "Done with the batch: 1061\n",
      "Done with the batch: 1062\n",
      "Done with the batch: 1063\n",
      "Done with the batch: 1064\n",
      "Done with the batch: 1065\n",
      "Done with the batch: 1066\n",
      "Done with the batch: 1067\n",
      "Done with the batch: 1068\n",
      "Done with the batch: 1069\n",
      "Done with the batch: 1070\n",
      "Done with the batch: 1071\n",
      "Done with the batch: 1072\n",
      "Done with the batch: 1073\n",
      "Done with the batch: 1074\n",
      "Done with the batch: 1075\n",
      "Done with the batch: 1076\n",
      "Done with the batch: 1077\n",
      "Done with the batch: 1078\n",
      "Done with the batch: 1079\n",
      "Done with the batch: 1080\n",
      "Done with the batch: 1081\n",
      "Done with the batch: 1082\n",
      "Done with the batch: 1083\n",
      "Done with the batch: 1084\n",
      "Done with the batch: 1085\n",
      "Done with the batch: 1086\n",
      "Done with the batch: 1087\n",
      "Done with the batch: 1088\n",
      "Done with the batch: 1089\n",
      "Done with the batch: 1090\n",
      "Done with the batch: 1091\n",
      "Done with the batch: 1092\n",
      "Done with the batch: 1093\n",
      "Done with the batch: 1094\n",
      "Done with the batch: 1095\n",
      "Done with the batch: 1096\n",
      "Done with the batch: 1097\n",
      "Done with the batch: 1098\n",
      "Done with the batch: 1099\n",
      "Done with the batch: 1100\n",
      "Done with the batch: 1101\n",
      "Done with the batch: 1102\n",
      "Done with the batch: 1103\n",
      "Done with the batch: 1104\n",
      "Done with the batch: 1105\n",
      "Done with the batch: 1106\n",
      "Done with the batch: 1107\n",
      "Done with the batch: 1108\n",
      "Done with the batch: 1109\n",
      "Done with the batch: 1110\n",
      "Done with the batch: 1111\n",
      "Done with the batch: 1112\n",
      "Done with the batch: 1113\n",
      "Done with the batch: 1114\n",
      "Done with the batch: 1115\n",
      "Done with the batch: 1116\n",
      "Done with the batch: 1117\n",
      "Done with the batch: 1118\n",
      "Done with the batch: 1119\n",
      "Done with the batch: 1120\n",
      "Done with the batch: 1121\n",
      "Done with the batch: 1122\n",
      "Done with the batch: 1123\n",
      "Done with the batch: 1124\n",
      "Done with the batch: 1125\n",
      "Done with the batch: 1126\n",
      "Done with the batch: 1127\n",
      "Done with the batch: 1128\n",
      "Done with the batch: 1129\n",
      "Done with the batch: 1130\n",
      "Done with the batch: 1131\n",
      "Done with the batch: 1132\n",
      "Done with the batch: 1133\n",
      "Done with the batch: 1134\n",
      "Done with the batch: 1135\n",
      "Done with the batch: 1136\n",
      "Done with the batch: 1137\n",
      "Done with the batch: 1138\n",
      "Done with the batch: 1139\n",
      "Done with the batch: 1140\n",
      "Done with the batch: 1141\n",
      "Done with the batch: 1142\n",
      "Done with the batch: 1143\n",
      "Done with the batch: 1144\n",
      "Done with the batch: 1145\n",
      "Done with the batch: 1146\n",
      "Done with the batch: 1147\n",
      "Done with the batch: 1148\n",
      "Done with the batch: 1149\n",
      "Done with the batch: 1150\n",
      "Done with the batch: 1151\n",
      "Done with the batch: 1152\n",
      "Done with the batch: 1153\n",
      "Done with the batch: 1154\n",
      "Done with the batch: 1155\n",
      "Done with the batch: 1156\n",
      "Done with the batch: 1157\n",
      "Done with the batch: 1158\n",
      "Done with the batch: 1159\n",
      "Done with the batch: 1160\n",
      "Done with the batch: 1161\n",
      "Done with the batch: 1162\n",
      "Done with the batch: 1163\n",
      "Done with the batch: 1164\n",
      "Done with the batch: 1165\n",
      "Done with the batch: 1166\n",
      "Done with the batch: 1167\n",
      "Done with the batch: 1168\n",
      "Done with the batch: 1169\n",
      "Done with the batch: 1170\n",
      "Done with the batch: 1171\n",
      "Done with the batch: 1172\n",
      "Done with the batch: 1173\n",
      "Done with the batch: 1174\n",
      "Done with the batch: 1175\n",
      "Done with the batch: 1176\n",
      "Done with the batch: 1177\n",
      "Done with the batch: 1178\n",
      "Done with the batch: 1179\n",
      "Done with the batch: 1180\n",
      "Done with the batch: 1181\n",
      "Done with the batch: 1182\n",
      "Done with the batch: 1183\n",
      "Done with the batch: 1184\n",
      "Done with the batch: 1185\n",
      "Done with the batch: 1186\n",
      "Done with the batch: 1187\n",
      "Done with the batch: 1188\n",
      "Done with the batch: 1189\n",
      "Done with the batch: 1190\n",
      "Done with the batch: 1191\n",
      "Done with the batch: 1192\n",
      "Done with the batch: 1193\n",
      "Done with the batch: 1194\n",
      "Done with the batch: 1195\n",
      "Done with the batch: 1196\n",
      "Done with the batch: 1197\n",
      "Done with the batch: 1198\n",
      "Done with the batch: 1199\n",
      "Done with the batch: 1200\n",
      "Done with the batch: 1201\n",
      "Done with the batch: 1202\n",
      "Done with the batch: 1203\n",
      "Done with the batch: 1204\n",
      "Done with the batch: 1205\n",
      "Done with the batch: 1206\n",
      "Done with the batch: 1207\n",
      "Done with the batch: 1208\n",
      "Done with the batch: 1209\n",
      "Done with the batch: 1210\n",
      "Done with the batch: 1211\n",
      "Done with the batch: 1212\n",
      "Done with the batch: 1213\n",
      "Done with the batch: 1214\n",
      "Done with the batch: 1215\n",
      "Done with the batch: 1216\n",
      "Done with the batch: 1217\n",
      "Done with the batch: 1218\n",
      "Done with the batch: 1219\n",
      "Done with the batch: 1220\n",
      "Done with the batch: 1221\n",
      "Done with the batch: 1222\n",
      "Done with the batch: 1223\n",
      "Done with the batch: 1224\n",
      "Done with the batch: 1225\n",
      "Done with the batch: 1226\n",
      "Done with the batch: 1227\n",
      "Done with the batch: 1228\n",
      "Done with the batch: 1229\n",
      "Done with the batch: 1230\n",
      "Done with the batch: 1231\n",
      "Done with the batch: 1232\n",
      "Done with the batch: 1233\n",
      "Done with the batch: 1234\n",
      "Done with the batch: 1235\n",
      "Done with the batch: 1236\n",
      "Done with the batch: 1237\n",
      "Done with the batch: 1238\n",
      "Done with the batch: 1239\n",
      "Done with the batch: 1240\n",
      "Done with the batch: 1241\n",
      "Done with the batch: 1242\n",
      "Done with the batch: 1243\n",
      "Done with the batch: 1244\n",
      "Done with the batch: 1245\n",
      "Done with the batch: 1246\n",
      "Done with the batch: 1247\n",
      "Done with the batch: 1248\n",
      "Done with the batch: 1249\n",
      "Done with the batch: 1250\n",
      "Done with the batch: 1251\n",
      "Done with the batch: 1252\n",
      "Done with the batch: 1253\n",
      "Done with the batch: 1254\n",
      "Done with the batch: 1255\n",
      "Done with the batch: 1256\n",
      "Done with the batch: 1257\n",
      "Done with the batch: 1258\n",
      "Done with the batch: 1259\n",
      "Done with the batch: 1260\n",
      "Done with the batch: 1261\n",
      "Done with the batch: 1262\n",
      "Done with the batch: 1263\n",
      "Done with the batch: 1264\n",
      "Done with the batch: 1265\n",
      "Done with the batch: 1266\n",
      "Done with the batch: 1267\n",
      "Done with the batch: 1268\n",
      "Done with the batch: 1269\n",
      "Done with the batch: 1270\n",
      "Done with the batch: 1271\n",
      "Done with the batch: 1272\n",
      "Done with the batch: 1273\n",
      "Done with the batch: 1274\n",
      "Done with the batch: 1275\n",
      "Done with the batch: 1276\n",
      "Done with the batch: 1277\n",
      "Done with the batch: 1278\n",
      "Done with the batch: 1279\n",
      "Done with the batch: 1280\n",
      "Done with the batch: 1281\n",
      "Done with the batch: 1282\n",
      "Done with the batch: 1283\n",
      "Done with the batch: 1284\n",
      "Done with the batch: 1285\n",
      "Done with the batch: 1286\n",
      "Done with the batch: 1287\n",
      "Done with the batch: 1288\n",
      "Done with the batch: 1289\n",
      "Done with the batch: 1290\n",
      "Done with the batch: 1291\n",
      "Done with the batch: 1292\n",
      "Done with the batch: 1293\n",
      "Done with the batch: 1294\n",
      "Done with the batch: 1295\n",
      "Done with the batch: 1296\n",
      "Done with the batch: 1297\n",
      "Done with the batch: 1298\n",
      "Done with the batch: 1299\n",
      "Done with the batch: 1300\n",
      "Done with the batch: 1301\n",
      "Done with the batch: 1302\n",
      "Done with the batch: 1303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 1304\n",
      "Done with the batch: 1305\n",
      "Done with the batch: 1306\n",
      "Done with the batch: 1307\n",
      "Done with the batch: 1308\n",
      "Done with the batch: 1309\n",
      "Done with the batch: 1310\n",
      "Done with the batch: 1311\n",
      "Done with the batch: 1312\n",
      "Done with the batch: 1313\n",
      "Done with the batch: 1314\n",
      "Done with the batch: 1315\n",
      "Done with the batch: 1316\n",
      "Done with the batch: 1317\n",
      "Done with the batch: 1318\n",
      "Done with the batch: 1319\n",
      "Done with the batch: 1320\n",
      "Done with the batch: 1321\n",
      "Done with the batch: 1322\n",
      "Done with the batch: 1323\n",
      "Done with the batch: 1324\n",
      "Done with the batch: 1325\n",
      "Done with the batch: 1326\n",
      "Done with the batch: 1327\n",
      "Done with the batch: 1328\n",
      "Done with the batch: 1329\n",
      "Done with the batch: 1330\n",
      "Done with the batch: 1331\n",
      "Done with the batch: 1332\n",
      "Done with the batch: 1333\n",
      "Done with the batch: 1334\n",
      "Done with the batch: 1335\n",
      "Done with the batch: 1336\n",
      "Done with the batch: 1337\n",
      "Done with the batch: 1338\n",
      "Done with the batch: 1339\n",
      "Done with the batch: 1340\n",
      "Done with the batch: 1341\n",
      "Done with the batch: 1342\n",
      "Done with the batch: 1343\n",
      "Done with the batch: 1344\n",
      "Done with the batch: 1345\n",
      "Done with the batch: 1346\n",
      "Done with the batch: 1347\n",
      "Done with the batch: 1348\n",
      "Done with the batch: 1349\n",
      "Done with the batch: 1350\n",
      "Done with the batch: 1351\n",
      "Done with the batch: 1352\n",
      "Done with the batch: 1353\n",
      "Done with the batch: 1354\n",
      "Done with the batch: 1355\n",
      "Done with the batch: 1356\n",
      "Done with the batch: 1357\n",
      "Done with the batch: 1358\n",
      "Done with the batch: 1359\n",
      "Done with the batch: 1360\n",
      "Done with the batch: 1361\n",
      "Done with the batch: 1362\n",
      "Done with the batch: 1363\n",
      "Done with the batch: 1364\n",
      "Done with the batch: 1365\n",
      "Done with the batch: 1366\n",
      "Done with the batch: 1367\n",
      "Done with the batch: 1368\n",
      "Done with the batch: 1369\n",
      "Done with the batch: 1370\n",
      "Done with the batch: 1371\n",
      "Done with the batch: 1372\n",
      "Done with the batch: 1373\n",
      "Done with the batch: 1374\n",
      "Done with the batch: 1375\n",
      "Done with the batch: 1376\n",
      "Done with the batch: 1377\n",
      "Done with the batch: 1378\n",
      "Done with the batch: 1379\n",
      "Done with the batch: 1380\n",
      "Done with the batch: 1381\n",
      "Done with the batch: 1382\n",
      "Done with the batch: 1383\n",
      "Done with the batch: 1384\n",
      "Done with the batch: 1385\n",
      "Done with the batch: 1386\n",
      "Done with the batch: 1387\n",
      "Done with the batch: 1388\n",
      "Done with the batch: 1389\n",
      "Done with the batch: 1390\n",
      "Done with the batch: 1391\n",
      "Done with the batch: 1392\n",
      "Done with the batch: 1393\n",
      "Done with the batch: 1394\n",
      "Done with the batch: 1395\n",
      "Done with the batch: 1396\n",
      "Done with the batch: 1397\n",
      "Done with the batch: 1398\n",
      "Done with the batch: 1399\n",
      "Done with the batch: 1400\n",
      "Done with the batch: 1401\n",
      "Done with the batch: 1402\n",
      "Done with the batch: 1403\n",
      "Done with the batch: 1404\n",
      "Done with the batch: 1405\n",
      "Done with the batch: 1406\n",
      "Done with the batch: 1407\n",
      "Done with the batch: 1408\n",
      "Done with the batch: 1409\n",
      "Done with the batch: 1410\n",
      "Done with the batch: 1411\n",
      "Done with the batch: 1412\n",
      "Done with the batch: 1413\n",
      "Done with the batch: 1414\n",
      "Done with the batch: 1415\n",
      "Done with the batch: 1416\n",
      "Done with the batch: 1417\n",
      "Done with the batch: 1418\n",
      "Done with the batch: 1419\n",
      "Done with the batch: 1420\n",
      "Done with the batch: 1421\n",
      "Done with the batch: 1422\n",
      "Done with the batch: 1423\n",
      "Done with the batch: 1424\n",
      "Done with the batch: 1425\n",
      "Done with the batch: 1426\n",
      "Done with the batch: 1427\n",
      "Done with the batch: 1428\n",
      "Done with the batch: 1429\n",
      "Done with the batch: 1430\n",
      "Done with the batch: 1431\n",
      "Done with the batch: 1432\n",
      "Done with the batch: 1433\n",
      "Done with the batch: 1434\n",
      "Done with the batch: 1435\n",
      "Done with the batch: 1436\n",
      "Done with the batch: 1437\n",
      "Done with the batch: 1438\n",
      "Done with the batch: 1439\n",
      "Done with the batch: 1440\n",
      "Done with the batch: 1441\n",
      "Done with the batch: 1442\n",
      "Done with the batch: 1443\n",
      "Done with the batch: 1444\n",
      "Done with the batch: 1445\n",
      "Done with the batch: 1446\n",
      "Done with the batch: 1447\n",
      "Done with the batch: 1448\n",
      "Done with the batch: 1449\n",
      "Done with the batch: 1450\n",
      "Done with the batch: 1451\n",
      "Done with the batch: 1452\n",
      "Done with the batch: 1453\n",
      "Done with the batch: 1454\n",
      "Done with the batch: 1455\n",
      "Done with the batch: 1456\n",
      "Done with the batch: 1457\n",
      "Done with the batch: 1458\n",
      "Done with the batch: 1459\n",
      "Done with the batch: 1460\n",
      "Done with the batch: 1461\n",
      "Done with the batch: 1462\n",
      "Done with the batch: 1463\n",
      "Done with the batch: 1464\n",
      "Done with the batch: 1465\n",
      "Done with the batch: 1466\n",
      "Done with the batch: 1467\n",
      "Done with the batch: 1468\n",
      "Done with the batch: 1469\n",
      "Done with the batch: 1470\n",
      "Done with the batch: 1471\n",
      "Done with the batch: 1472\n",
      "Done with the batch: 1473\n",
      "Done with the batch: 1474\n",
      "Done with the batch: 1475\n",
      "Done with the batch: 1476\n",
      "Done with the batch: 1477\n",
      "Done with the batch: 1478\n",
      "Done with the batch: 1479\n",
      "Done with the batch: 1480\n",
      "Done with the batch: 1481\n",
      "Done with the batch: 1482\n",
      "Done with the batch: 1483\n",
      "Done with the batch: 1484\n",
      "Done with the batch: 1485\n",
      "Done with the batch: 1486\n",
      "Done with the batch: 1487\n",
      "Done with the batch: 1488\n",
      "Done with the batch: 1489\n",
      "Done with the batch: 1490\n",
      "Done with the batch: 1491\n",
      "Done with the batch: 1492\n",
      "Done with the batch: 1493\n",
      "Done with the batch: 1494\n",
      "Done with the batch: 1495\n",
      "Done with the batch: 1496\n",
      "Done with the batch: 1497\n",
      "Done with the batch: 1498\n",
      "Done with the batch: 1499\n",
      "Done with the batch: 1500\n",
      "Done with the batch: 1501\n",
      "Done with the batch: 1502\n",
      "Done with the batch: 1503\n",
      "Done with the batch: 1504\n",
      "Done with the batch: 1505\n",
      "Done with the batch: 1506\n",
      "Done with the batch: 1507\n",
      "Done with the batch: 1508\n",
      "Done with the batch: 1509\n",
      "Done with the batch: 1510\n",
      "Done with the batch: 1511\n",
      "Done with the batch: 1512\n",
      "Done with the batch: 1513\n",
      "Done with the batch: 1514\n",
      "Done with the batch: 1515\n",
      "Done with the batch: 1516\n",
      "Done with the batch: 1517\n",
      "Done with the batch: 1518\n",
      "Done with the batch: 1519\n",
      "Done with the batch: 1520\n",
      "Done with the batch: 1521\n",
      "Done with the batch: 1522\n",
      "Done with the batch: 1523\n",
      "Done with the batch: 1524\n",
      "Done with the batch: 1525\n",
      "Done with the batch: 1526\n",
      "Done with the batch: 1527\n",
      "Done with the batch: 1528\n",
      "Done with the batch: 1529\n",
      "Done with the batch: 1530\n",
      "Done with the batch: 1531\n",
      "Done with the batch: 1532\n",
      "Done with the batch: 1533\n",
      "Done with the batch: 1534\n",
      "Done with the batch: 1535\n",
      "Done with the batch: 1536\n",
      "Done with the batch: 1537\n",
      "Done with the batch: 1538\n",
      "Done with the batch: 1539\n",
      "Done with the batch: 1540\n",
      "Done with the batch: 1541\n",
      "Done with the batch: 1542\n",
      "Done with the batch: 1543\n",
      "Done with the batch: 1544\n",
      "Done with the batch: 1545\n",
      "Done with the batch: 1546\n",
      "Done with the batch: 1547\n",
      "Done with the batch: 1548\n",
      "Done with the batch: 1549\n",
      "Done with the batch: 1550\n",
      "Done with the batch: 1551\n",
      "Done with the batch: 1552\n",
      "Done with the batch: 1553\n",
      "Done with the batch: 1554\n",
      "Done with the batch: 1555\n",
      "Done with the batch: 1556\n",
      "Done with the batch: 1557\n",
      "Done with the batch: 1558\n",
      "Done with the batch: 1559\n",
      "Done with the batch: 1560\n",
      "Done with the batch: 1561\n",
      "Done with the batch: 1562\n",
      "Done with the batch: 1563\n",
      "Done with the batch: 1564\n",
      "Done with the batch: 1565\n",
      "Done with the batch: 1566\n",
      "Done with the batch: 1567\n",
      "Done with the batch: 1568\n",
      "Done with the batch: 1569\n",
      "Done with the batch: 1570\n",
      "Done with the batch: 1571\n",
      "Done with the batch: 1572\n",
      "Done with the batch: 1573\n",
      "Done with the batch: 1574\n",
      "Done with the batch: 1575\n",
      "Done with the batch: 1576\n",
      "Done with the batch: 1577\n",
      "Done with the batch: 1578\n",
      "Done with the batch: 1579\n",
      "Done with the batch: 1580\n",
      "Done with the batch: 1581\n",
      "Done with the batch: 1582\n",
      "Done with the batch: 1583\n",
      "Done with the batch: 1584\n",
      "Done with the batch: 1585\n",
      "Done with the batch: 1586\n",
      "Done with the batch: 1587\n",
      "Done with the batch: 1588\n",
      "Done with the batch: 1589\n",
      "Done with the batch: 1590\n",
      "Done with the batch: 1591\n",
      "Done with the batch: 1592\n",
      "Done with the batch: 1593\n",
      "Done with the batch: 1594\n",
      "Done with the batch: 1595\n",
      "Done with the batch: 1596\n",
      "Done with the batch: 1597\n",
      "Done with the batch: 1598\n",
      "Done with the batch: 1599\n",
      "Done with the batch: 1600\n",
      "Done with the batch: 1601\n",
      "Done with the batch: 1602\n",
      "Done with the batch: 1603\n",
      "Done with the batch: 1604\n",
      "Done with the batch: 1605\n",
      "Done with the batch: 1606\n",
      "Done with the batch: 1607\n",
      "Done with the batch: 1608\n",
      "Done with the batch: 1609\n",
      "Done with the batch: 1610\n",
      "Done with the batch: 1611\n",
      "Done with the batch: 1612\n",
      "Done with the batch: 1613\n",
      "Done with the batch: 1614\n",
      "Done with the batch: 1615\n",
      "Done with the batch: 1616\n",
      "Done with the batch: 1617\n",
      "(6470, 512) (6470,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"7e80a13f-3059-45ff-8956-d56517092bff\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"7e80a13f-3059-45ff-8956-d56517092bff\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "\n",
    "X_Train=np.empty((0,512))\n",
    "Y_Train=np.empty((0,batch_size))\n",
    "print(X_Train.shape)\n",
    "for i,data in enumerate(trainloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    FCLayer=net.get_first_FC_Layer(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Train=np.append(X_Train,FCLayer,axis=0)\n",
    "    Y_Train=np.append(Y_Train,labels.numpy())\n",
    "print(X_Train.shape,Y_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 512)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "(1618, 512) (1618,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"658525ca-c7b5-4da3-9214-76ade2c74b62\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"658525ca-c7b5-4da3-9214-76ade2c74b62\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "X_Test=np.empty((0,512))\n",
    "Y_Test=np.empty((0,batch_size))\n",
    "print(X_Test.shape)\n",
    "for i,data in enumerate(testloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    FCLayer=net.get_first_FC_Layer(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Test=np.append(X_Test,FCLayer,axis=0)\n",
    "    Y_Test=np.append(Y_Test,labels.numpy())\n",
    "print(X_Test.shape,Y_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3136)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "Done with the batch: 405\n",
      "Done with the batch: 406\n",
      "Done with the batch: 407\n",
      "Done with the batch: 408\n",
      "Done with the batch: 409\n",
      "Done with the batch: 410\n",
      "Done with the batch: 411\n",
      "Done with the batch: 412\n",
      "Done with the batch: 413\n",
      "Done with the batch: 414\n",
      "Done with the batch: 415\n",
      "Done with the batch: 416\n",
      "Done with the batch: 417\n",
      "Done with the batch: 418\n",
      "Done with the batch: 419\n",
      "Done with the batch: 420\n",
      "Done with the batch: 421\n",
      "Done with the batch: 422\n",
      "Done with the batch: 423\n",
      "Done with the batch: 424\n",
      "Done with the batch: 425\n",
      "Done with the batch: 426\n",
      "Done with the batch: 427\n",
      "Done with the batch: 428\n",
      "Done with the batch: 429\n",
      "Done with the batch: 430\n",
      "Done with the batch: 431\n",
      "Done with the batch: 432\n",
      "Done with the batch: 433\n",
      "Done with the batch: 434\n",
      "Done with the batch: 435\n",
      "Done with the batch: 436\n",
      "Done with the batch: 437\n",
      "Done with the batch: 438\n",
      "Done with the batch: 439\n",
      "Done with the batch: 440\n",
      "Done with the batch: 441\n",
      "Done with the batch: 442\n",
      "Done with the batch: 443\n",
      "Done with the batch: 444\n",
      "Done with the batch: 445\n",
      "Done with the batch: 446\n",
      "Done with the batch: 447\n",
      "Done with the batch: 448\n",
      "Done with the batch: 449\n",
      "Done with the batch: 450\n",
      "Done with the batch: 451\n",
      "Done with the batch: 452\n",
      "Done with the batch: 453\n",
      "Done with the batch: 454\n",
      "Done with the batch: 455\n",
      "Done with the batch: 456\n",
      "Done with the batch: 457\n",
      "Done with the batch: 458\n",
      "Done with the batch: 459\n",
      "Done with the batch: 460\n",
      "Done with the batch: 461\n",
      "Done with the batch: 462\n",
      "Done with the batch: 463\n",
      "Done with the batch: 464\n",
      "Done with the batch: 465\n",
      "Done with the batch: 466\n",
      "Done with the batch: 467\n",
      "Done with the batch: 468\n",
      "Done with the batch: 469\n",
      "Done with the batch: 470\n",
      "Done with the batch: 471\n",
      "Done with the batch: 472\n",
      "Done with the batch: 473\n",
      "Done with the batch: 474\n",
      "Done with the batch: 475\n",
      "Done with the batch: 476\n",
      "Done with the batch: 477\n",
      "Done with the batch: 478\n",
      "Done with the batch: 479\n",
      "Done with the batch: 480\n",
      "Done with the batch: 481\n",
      "Done with the batch: 482\n",
      "Done with the batch: 483\n",
      "Done with the batch: 484\n",
      "Done with the batch: 485\n",
      "Done with the batch: 486\n",
      "Done with the batch: 487\n",
      "Done with the batch: 488\n",
      "Done with the batch: 489\n",
      "Done with the batch: 490\n",
      "Done with the batch: 491\n",
      "Done with the batch: 492\n",
      "Done with the batch: 493\n",
      "Done with the batch: 494\n",
      "Done with the batch: 495\n",
      "Done with the batch: 496\n",
      "Done with the batch: 497\n",
      "Done with the batch: 498\n",
      "Done with the batch: 499\n",
      "Done with the batch: 500\n",
      "Done with the batch: 501\n",
      "Done with the batch: 502\n",
      "Done with the batch: 503\n",
      "Done with the batch: 504\n",
      "Done with the batch: 505\n",
      "Done with the batch: 506\n",
      "Done with the batch: 507\n",
      "Done with the batch: 508\n",
      "Done with the batch: 509\n",
      "Done with the batch: 510\n",
      "Done with the batch: 511\n",
      "Done with the batch: 512\n",
      "Done with the batch: 513\n",
      "Done with the batch: 514\n",
      "Done with the batch: 515\n",
      "Done with the batch: 516\n",
      "Done with the batch: 517\n",
      "Done with the batch: 518\n",
      "Done with the batch: 519\n",
      "Done with the batch: 520\n",
      "Done with the batch: 521\n",
      "Done with the batch: 522\n",
      "Done with the batch: 523\n",
      "Done with the batch: 524\n",
      "Done with the batch: 525\n",
      "Done with the batch: 526\n",
      "Done with the batch: 527\n",
      "Done with the batch: 528\n",
      "Done with the batch: 529\n",
      "Done with the batch: 530\n",
      "Done with the batch: 531\n",
      "Done with the batch: 532\n",
      "Done with the batch: 533\n",
      "Done with the batch: 534\n",
      "Done with the batch: 535\n",
      "Done with the batch: 536\n",
      "Done with the batch: 537\n",
      "Done with the batch: 538\n",
      "Done with the batch: 539\n",
      "Done with the batch: 540\n",
      "Done with the batch: 541\n",
      "Done with the batch: 542\n",
      "Done with the batch: 543\n",
      "Done with the batch: 544\n",
      "Done with the batch: 545\n",
      "Done with the batch: 546\n",
      "Done with the batch: 547\n",
      "Done with the batch: 548\n",
      "Done with the batch: 549\n",
      "Done with the batch: 550\n",
      "Done with the batch: 551\n",
      "Done with the batch: 552\n",
      "Done with the batch: 553\n",
      "Done with the batch: 554\n",
      "Done with the batch: 555\n",
      "Done with the batch: 556\n",
      "Done with the batch: 557\n",
      "Done with the batch: 558\n",
      "Done with the batch: 559\n",
      "Done with the batch: 560\n",
      "Done with the batch: 561\n",
      "Done with the batch: 562\n",
      "Done with the batch: 563\n",
      "Done with the batch: 564\n",
      "Done with the batch: 565\n",
      "Done with the batch: 566\n",
      "Done with the batch: 567\n",
      "Done with the batch: 568\n",
      "Done with the batch: 569\n",
      "Done with the batch: 570\n",
      "Done with the batch: 571\n",
      "Done with the batch: 572\n",
      "Done with the batch: 573\n",
      "Done with the batch: 574\n",
      "Done with the batch: 575\n",
      "Done with the batch: 576\n",
      "Done with the batch: 577\n",
      "Done with the batch: 578\n",
      "Done with the batch: 579\n",
      "Done with the batch: 580\n",
      "Done with the batch: 581\n",
      "Done with the batch: 582\n",
      "Done with the batch: 583\n",
      "Done with the batch: 584\n",
      "Done with the batch: 585\n",
      "Done with the batch: 586\n",
      "Done with the batch: 587\n",
      "Done with the batch: 588\n",
      "Done with the batch: 589\n",
      "Done with the batch: 590\n",
      "Done with the batch: 591\n",
      "Done with the batch: 592\n",
      "Done with the batch: 593\n",
      "Done with the batch: 594\n",
      "Done with the batch: 595\n",
      "Done with the batch: 596\n",
      "Done with the batch: 597\n",
      "Done with the batch: 598\n",
      "Done with the batch: 599\n",
      "Done with the batch: 600\n",
      "Done with the batch: 601\n",
      "Done with the batch: 602\n",
      "Done with the batch: 603\n",
      "Done with the batch: 604\n",
      "Done with the batch: 605\n",
      "Done with the batch: 606\n",
      "Done with the batch: 607\n",
      "Done with the batch: 608\n",
      "Done with the batch: 609\n",
      "Done with the batch: 610\n",
      "Done with the batch: 611\n",
      "Done with the batch: 612\n",
      "Done with the batch: 613\n",
      "Done with the batch: 614\n",
      "Done with the batch: 615\n",
      "Done with the batch: 616\n",
      "Done with the batch: 617\n",
      "Done with the batch: 618\n",
      "Done with the batch: 619\n",
      "Done with the batch: 620\n",
      "Done with the batch: 621\n",
      "Done with the batch: 622\n",
      "Done with the batch: 623\n",
      "Done with the batch: 624\n",
      "Done with the batch: 625\n",
      "Done with the batch: 626\n",
      "Done with the batch: 627\n",
      "Done with the batch: 628\n",
      "Done with the batch: 629\n",
      "Done with the batch: 630\n",
      "Done with the batch: 631\n",
      "Done with the batch: 632\n",
      "Done with the batch: 633\n",
      "Done with the batch: 634\n",
      "Done with the batch: 635\n",
      "Done with the batch: 636\n",
      "Done with the batch: 637\n",
      "Done with the batch: 638\n",
      "Done with the batch: 639\n",
      "Done with the batch: 640\n",
      "Done with the batch: 641\n",
      "Done with the batch: 642\n",
      "Done with the batch: 643\n",
      "Done with the batch: 644\n",
      "Done with the batch: 645\n",
      "Done with the batch: 646\n",
      "Done with the batch: 647\n",
      "Done with the batch: 648\n",
      "Done with the batch: 649\n",
      "Done with the batch: 650\n",
      "Done with the batch: 651\n",
      "Done with the batch: 652\n",
      "Done with the batch: 653\n",
      "Done with the batch: 654\n",
      "Done with the batch: 655\n",
      "Done with the batch: 656\n",
      "Done with the batch: 657\n",
      "Done with the batch: 658\n",
      "Done with the batch: 659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 660\n",
      "Done with the batch: 661\n",
      "Done with the batch: 662\n",
      "Done with the batch: 663\n",
      "Done with the batch: 664\n",
      "Done with the batch: 665\n",
      "Done with the batch: 666\n",
      "Done with the batch: 667\n",
      "Done with the batch: 668\n",
      "Done with the batch: 669\n",
      "Done with the batch: 670\n",
      "Done with the batch: 671\n",
      "Done with the batch: 672\n",
      "Done with the batch: 673\n",
      "Done with the batch: 674\n",
      "Done with the batch: 675\n",
      "Done with the batch: 676\n",
      "Done with the batch: 677\n",
      "Done with the batch: 678\n",
      "Done with the batch: 679\n",
      "Done with the batch: 680\n",
      "Done with the batch: 681\n",
      "Done with the batch: 682\n",
      "Done with the batch: 683\n",
      "Done with the batch: 684\n",
      "Done with the batch: 685\n",
      "Done with the batch: 686\n",
      "Done with the batch: 687\n",
      "Done with the batch: 688\n",
      "Done with the batch: 689\n",
      "Done with the batch: 690\n",
      "Done with the batch: 691\n",
      "Done with the batch: 692\n",
      "Done with the batch: 693\n",
      "Done with the batch: 694\n",
      "Done with the batch: 695\n",
      "Done with the batch: 696\n",
      "Done with the batch: 697\n",
      "Done with the batch: 698\n",
      "Done with the batch: 699\n",
      "Done with the batch: 700\n",
      "Done with the batch: 701\n",
      "Done with the batch: 702\n",
      "Done with the batch: 703\n",
      "Done with the batch: 704\n",
      "Done with the batch: 705\n",
      "Done with the batch: 706\n",
      "Done with the batch: 707\n",
      "Done with the batch: 708\n",
      "Done with the batch: 709\n",
      "Done with the batch: 710\n",
      "Done with the batch: 711\n",
      "Done with the batch: 712\n",
      "Done with the batch: 713\n",
      "Done with the batch: 714\n",
      "Done with the batch: 715\n",
      "Done with the batch: 716\n",
      "Done with the batch: 717\n",
      "Done with the batch: 718\n",
      "Done with the batch: 719\n",
      "Done with the batch: 720\n",
      "Done with the batch: 721\n",
      "Done with the batch: 722\n",
      "Done with the batch: 723\n",
      "Done with the batch: 724\n",
      "Done with the batch: 725\n",
      "Done with the batch: 726\n",
      "Done with the batch: 727\n",
      "Done with the batch: 728\n",
      "Done with the batch: 729\n",
      "Done with the batch: 730\n",
      "Done with the batch: 731\n",
      "Done with the batch: 732\n",
      "Done with the batch: 733\n",
      "Done with the batch: 734\n",
      "Done with the batch: 735\n",
      "Done with the batch: 736\n",
      "Done with the batch: 737\n",
      "Done with the batch: 738\n",
      "Done with the batch: 739\n",
      "Done with the batch: 740\n",
      "Done with the batch: 741\n",
      "Done with the batch: 742\n",
      "Done with the batch: 743\n",
      "Done with the batch: 744\n",
      "Done with the batch: 745\n",
      "Done with the batch: 746\n",
      "Done with the batch: 747\n",
      "Done with the batch: 748\n",
      "Done with the batch: 749\n",
      "Done with the batch: 750\n",
      "Done with the batch: 751\n",
      "Done with the batch: 752\n",
      "Done with the batch: 753\n",
      "Done with the batch: 754\n",
      "Done with the batch: 755\n",
      "Done with the batch: 756\n",
      "Done with the batch: 757\n",
      "Done with the batch: 758\n",
      "Done with the batch: 759\n",
      "Done with the batch: 760\n",
      "Done with the batch: 761\n",
      "Done with the batch: 762\n",
      "Done with the batch: 763\n",
      "Done with the batch: 764\n",
      "Done with the batch: 765\n",
      "Done with the batch: 766\n",
      "Done with the batch: 767\n",
      "Done with the batch: 768\n",
      "Done with the batch: 769\n",
      "Done with the batch: 770\n",
      "Done with the batch: 771\n",
      "Done with the batch: 772\n",
      "Done with the batch: 773\n",
      "Done with the batch: 774\n",
      "Done with the batch: 775\n",
      "Done with the batch: 776\n",
      "Done with the batch: 777\n",
      "Done with the batch: 778\n",
      "Done with the batch: 779\n",
      "Done with the batch: 780\n",
      "Done with the batch: 781\n",
      "Done with the batch: 782\n",
      "Done with the batch: 783\n",
      "Done with the batch: 784\n",
      "Done with the batch: 785\n",
      "Done with the batch: 786\n",
      "Done with the batch: 787\n",
      "Done with the batch: 788\n",
      "Done with the batch: 789\n",
      "Done with the batch: 790\n",
      "Done with the batch: 791\n",
      "Done with the batch: 792\n",
      "Done with the batch: 793\n",
      "Done with the batch: 794\n",
      "Done with the batch: 795\n",
      "Done with the batch: 796\n",
      "Done with the batch: 797\n",
      "Done with the batch: 798\n",
      "Done with the batch: 799\n",
      "Done with the batch: 800\n",
      "Done with the batch: 801\n",
      "Done with the batch: 802\n",
      "Done with the batch: 803\n",
      "Done with the batch: 804\n",
      "Done with the batch: 805\n",
      "Done with the batch: 806\n",
      "Done with the batch: 807\n",
      "Done with the batch: 808\n",
      "Done with the batch: 809\n",
      "Done with the batch: 810\n",
      "Done with the batch: 811\n",
      "Done with the batch: 812\n",
      "Done with the batch: 813\n",
      "Done with the batch: 814\n",
      "Done with the batch: 815\n",
      "Done with the batch: 816\n",
      "Done with the batch: 817\n",
      "Done with the batch: 818\n",
      "Done with the batch: 819\n",
      "Done with the batch: 820\n",
      "Done with the batch: 821\n",
      "Done with the batch: 822\n",
      "Done with the batch: 823\n",
      "Done with the batch: 824\n",
      "Done with the batch: 825\n",
      "Done with the batch: 826\n",
      "Done with the batch: 827\n",
      "Done with the batch: 828\n",
      "Done with the batch: 829\n",
      "Done with the batch: 830\n",
      "Done with the batch: 831\n",
      "Done with the batch: 832\n",
      "Done with the batch: 833\n",
      "Done with the batch: 834\n",
      "Done with the batch: 835\n",
      "Done with the batch: 836\n",
      "Done with the batch: 837\n",
      "Done with the batch: 838\n",
      "Done with the batch: 839\n",
      "Done with the batch: 840\n",
      "Done with the batch: 841\n",
      "Done with the batch: 842\n",
      "Done with the batch: 843\n",
      "Done with the batch: 844\n",
      "Done with the batch: 845\n",
      "Done with the batch: 846\n",
      "Done with the batch: 847\n",
      "Done with the batch: 848\n",
      "Done with the batch: 849\n",
      "Done with the batch: 850\n",
      "Done with the batch: 851\n",
      "Done with the batch: 852\n",
      "Done with the batch: 853\n",
      "Done with the batch: 854\n",
      "Done with the batch: 855\n",
      "Done with the batch: 856\n",
      "Done with the batch: 857\n",
      "Done with the batch: 858\n",
      "Done with the batch: 859\n",
      "Done with the batch: 860\n",
      "Done with the batch: 861\n",
      "Done with the batch: 862\n",
      "Done with the batch: 863\n",
      "Done with the batch: 864\n",
      "Done with the batch: 865\n",
      "Done with the batch: 866\n",
      "Done with the batch: 867\n",
      "Done with the batch: 868\n",
      "Done with the batch: 869\n",
      "Done with the batch: 870\n",
      "Done with the batch: 871\n",
      "Done with the batch: 872\n",
      "Done with the batch: 873\n",
      "Done with the batch: 874\n",
      "Done with the batch: 875\n",
      "Done with the batch: 876\n",
      "Done with the batch: 877\n",
      "Done with the batch: 878\n",
      "Done with the batch: 879\n",
      "Done with the batch: 880\n",
      "Done with the batch: 881\n",
      "Done with the batch: 882\n",
      "Done with the batch: 883\n",
      "Done with the batch: 884\n",
      "Done with the batch: 885\n",
      "Done with the batch: 886\n",
      "Done with the batch: 887\n",
      "Done with the batch: 888\n",
      "Done with the batch: 889\n",
      "Done with the batch: 890\n",
      "Done with the batch: 891\n",
      "Done with the batch: 892\n",
      "Done with the batch: 893\n",
      "Done with the batch: 894\n",
      "Done with the batch: 895\n",
      "Done with the batch: 896\n",
      "Done with the batch: 897\n",
      "Done with the batch: 898\n",
      "Done with the batch: 899\n",
      "Done with the batch: 900\n",
      "Done with the batch: 901\n",
      "Done with the batch: 902\n",
      "Done with the batch: 903\n",
      "Done with the batch: 904\n",
      "Done with the batch: 905\n",
      "Done with the batch: 906\n",
      "Done with the batch: 907\n",
      "Done with the batch: 908\n",
      "Done with the batch: 909\n",
      "Done with the batch: 910\n",
      "Done with the batch: 911\n",
      "Done with the batch: 912\n",
      "Done with the batch: 913\n",
      "Done with the batch: 914\n",
      "Done with the batch: 915\n",
      "Done with the batch: 916\n",
      "Done with the batch: 917\n",
      "Done with the batch: 918\n",
      "Done with the batch: 919\n",
      "Done with the batch: 920\n",
      "Done with the batch: 921\n",
      "Done with the batch: 922\n",
      "Done with the batch: 923\n",
      "Done with the batch: 924\n",
      "Done with the batch: 925\n",
      "Done with the batch: 926\n",
      "Done with the batch: 927\n",
      "Done with the batch: 928\n",
      "Done with the batch: 929\n",
      "Done with the batch: 930\n",
      "Done with the batch: 931\n",
      "Done with the batch: 932\n",
      "Done with the batch: 933\n",
      "Done with the batch: 934\n",
      "Done with the batch: 935\n",
      "Done with the batch: 936\n",
      "Done with the batch: 937\n",
      "Done with the batch: 938\n",
      "Done with the batch: 939\n",
      "Done with the batch: 940\n",
      "Done with the batch: 941\n",
      "Done with the batch: 942\n",
      "Done with the batch: 943\n",
      "Done with the batch: 944\n",
      "Done with the batch: 945\n",
      "Done with the batch: 946\n",
      "Done with the batch: 947\n",
      "Done with the batch: 948\n",
      "Done with the batch: 949\n",
      "Done with the batch: 950\n",
      "Done with the batch: 951\n",
      "Done with the batch: 952\n",
      "Done with the batch: 953\n",
      "Done with the batch: 954\n",
      "Done with the batch: 955\n",
      "Done with the batch: 956\n",
      "Done with the batch: 957\n",
      "Done with the batch: 958\n",
      "Done with the batch: 959\n",
      "Done with the batch: 960\n",
      "Done with the batch: 961\n",
      "Done with the batch: 962\n",
      "Done with the batch: 963\n",
      "Done with the batch: 964\n",
      "Done with the batch: 965\n",
      "Done with the batch: 966\n",
      "Done with the batch: 967\n",
      "Done with the batch: 968\n",
      "Done with the batch: 969\n",
      "Done with the batch: 970\n",
      "Done with the batch: 971\n",
      "Done with the batch: 972\n",
      "Done with the batch: 973\n",
      "Done with the batch: 974\n",
      "Done with the batch: 975\n",
      "Done with the batch: 976\n",
      "Done with the batch: 977\n",
      "Done with the batch: 978\n",
      "Done with the batch: 979\n",
      "Done with the batch: 980\n",
      "Done with the batch: 981\n",
      "Done with the batch: 982\n",
      "Done with the batch: 983\n",
      "Done with the batch: 984\n",
      "Done with the batch: 985\n",
      "Done with the batch: 986\n",
      "Done with the batch: 987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 988\n",
      "Done with the batch: 989\n",
      "Done with the batch: 990\n",
      "Done with the batch: 991\n",
      "Done with the batch: 992\n",
      "Done with the batch: 993\n",
      "Done with the batch: 994\n",
      "Done with the batch: 995\n",
      "Done with the batch: 996\n",
      "Done with the batch: 997\n",
      "Done with the batch: 998\n",
      "Done with the batch: 999\n",
      "Done with the batch: 1000\n",
      "Done with the batch: 1001\n",
      "Done with the batch: 1002\n",
      "Done with the batch: 1003\n",
      "Done with the batch: 1004\n",
      "Done with the batch: 1005\n",
      "Done with the batch: 1006\n",
      "Done with the batch: 1007\n",
      "Done with the batch: 1008\n",
      "Done with the batch: 1009\n",
      "Done with the batch: 1010\n",
      "Done with the batch: 1011\n",
      "Done with the batch: 1012\n",
      "Done with the batch: 1013\n",
      "Done with the batch: 1014\n",
      "Done with the batch: 1015\n",
      "Done with the batch: 1016\n",
      "Done with the batch: 1017\n",
      "Done with the batch: 1018\n",
      "Done with the batch: 1019\n",
      "Done with the batch: 1020\n",
      "Done with the batch: 1021\n",
      "Done with the batch: 1022\n",
      "Done with the batch: 1023\n",
      "Done with the batch: 1024\n",
      "Done with the batch: 1025\n",
      "Done with the batch: 1026\n",
      "Done with the batch: 1027\n",
      "Done with the batch: 1028\n",
      "Done with the batch: 1029\n",
      "Done with the batch: 1030\n",
      "Done with the batch: 1031\n",
      "Done with the batch: 1032\n",
      "Done with the batch: 1033\n",
      "Done with the batch: 1034\n",
      "Done with the batch: 1035\n",
      "Done with the batch: 1036\n",
      "Done with the batch: 1037\n",
      "Done with the batch: 1038\n",
      "Done with the batch: 1039\n",
      "Done with the batch: 1040\n",
      "Done with the batch: 1041\n",
      "Done with the batch: 1042\n",
      "Done with the batch: 1043\n",
      "Done with the batch: 1044\n",
      "Done with the batch: 1045\n",
      "Done with the batch: 1046\n",
      "Done with the batch: 1047\n",
      "Done with the batch: 1048\n",
      "Done with the batch: 1049\n",
      "Done with the batch: 1050\n",
      "Done with the batch: 1051\n",
      "Done with the batch: 1052\n",
      "Done with the batch: 1053\n",
      "Done with the batch: 1054\n",
      "Done with the batch: 1055\n",
      "Done with the batch: 1056\n",
      "Done with the batch: 1057\n",
      "Done with the batch: 1058\n",
      "Done with the batch: 1059\n",
      "Done with the batch: 1060\n",
      "Done with the batch: 1061\n",
      "Done with the batch: 1062\n",
      "Done with the batch: 1063\n",
      "Done with the batch: 1064\n",
      "Done with the batch: 1065\n",
      "Done with the batch: 1066\n",
      "Done with the batch: 1067\n",
      "Done with the batch: 1068\n",
      "Done with the batch: 1069\n",
      "Done with the batch: 1070\n",
      "Done with the batch: 1071\n",
      "Done with the batch: 1072\n",
      "Done with the batch: 1073\n",
      "Done with the batch: 1074\n",
      "Done with the batch: 1075\n",
      "Done with the batch: 1076\n",
      "Done with the batch: 1077\n",
      "Done with the batch: 1078\n",
      "Done with the batch: 1079\n",
      "Done with the batch: 1080\n",
      "Done with the batch: 1081\n",
      "Done with the batch: 1082\n",
      "Done with the batch: 1083\n",
      "Done with the batch: 1084\n",
      "Done with the batch: 1085\n",
      "Done with the batch: 1086\n",
      "Done with the batch: 1087\n",
      "Done with the batch: 1088\n",
      "Done with the batch: 1089\n",
      "Done with the batch: 1090\n",
      "Done with the batch: 1091\n",
      "Done with the batch: 1092\n",
      "Done with the batch: 1093\n",
      "Done with the batch: 1094\n",
      "Done with the batch: 1095\n",
      "Done with the batch: 1096\n",
      "Done with the batch: 1097\n",
      "Done with the batch: 1098\n",
      "Done with the batch: 1099\n",
      "Done with the batch: 1100\n",
      "Done with the batch: 1101\n",
      "Done with the batch: 1102\n",
      "Done with the batch: 1103\n",
      "Done with the batch: 1104\n",
      "Done with the batch: 1105\n",
      "Done with the batch: 1106\n",
      "Done with the batch: 1107\n",
      "Done with the batch: 1108\n",
      "Done with the batch: 1109\n",
      "Done with the batch: 1110\n",
      "Done with the batch: 1111\n",
      "Done with the batch: 1112\n",
      "Done with the batch: 1113\n",
      "Done with the batch: 1114\n",
      "Done with the batch: 1115\n",
      "Done with the batch: 1116\n",
      "Done with the batch: 1117\n",
      "Done with the batch: 1118\n",
      "Done with the batch: 1119\n",
      "Done with the batch: 1120\n",
      "Done with the batch: 1121\n",
      "Done with the batch: 1122\n",
      "Done with the batch: 1123\n",
      "Done with the batch: 1124\n",
      "Done with the batch: 1125\n",
      "Done with the batch: 1126\n",
      "Done with the batch: 1127\n",
      "Done with the batch: 1128\n",
      "Done with the batch: 1129\n",
      "Done with the batch: 1130\n",
      "Done with the batch: 1131\n",
      "Done with the batch: 1132\n",
      "Done with the batch: 1133\n",
      "Done with the batch: 1134\n",
      "Done with the batch: 1135\n",
      "Done with the batch: 1136\n",
      "Done with the batch: 1137\n",
      "Done with the batch: 1138\n",
      "Done with the batch: 1139\n",
      "Done with the batch: 1140\n",
      "Done with the batch: 1141\n",
      "Done with the batch: 1142\n",
      "Done with the batch: 1143\n",
      "Done with the batch: 1144\n",
      "Done with the batch: 1145\n",
      "Done with the batch: 1146\n",
      "Done with the batch: 1147\n",
      "Done with the batch: 1148\n",
      "Done with the batch: 1149\n",
      "Done with the batch: 1150\n",
      "Done with the batch: 1151\n",
      "Done with the batch: 1152\n",
      "Done with the batch: 1153\n",
      "Done with the batch: 1154\n",
      "Done with the batch: 1155\n",
      "Done with the batch: 1156\n",
      "Done with the batch: 1157\n",
      "Done with the batch: 1158\n",
      "Done with the batch: 1159\n",
      "Done with the batch: 1160\n",
      "Done with the batch: 1161\n",
      "Done with the batch: 1162\n",
      "Done with the batch: 1163\n",
      "Done with the batch: 1164\n",
      "Done with the batch: 1165\n",
      "Done with the batch: 1166\n",
      "Done with the batch: 1167\n",
      "Done with the batch: 1168\n",
      "Done with the batch: 1169\n",
      "Done with the batch: 1170\n",
      "Done with the batch: 1171\n",
      "Done with the batch: 1172\n",
      "Done with the batch: 1173\n",
      "Done with the batch: 1174\n",
      "Done with the batch: 1175\n",
      "Done with the batch: 1176\n",
      "Done with the batch: 1177\n",
      "Done with the batch: 1178\n",
      "Done with the batch: 1179\n",
      "Done with the batch: 1180\n",
      "Done with the batch: 1181\n",
      "Done with the batch: 1182\n",
      "Done with the batch: 1183\n",
      "Done with the batch: 1184\n",
      "Done with the batch: 1185\n",
      "Done with the batch: 1186\n",
      "Done with the batch: 1187\n",
      "Done with the batch: 1188\n",
      "Done with the batch: 1189\n",
      "Done with the batch: 1190\n",
      "Done with the batch: 1191\n",
      "Done with the batch: 1192\n",
      "Done with the batch: 1193\n",
      "Done with the batch: 1194\n",
      "Done with the batch: 1195\n",
      "Done with the batch: 1196\n",
      "Done with the batch: 1197\n",
      "Done with the batch: 1198\n",
      "Done with the batch: 1199\n",
      "Done with the batch: 1200\n",
      "Done with the batch: 1201\n",
      "Done with the batch: 1202\n",
      "Done with the batch: 1203\n",
      "Done with the batch: 1204\n",
      "Done with the batch: 1205\n",
      "Done with the batch: 1206\n",
      "Done with the batch: 1207\n",
      "Done with the batch: 1208\n",
      "Done with the batch: 1209\n",
      "Done with the batch: 1210\n",
      "Done with the batch: 1211\n",
      "Done with the batch: 1212\n",
      "Done with the batch: 1213\n",
      "Done with the batch: 1214\n",
      "Done with the batch: 1215\n",
      "Done with the batch: 1216\n",
      "Done with the batch: 1217\n",
      "Done with the batch: 1218\n",
      "Done with the batch: 1219\n",
      "Done with the batch: 1220\n",
      "Done with the batch: 1221\n",
      "Done with the batch: 1222\n",
      "Done with the batch: 1223\n",
      "Done with the batch: 1224\n",
      "Done with the batch: 1225\n",
      "Done with the batch: 1226\n",
      "Done with the batch: 1227\n",
      "Done with the batch: 1228\n",
      "Done with the batch: 1229\n",
      "Done with the batch: 1230\n",
      "Done with the batch: 1231\n",
      "Done with the batch: 1232\n",
      "Done with the batch: 1233\n",
      "Done with the batch: 1234\n",
      "Done with the batch: 1235\n",
      "Done with the batch: 1236\n",
      "Done with the batch: 1237\n",
      "Done with the batch: 1238\n",
      "Done with the batch: 1239\n",
      "Done with the batch: 1240\n",
      "Done with the batch: 1241\n",
      "Done with the batch: 1242\n",
      "Done with the batch: 1243\n",
      "Done with the batch: 1244\n",
      "Done with the batch: 1245\n",
      "Done with the batch: 1246\n",
      "Done with the batch: 1247\n",
      "Done with the batch: 1248\n",
      "Done with the batch: 1249\n",
      "Done with the batch: 1250\n",
      "Done with the batch: 1251\n",
      "Done with the batch: 1252\n",
      "Done with the batch: 1253\n",
      "Done with the batch: 1254\n",
      "Done with the batch: 1255\n",
      "Done with the batch: 1256\n",
      "Done with the batch: 1257\n",
      "Done with the batch: 1258\n",
      "Done with the batch: 1259\n",
      "Done with the batch: 1260\n",
      "Done with the batch: 1261\n",
      "Done with the batch: 1262\n",
      "Done with the batch: 1263\n",
      "Done with the batch: 1264\n",
      "Done with the batch: 1265\n",
      "Done with the batch: 1266\n",
      "Done with the batch: 1267\n",
      "Done with the batch: 1268\n",
      "Done with the batch: 1269\n",
      "Done with the batch: 1270\n",
      "Done with the batch: 1271\n",
      "Done with the batch: 1272\n",
      "Done with the batch: 1273\n",
      "Done with the batch: 1274\n",
      "Done with the batch: 1275\n",
      "Done with the batch: 1276\n",
      "Done with the batch: 1277\n",
      "Done with the batch: 1278\n",
      "Done with the batch: 1279\n",
      "Done with the batch: 1280\n",
      "Done with the batch: 1281\n",
      "Done with the batch: 1282\n",
      "Done with the batch: 1283\n",
      "Done with the batch: 1284\n",
      "Done with the batch: 1285\n",
      "Done with the batch: 1286\n",
      "Done with the batch: 1287\n",
      "Done with the batch: 1288\n",
      "Done with the batch: 1289\n",
      "Done with the batch: 1290\n",
      "Done with the batch: 1291\n",
      "Done with the batch: 1292\n",
      "Done with the batch: 1293\n",
      "Done with the batch: 1294\n",
      "Done with the batch: 1295\n",
      "Done with the batch: 1296\n",
      "Done with the batch: 1297\n",
      "Done with the batch: 1298\n",
      "Done with the batch: 1299\n",
      "Done with the batch: 1300\n",
      "Done with the batch: 1301\n",
      "Done with the batch: 1302\n",
      "Done with the batch: 1303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 1304\n",
      "Done with the batch: 1305\n",
      "Done with the batch: 1306\n",
      "Done with the batch: 1307\n",
      "Done with the batch: 1308\n",
      "Done with the batch: 1309\n",
      "Done with the batch: 1310\n",
      "Done with the batch: 1311\n",
      "Done with the batch: 1312\n",
      "Done with the batch: 1313\n",
      "Done with the batch: 1314\n",
      "Done with the batch: 1315\n",
      "Done with the batch: 1316\n",
      "Done with the batch: 1317\n",
      "Done with the batch: 1318\n",
      "Done with the batch: 1319\n",
      "Done with the batch: 1320\n",
      "Done with the batch: 1321\n",
      "Done with the batch: 1322\n",
      "Done with the batch: 1323\n",
      "Done with the batch: 1324\n",
      "Done with the batch: 1325\n",
      "Done with the batch: 1326\n",
      "Done with the batch: 1327\n",
      "Done with the batch: 1328\n",
      "Done with the batch: 1329\n",
      "Done with the batch: 1330\n",
      "Done with the batch: 1331\n",
      "Done with the batch: 1332\n",
      "Done with the batch: 1333\n",
      "Done with the batch: 1334\n",
      "Done with the batch: 1335\n",
      "Done with the batch: 1336\n",
      "Done with the batch: 1337\n",
      "Done with the batch: 1338\n",
      "Done with the batch: 1339\n",
      "Done with the batch: 1340\n",
      "Done with the batch: 1341\n",
      "Done with the batch: 1342\n",
      "Done with the batch: 1343\n",
      "Done with the batch: 1344\n",
      "Done with the batch: 1345\n",
      "Done with the batch: 1346\n",
      "Done with the batch: 1347\n",
      "Done with the batch: 1348\n",
      "Done with the batch: 1349\n",
      "Done with the batch: 1350\n",
      "Done with the batch: 1351\n",
      "Done with the batch: 1352\n",
      "Done with the batch: 1353\n",
      "Done with the batch: 1354\n",
      "Done with the batch: 1355\n",
      "Done with the batch: 1356\n",
      "Done with the batch: 1357\n",
      "Done with the batch: 1358\n",
      "Done with the batch: 1359\n",
      "Done with the batch: 1360\n",
      "Done with the batch: 1361\n",
      "Done with the batch: 1362\n",
      "Done with the batch: 1363\n",
      "Done with the batch: 1364\n",
      "Done with the batch: 1365\n",
      "Done with the batch: 1366\n",
      "Done with the batch: 1367\n",
      "Done with the batch: 1368\n",
      "Done with the batch: 1369\n",
      "Done with the batch: 1370\n",
      "Done with the batch: 1371\n",
      "Done with the batch: 1372\n",
      "Done with the batch: 1373\n",
      "Done with the batch: 1374\n",
      "Done with the batch: 1375\n",
      "Done with the batch: 1376\n",
      "Done with the batch: 1377\n",
      "Done with the batch: 1378\n",
      "Done with the batch: 1379\n",
      "Done with the batch: 1380\n",
      "Done with the batch: 1381\n",
      "Done with the batch: 1382\n",
      "Done with the batch: 1383\n",
      "Done with the batch: 1384\n",
      "Done with the batch: 1385\n",
      "Done with the batch: 1386\n",
      "Done with the batch: 1387\n",
      "Done with the batch: 1388\n",
      "Done with the batch: 1389\n",
      "Done with the batch: 1390\n",
      "Done with the batch: 1391\n",
      "Done with the batch: 1392\n",
      "Done with the batch: 1393\n",
      "Done with the batch: 1394\n",
      "Done with the batch: 1395\n",
      "Done with the batch: 1396\n",
      "Done with the batch: 1397\n",
      "Done with the batch: 1398\n",
      "Done with the batch: 1399\n",
      "Done with the batch: 1400\n",
      "Done with the batch: 1401\n",
      "Done with the batch: 1402\n",
      "Done with the batch: 1403\n",
      "Done with the batch: 1404\n",
      "Done with the batch: 1405\n",
      "Done with the batch: 1406\n",
      "Done with the batch: 1407\n",
      "Done with the batch: 1408\n",
      "Done with the batch: 1409\n",
      "Done with the batch: 1410\n",
      "Done with the batch: 1411\n",
      "Done with the batch: 1412\n",
      "Done with the batch: 1413\n",
      "Done with the batch: 1414\n",
      "Done with the batch: 1415\n",
      "Done with the batch: 1416\n",
      "Done with the batch: 1417\n",
      "Done with the batch: 1418\n",
      "Done with the batch: 1419\n",
      "Done with the batch: 1420\n",
      "Done with the batch: 1421\n",
      "Done with the batch: 1422\n",
      "Done with the batch: 1423\n",
      "Done with the batch: 1424\n",
      "Done with the batch: 1425\n",
      "Done with the batch: 1426\n",
      "Done with the batch: 1427\n",
      "Done with the batch: 1428\n",
      "Done with the batch: 1429\n",
      "Done with the batch: 1430\n",
      "Done with the batch: 1431\n",
      "Done with the batch: 1432\n",
      "Done with the batch: 1433\n",
      "Done with the batch: 1434\n",
      "Done with the batch: 1435\n",
      "Done with the batch: 1436\n",
      "Done with the batch: 1437\n",
      "Done with the batch: 1438\n",
      "Done with the batch: 1439\n",
      "Done with the batch: 1440\n",
      "Done with the batch: 1441\n",
      "Done with the batch: 1442\n",
      "Done with the batch: 1443\n",
      "Done with the batch: 1444\n",
      "Done with the batch: 1445\n",
      "Done with the batch: 1446\n",
      "Done with the batch: 1447\n",
      "Done with the batch: 1448\n",
      "Done with the batch: 1449\n",
      "Done with the batch: 1450\n",
      "Done with the batch: 1451\n",
      "Done with the batch: 1452\n",
      "Done with the batch: 1453\n",
      "Done with the batch: 1454\n",
      "Done with the batch: 1455\n",
      "Done with the batch: 1456\n",
      "Done with the batch: 1457\n",
      "Done with the batch: 1458\n",
      "Done with the batch: 1459\n",
      "Done with the batch: 1460\n",
      "Done with the batch: 1461\n",
      "Done with the batch: 1462\n",
      "Done with the batch: 1463\n",
      "Done with the batch: 1464\n",
      "Done with the batch: 1465\n",
      "Done with the batch: 1466\n",
      "Done with the batch: 1467\n",
      "Done with the batch: 1468\n",
      "Done with the batch: 1469\n",
      "Done with the batch: 1470\n",
      "Done with the batch: 1471\n",
      "Done with the batch: 1472\n",
      "Done with the batch: 1473\n",
      "Done with the batch: 1474\n",
      "Done with the batch: 1475\n",
      "Done with the batch: 1476\n",
      "Done with the batch: 1477\n",
      "Done with the batch: 1478\n",
      "Done with the batch: 1479\n",
      "Done with the batch: 1480\n",
      "Done with the batch: 1481\n",
      "Done with the batch: 1482\n",
      "Done with the batch: 1483\n",
      "Done with the batch: 1484\n",
      "Done with the batch: 1485\n",
      "Done with the batch: 1486\n",
      "Done with the batch: 1487\n",
      "Done with the batch: 1488\n",
      "Done with the batch: 1489\n",
      "Done with the batch: 1490\n",
      "Done with the batch: 1491\n",
      "Done with the batch: 1492\n",
      "Done with the batch: 1493\n",
      "Done with the batch: 1494\n",
      "Done with the batch: 1495\n",
      "Done with the batch: 1496\n",
      "Done with the batch: 1497\n",
      "Done with the batch: 1498\n",
      "Done with the batch: 1499\n",
      "Done with the batch: 1500\n",
      "Done with the batch: 1501\n",
      "Done with the batch: 1502\n",
      "Done with the batch: 1503\n",
      "Done with the batch: 1504\n",
      "Done with the batch: 1505\n",
      "Done with the batch: 1506\n",
      "Done with the batch: 1507\n",
      "Done with the batch: 1508\n",
      "Done with the batch: 1509\n",
      "Done with the batch: 1510\n",
      "Done with the batch: 1511\n",
      "Done with the batch: 1512\n",
      "Done with the batch: 1513\n",
      "Done with the batch: 1514\n",
      "Done with the batch: 1515\n",
      "Done with the batch: 1516\n",
      "Done with the batch: 1517\n",
      "Done with the batch: 1518\n",
      "Done with the batch: 1519\n",
      "Done with the batch: 1520\n",
      "Done with the batch: 1521\n",
      "Done with the batch: 1522\n",
      "Done with the batch: 1523\n",
      "Done with the batch: 1524\n",
      "Done with the batch: 1525\n",
      "Done with the batch: 1526\n",
      "Done with the batch: 1527\n",
      "Done with the batch: 1528\n",
      "Done with the batch: 1529\n",
      "Done with the batch: 1530\n",
      "Done with the batch: 1531\n",
      "Done with the batch: 1532\n",
      "Done with the batch: 1533\n",
      "Done with the batch: 1534\n",
      "Done with the batch: 1535\n",
      "Done with the batch: 1536\n",
      "Done with the batch: 1537\n",
      "Done with the batch: 1538\n",
      "Done with the batch: 1539\n",
      "Done with the batch: 1540\n",
      "Done with the batch: 1541\n",
      "Done with the batch: 1542\n",
      "Done with the batch: 1543\n",
      "Done with the batch: 1544\n",
      "Done with the batch: 1545\n",
      "Done with the batch: 1546\n",
      "Done with the batch: 1547\n",
      "Done with the batch: 1548\n",
      "Done with the batch: 1549\n",
      "Done with the batch: 1550\n",
      "Done with the batch: 1551\n",
      "Done with the batch: 1552\n",
      "Done with the batch: 1553\n",
      "Done with the batch: 1554\n",
      "Done with the batch: 1555\n",
      "Done with the batch: 1556\n",
      "Done with the batch: 1557\n",
      "Done with the batch: 1558\n",
      "Done with the batch: 1559\n",
      "Done with the batch: 1560\n",
      "Done with the batch: 1561\n",
      "Done with the batch: 1562\n",
      "Done with the batch: 1563\n",
      "Done with the batch: 1564\n",
      "Done with the batch: 1565\n",
      "Done with the batch: 1566\n",
      "Done with the batch: 1567\n",
      "Done with the batch: 1568\n",
      "Done with the batch: 1569\n",
      "Done with the batch: 1570\n",
      "Done with the batch: 1571\n",
      "Done with the batch: 1572\n",
      "Done with the batch: 1573\n",
      "Done with the batch: 1574\n",
      "Done with the batch: 1575\n",
      "Done with the batch: 1576\n",
      "Done with the batch: 1577\n",
      "Done with the batch: 1578\n",
      "Done with the batch: 1579\n",
      "Done with the batch: 1580\n",
      "Done with the batch: 1581\n",
      "Done with the batch: 1582\n",
      "Done with the batch: 1583\n",
      "Done with the batch: 1584\n",
      "Done with the batch: 1585\n",
      "Done with the batch: 1586\n",
      "Done with the batch: 1587\n",
      "Done with the batch: 1588\n",
      "Done with the batch: 1589\n",
      "Done with the batch: 1590\n",
      "Done with the batch: 1591\n",
      "Done with the batch: 1592\n",
      "Done with the batch: 1593\n",
      "Done with the batch: 1594\n",
      "Done with the batch: 1595\n",
      "Done with the batch: 1596\n",
      "Done with the batch: 1597\n",
      "Done with the batch: 1598\n",
      "Done with the batch: 1599\n",
      "Done with the batch: 1600\n",
      "Done with the batch: 1601\n",
      "Done with the batch: 1602\n",
      "Done with the batch: 1603\n",
      "Done with the batch: 1604\n",
      "Done with the batch: 1605\n",
      "Done with the batch: 1606\n",
      "Done with the batch: 1607\n",
      "Done with the batch: 1608\n",
      "Done with the batch: 1609\n",
      "Done with the batch: 1610\n",
      "Done with the batch: 1611\n",
      "Done with the batch: 1612\n",
      "Done with the batch: 1613\n",
      "Done with the batch: 1614\n",
      "Done with the batch: 1615\n",
      "Done with the batch: 1616\n",
      "Done with the batch: 1617\n",
      "(6470, 3136) (6470,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"6a4cfe0f-5691-4dec-b6c7-1856b48d74d9\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"6a4cfe0f-5691-4dec-b6c7-1856b48d74d9\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "X_Train_FeatureMap=np.empty((0,3136))\n",
    "Y_Train_FeatureMap=np.empty((0,batch_size))\n",
    "print(X_Train_FeatureMap.shape)\n",
    "for i,data in enumerate(trainloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    featureMap=net.get_Representation_Net(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Train_FeatureMap=np.append(X_Train_FeatureMap,featureMap,axis=0)\n",
    "    Y_Train_FeatureMap=np.append(Y_Train_FeatureMap,labels.numpy())\n",
    "print(X_Train_FeatureMap.shape,Y_Train_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3136)\n",
      "Done with the batch: 0\n",
      "Done with the batch: 1\n",
      "Done with the batch: 2\n",
      "Done with the batch: 3\n",
      "Done with the batch: 4\n",
      "Done with the batch: 5\n",
      "Done with the batch: 6\n",
      "Done with the batch: 7\n",
      "Done with the batch: 8\n",
      "Done with the batch: 9\n",
      "Done with the batch: 10\n",
      "Done with the batch: 11\n",
      "Done with the batch: 12\n",
      "Done with the batch: 13\n",
      "Done with the batch: 14\n",
      "Done with the batch: 15\n",
      "Done with the batch: 16\n",
      "Done with the batch: 17\n",
      "Done with the batch: 18\n",
      "Done with the batch: 19\n",
      "Done with the batch: 20\n",
      "Done with the batch: 21\n",
      "Done with the batch: 22\n",
      "Done with the batch: 23\n",
      "Done with the batch: 24\n",
      "Done with the batch: 25\n",
      "Done with the batch: 26\n",
      "Done with the batch: 27\n",
      "Done with the batch: 28\n",
      "Done with the batch: 29\n",
      "Done with the batch: 30\n",
      "Done with the batch: 31\n",
      "Done with the batch: 32\n",
      "Done with the batch: 33\n",
      "Done with the batch: 34\n",
      "Done with the batch: 35\n",
      "Done with the batch: 36\n",
      "Done with the batch: 37\n",
      "Done with the batch: 38\n",
      "Done with the batch: 39\n",
      "Done with the batch: 40\n",
      "Done with the batch: 41\n",
      "Done with the batch: 42\n",
      "Done with the batch: 43\n",
      "Done with the batch: 44\n",
      "Done with the batch: 45\n",
      "Done with the batch: 46\n",
      "Done with the batch: 47\n",
      "Done with the batch: 48\n",
      "Done with the batch: 49\n",
      "Done with the batch: 50\n",
      "Done with the batch: 51\n",
      "Done with the batch: 52\n",
      "Done with the batch: 53\n",
      "Done with the batch: 54\n",
      "Done with the batch: 55\n",
      "Done with the batch: 56\n",
      "Done with the batch: 57\n",
      "Done with the batch: 58\n",
      "Done with the batch: 59\n",
      "Done with the batch: 60\n",
      "Done with the batch: 61\n",
      "Done with the batch: 62\n",
      "Done with the batch: 63\n",
      "Done with the batch: 64\n",
      "Done with the batch: 65\n",
      "Done with the batch: 66\n",
      "Done with the batch: 67\n",
      "Done with the batch: 68\n",
      "Done with the batch: 69\n",
      "Done with the batch: 70\n",
      "Done with the batch: 71\n",
      "Done with the batch: 72\n",
      "Done with the batch: 73\n",
      "Done with the batch: 74\n",
      "Done with the batch: 75\n",
      "Done with the batch: 76\n",
      "Done with the batch: 77\n",
      "Done with the batch: 78\n",
      "Done with the batch: 79\n",
      "Done with the batch: 80\n",
      "Done with the batch: 81\n",
      "Done with the batch: 82\n",
      "Done with the batch: 83\n",
      "Done with the batch: 84\n",
      "Done with the batch: 85\n",
      "Done with the batch: 86\n",
      "Done with the batch: 87\n",
      "Done with the batch: 88\n",
      "Done with the batch: 89\n",
      "Done with the batch: 90\n",
      "Done with the batch: 91\n",
      "Done with the batch: 92\n",
      "Done with the batch: 93\n",
      "Done with the batch: 94\n",
      "Done with the batch: 95\n",
      "Done with the batch: 96\n",
      "Done with the batch: 97\n",
      "Done with the batch: 98\n",
      "Done with the batch: 99\n",
      "Done with the batch: 100\n",
      "Done with the batch: 101\n",
      "Done with the batch: 102\n",
      "Done with the batch: 103\n",
      "Done with the batch: 104\n",
      "Done with the batch: 105\n",
      "Done with the batch: 106\n",
      "Done with the batch: 107\n",
      "Done with the batch: 108\n",
      "Done with the batch: 109\n",
      "Done with the batch: 110\n",
      "Done with the batch: 111\n",
      "Done with the batch: 112\n",
      "Done with the batch: 113\n",
      "Done with the batch: 114\n",
      "Done with the batch: 115\n",
      "Done with the batch: 116\n",
      "Done with the batch: 117\n",
      "Done with the batch: 118\n",
      "Done with the batch: 119\n",
      "Done with the batch: 120\n",
      "Done with the batch: 121\n",
      "Done with the batch: 122\n",
      "Done with the batch: 123\n",
      "Done with the batch: 124\n",
      "Done with the batch: 125\n",
      "Done with the batch: 126\n",
      "Done with the batch: 127\n",
      "Done with the batch: 128\n",
      "Done with the batch: 129\n",
      "Done with the batch: 130\n",
      "Done with the batch: 131\n",
      "Done with the batch: 132\n",
      "Done with the batch: 133\n",
      "Done with the batch: 134\n",
      "Done with the batch: 135\n",
      "Done with the batch: 136\n",
      "Done with the batch: 137\n",
      "Done with the batch: 138\n",
      "Done with the batch: 139\n",
      "Done with the batch: 140\n",
      "Done with the batch: 141\n",
      "Done with the batch: 142\n",
      "Done with the batch: 143\n",
      "Done with the batch: 144\n",
      "Done with the batch: 145\n",
      "Done with the batch: 146\n",
      "Done with the batch: 147\n",
      "Done with the batch: 148\n",
      "Done with the batch: 149\n",
      "Done with the batch: 150\n",
      "Done with the batch: 151\n",
      "Done with the batch: 152\n",
      "Done with the batch: 153\n",
      "Done with the batch: 154\n",
      "Done with the batch: 155\n",
      "Done with the batch: 156\n",
      "Done with the batch: 157\n",
      "Done with the batch: 158\n",
      "Done with the batch: 159\n",
      "Done with the batch: 160\n",
      "Done with the batch: 161\n",
      "Done with the batch: 162\n",
      "Done with the batch: 163\n",
      "Done with the batch: 164\n",
      "Done with the batch: 165\n",
      "Done with the batch: 166\n",
      "Done with the batch: 167\n",
      "Done with the batch: 168\n",
      "Done with the batch: 169\n",
      "Done with the batch: 170\n",
      "Done with the batch: 171\n",
      "Done with the batch: 172\n",
      "Done with the batch: 173\n",
      "Done with the batch: 174\n",
      "Done with the batch: 175\n",
      "Done with the batch: 176\n",
      "Done with the batch: 177\n",
      "Done with the batch: 178\n",
      "Done with the batch: 179\n",
      "Done with the batch: 180\n",
      "Done with the batch: 181\n",
      "Done with the batch: 182\n",
      "Done with the batch: 183\n",
      "Done with the batch: 184\n",
      "Done with the batch: 185\n",
      "Done with the batch: 186\n",
      "Done with the batch: 187\n",
      "Done with the batch: 188\n",
      "Done with the batch: 189\n",
      "Done with the batch: 190\n",
      "Done with the batch: 191\n",
      "Done with the batch: 192\n",
      "Done with the batch: 193\n",
      "Done with the batch: 194\n",
      "Done with the batch: 195\n",
      "Done with the batch: 196\n",
      "Done with the batch: 197\n",
      "Done with the batch: 198\n",
      "Done with the batch: 199\n",
      "Done with the batch: 200\n",
      "Done with the batch: 201\n",
      "Done with the batch: 202\n",
      "Done with the batch: 203\n",
      "Done with the batch: 204\n",
      "Done with the batch: 205\n",
      "Done with the batch: 206\n",
      "Done with the batch: 207\n",
      "Done with the batch: 208\n",
      "Done with the batch: 209\n",
      "Done with the batch: 210\n",
      "Done with the batch: 211\n",
      "Done with the batch: 212\n",
      "Done with the batch: 213\n",
      "Done with the batch: 214\n",
      "Done with the batch: 215\n",
      "Done with the batch: 216\n",
      "Done with the batch: 217\n",
      "Done with the batch: 218\n",
      "Done with the batch: 219\n",
      "Done with the batch: 220\n",
      "Done with the batch: 221\n",
      "Done with the batch: 222\n",
      "Done with the batch: 223\n",
      "Done with the batch: 224\n",
      "Done with the batch: 225\n",
      "Done with the batch: 226\n",
      "Done with the batch: 227\n",
      "Done with the batch: 228\n",
      "Done with the batch: 229\n",
      "Done with the batch: 230\n",
      "Done with the batch: 231\n",
      "Done with the batch: 232\n",
      "Done with the batch: 233\n",
      "Done with the batch: 234\n",
      "Done with the batch: 235\n",
      "Done with the batch: 236\n",
      "Done with the batch: 237\n",
      "Done with the batch: 238\n",
      "Done with the batch: 239\n",
      "Done with the batch: 240\n",
      "Done with the batch: 241\n",
      "Done with the batch: 242\n",
      "Done with the batch: 243\n",
      "Done with the batch: 244\n",
      "Done with the batch: 245\n",
      "Done with the batch: 246\n",
      "Done with the batch: 247\n",
      "Done with the batch: 248\n",
      "Done with the batch: 249\n",
      "Done with the batch: 250\n",
      "Done with the batch: 251\n",
      "Done with the batch: 252\n",
      "Done with the batch: 253\n",
      "Done with the batch: 254\n",
      "Done with the batch: 255\n",
      "Done with the batch: 256\n",
      "Done with the batch: 257\n",
      "Done with the batch: 258\n",
      "Done with the batch: 259\n",
      "Done with the batch: 260\n",
      "Done with the batch: 261\n",
      "Done with the batch: 262\n",
      "Done with the batch: 263\n",
      "Done with the batch: 264\n",
      "Done with the batch: 265\n",
      "Done with the batch: 266\n",
      "Done with the batch: 267\n",
      "Done with the batch: 268\n",
      "Done with the batch: 269\n",
      "Done with the batch: 270\n",
      "Done with the batch: 271\n",
      "Done with the batch: 272\n",
      "Done with the batch: 273\n",
      "Done with the batch: 274\n",
      "Done with the batch: 275\n",
      "Done with the batch: 276\n",
      "Done with the batch: 277\n",
      "Done with the batch: 278\n",
      "Done with the batch: 279\n",
      "Done with the batch: 280\n",
      "Done with the batch: 281\n",
      "Done with the batch: 282\n",
      "Done with the batch: 283\n",
      "Done with the batch: 284\n",
      "Done with the batch: 285\n",
      "Done with the batch: 286\n",
      "Done with the batch: 287\n",
      "Done with the batch: 288\n",
      "Done with the batch: 289\n",
      "Done with the batch: 290\n",
      "Done with the batch: 291\n",
      "Done with the batch: 292\n",
      "Done with the batch: 293\n",
      "Done with the batch: 294\n",
      "Done with the batch: 295\n",
      "Done with the batch: 296\n",
      "Done with the batch: 297\n",
      "Done with the batch: 298\n",
      "Done with the batch: 299\n",
      "Done with the batch: 300\n",
      "Done with the batch: 301\n",
      "Done with the batch: 302\n",
      "Done with the batch: 303\n",
      "Done with the batch: 304\n",
      "Done with the batch: 305\n",
      "Done with the batch: 306\n",
      "Done with the batch: 307\n",
      "Done with the batch: 308\n",
      "Done with the batch: 309\n",
      "Done with the batch: 310\n",
      "Done with the batch: 311\n",
      "Done with the batch: 312\n",
      "Done with the batch: 313\n",
      "Done with the batch: 314\n",
      "Done with the batch: 315\n",
      "Done with the batch: 316\n",
      "Done with the batch: 317\n",
      "Done with the batch: 318\n",
      "Done with the batch: 319\n",
      "Done with the batch: 320\n",
      "Done with the batch: 321\n",
      "Done with the batch: 322\n",
      "Done with the batch: 323\n",
      "Done with the batch: 324\n",
      "Done with the batch: 325\n",
      "Done with the batch: 326\n",
      "Done with the batch: 327\n",
      "Done with the batch: 328\n",
      "Done with the batch: 329\n",
      "Done with the batch: 330\n",
      "Done with the batch: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the batch: 332\n",
      "Done with the batch: 333\n",
      "Done with the batch: 334\n",
      "Done with the batch: 335\n",
      "Done with the batch: 336\n",
      "Done with the batch: 337\n",
      "Done with the batch: 338\n",
      "Done with the batch: 339\n",
      "Done with the batch: 340\n",
      "Done with the batch: 341\n",
      "Done with the batch: 342\n",
      "Done with the batch: 343\n",
      "Done with the batch: 344\n",
      "Done with the batch: 345\n",
      "Done with the batch: 346\n",
      "Done with the batch: 347\n",
      "Done with the batch: 348\n",
      "Done with the batch: 349\n",
      "Done with the batch: 350\n",
      "Done with the batch: 351\n",
      "Done with the batch: 352\n",
      "Done with the batch: 353\n",
      "Done with the batch: 354\n",
      "Done with the batch: 355\n",
      "Done with the batch: 356\n",
      "Done with the batch: 357\n",
      "Done with the batch: 358\n",
      "Done with the batch: 359\n",
      "Done with the batch: 360\n",
      "Done with the batch: 361\n",
      "Done with the batch: 362\n",
      "Done with the batch: 363\n",
      "Done with the batch: 364\n",
      "Done with the batch: 365\n",
      "Done with the batch: 366\n",
      "Done with the batch: 367\n",
      "Done with the batch: 368\n",
      "Done with the batch: 369\n",
      "Done with the batch: 370\n",
      "Done with the batch: 371\n",
      "Done with the batch: 372\n",
      "Done with the batch: 373\n",
      "Done with the batch: 374\n",
      "Done with the batch: 375\n",
      "Done with the batch: 376\n",
      "Done with the batch: 377\n",
      "Done with the batch: 378\n",
      "Done with the batch: 379\n",
      "Done with the batch: 380\n",
      "Done with the batch: 381\n",
      "Done with the batch: 382\n",
      "Done with the batch: 383\n",
      "Done with the batch: 384\n",
      "Done with the batch: 385\n",
      "Done with the batch: 386\n",
      "Done with the batch: 387\n",
      "Done with the batch: 388\n",
      "Done with the batch: 389\n",
      "Done with the batch: 390\n",
      "Done with the batch: 391\n",
      "Done with the batch: 392\n",
      "Done with the batch: 393\n",
      "Done with the batch: 394\n",
      "Done with the batch: 395\n",
      "Done with the batch: 396\n",
      "Done with the batch: 397\n",
      "Done with the batch: 398\n",
      "Done with the batch: 399\n",
      "Done with the batch: 400\n",
      "Done with the batch: 401\n",
      "Done with the batch: 402\n",
      "Done with the batch: 403\n",
      "Done with the batch: 404\n",
      "(1618, 3136) (1618,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"3bb7287d-5538-46ec-aefa-6425a0871df7\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"3bb7287d-5538-46ec-aefa-6425a0871df7\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "X_Test_FeatureMap=np.empty((0,3136))\n",
    "Y_Test_FeatureMap=np.empty((0,batch_size))\n",
    "print(X_Test_FeatureMap.shape)\n",
    "for i,data in enumerate(testloader):\n",
    "    print(f'Done with the batch: {i}')\n",
    "    images,labels=data\n",
    "    featuremap=net.get_Representation_Net(images).detach().numpy();\n",
    "#     print(FCLayer,FCLayer.shape,labels.numpy())\n",
    "    X_Test_FeatureMap=np.append(X_Test_FeatureMap,featuremap,axis=0)\n",
    "    Y_Test_FeatureMap=np.append(Y_Test_FeatureMap,labels.numpy())\n",
    "print(X_Test_FeatureMap.shape,Y_Test_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV 1/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n",
      "[CV 1/5; 1/25] END C=0.001, gamma=0.001, kernel=rbf;, score=0.805 total time=  14.1s\n",
      "[CV 2/5; 1/25] START C=0.001, gamma=0.001, kernel=rbf...........................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-88efdca743eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuned_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_Train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror_score\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mScore\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0mon\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \"\"\"\n\u001b[1;32m--> 199\u001b[1;33m         return self._score(partial(_cached_call, None), estimator, X, y_true,\n\u001b[0m\u001b[0;32m    200\u001b[0m                            sample_weight=sample_weight)\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    622\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0msvm_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLIBSVM_IMPL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m         return libsvm.predict(\n\u001b[0m\u001b[0;32m    362\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dual_coef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_intercept_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuned_parameters = {'kernel': ['rbf'], 'gamma': [1e-3, 1e-2,0.1,1,10],\n",
    "                     'C': [0.001,0.1,1, 10, 100],\n",
    "}\n",
    "# tuned_parameters = {'kernel': ['rbf'], 'gamma': [1e-3],\n",
    "#                      'C': [0.001],\n",
    "#                    }\n",
    "clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring= 'accuracy',verbose=10\n",
    "    )\n",
    "clf.fit(X_Train, Y_Train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_svm(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        clf=SVC(C=10,kernel='rbf',gamma=0.001)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del clf\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 95.5177743431221\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 95.13137557959814\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 95.20865533230294\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 94.51313755795981\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 95.6723338485317\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 95.5177743431221 %\n",
      "Fold 1: 95.13137557959814 %\n",
      "Fold 2: 95.20865533230294 %\n",
      "Fold 3: 94.51313755795981 %\n",
      "Fold 4: 95.6723338485317 %\n",
      "Average: 95.20865533230294 %\n",
      "Accuracy:  0.9283065512978986\n",
      "Precision:  0.921875\n",
      "Recall:  0.9376528117359413\n",
      "F1-Score:  0.9296969696969697\n",
      "AUC:  0.9282014058679706\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_svm(X_Train,Y_Train.ravel())\n",
    "y_pred=clf.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test,y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test,y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test,y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test,y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_dtree(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        decision_tree = DecisionTreeClassifier(random_state=102)\n",
    "        decision_tree = decision_tree.fit(X_train, y_train.ravel())\n",
    "        y_pred = decision_tree.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del decision_tree\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 90.72642967542504\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 91.57650695517773\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 91.19010819165379\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 90.49459041731066\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 91.42194744976815\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 90.72642967542504 %\n",
      "Fold 1: 91.57650695517773 %\n",
      "Fold 2: 91.19010819165379 %\n",
      "Fold 3: 90.49459041731066 %\n",
      "Fold 4: 91.42194744976815 %\n",
      "Average: 91.08191653786707 %\n",
      "Accuracy:  0.892459826946848\n",
      "Precision:  0.8936430317848411\n",
      "Recall:  0.8936430317848411\n",
      "F1-Score:  0.8936430317848411\n",
      "AUC:  0.8924465158924205\n"
     ]
    }
   ],
   "source": [
    "dtree=k_fold_cv_dtree(X_Train,Y_Train.ravel())\n",
    "y_pred=dtree.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test,y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test,y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test,y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test,y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_rforest(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        random_forest = RandomForestClassifier(n_estimators=100,criterion='gini',random_state=102)\n",
    "        random_forest = random_forest.fit(X_train, y_train.ravel())\n",
    "        y_pred = random_forest.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del random_forest\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 94.43585780525503\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 95.5950540958269\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 94.97681607418856\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 93.97217928902627\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 94.82225656877898\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 94.43585780525503 %\n",
      "Fold 1: 95.5950540958269 %\n",
      "Fold 2: 94.97681607418856 %\n",
      "Fold 3: 93.97217928902627 %\n",
      "Fold 4: 94.82225656877898 %\n",
      "Average: 94.76043276661514 %\n",
      "Accuracy:  0.9128553770086527\n",
      "Precision:  0.9034564958283671\n",
      "Recall:  0.9266503667481663\n",
      "F1-Score:  0.9149064574532287\n",
      "AUC:  0.9127001833740831\n"
     ]
    }
   ],
   "source": [
    "random_forest=k_fold_cv_rforest(X_Train,Y_Train.ravel())\n",
    "y_pred=random_forest.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test,y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test,y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test,y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test,y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_xgb(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        eval_set = [(X_train, y_train.ravel()), (X_test, y_test)]\n",
    "        xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=102,use_label_encoder=False)\n",
    "        xg_cl.fit(X_train,y_train.ravel())\n",
    "        y_pred = xg_cl.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del xg_cl\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return xg_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "[19:39:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 94.43585780525503\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "[19:39:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 94.20401854714065\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "[19:39:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 94.74497681607419\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "[19:40:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 94.51313755795981\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "[19:40:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 95.90417310664606\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 94.43585780525503 %\n",
      "Fold 1: 94.20401854714065 %\n",
      "Fold 2: 94.74497681607419 %\n",
      "Fold 3: 94.51313755795981 %\n",
      "Fold 4: 95.90417310664606 %\n",
      "Average: 94.76043276661514 %\n",
      "Accuracy:  0.9128553770086527\n",
      "Precision:  0.8968347010550997\n",
      "Recall:  0.9352078239608802\n",
      "F1-Score:  0.9156193895870736\n",
      "AUC:  0.91260391198044\n"
     ]
    }
   ],
   "source": [
    "xg=k_fold_cv_xgb(X_Train,Y_Train.ravel())\n",
    "y_pred=xg.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test,y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test,y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test,y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test,y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_mlp(X,Y,k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        X_train, X_test = X[train_ids], X[test_ids]\n",
    "        y_train, y_test = Y[train_ids], Y[test_ids]\n",
    "        clf = MLPClassifier(random_state=102, max_iter=3000, verbose=True).fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results[fold] = 100.0 * accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\",results[fold])\n",
    "        if fold != k_folds-1:\n",
    "            # The last model used for testing accuracy\n",
    "            del clf\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.29601974\n",
      "Iteration 2, loss = 0.20136104\n",
      "Iteration 3, loss = 0.17591851\n",
      "Iteration 4, loss = 0.16658832\n",
      "Iteration 5, loss = 0.15833531\n",
      "Iteration 6, loss = 0.15224243\n",
      "Iteration 7, loss = 0.14670272\n",
      "Iteration 8, loss = 0.14397531\n",
      "Iteration 9, loss = 0.15224828\n",
      "Iteration 10, loss = 0.15500380\n",
      "Iteration 11, loss = 0.13527895\n",
      "Iteration 12, loss = 0.13363870\n",
      "Iteration 13, loss = 0.14722962\n",
      "Iteration 14, loss = 0.13142271\n",
      "Iteration 15, loss = 0.13059529\n",
      "Iteration 16, loss = 0.13672617\n",
      "Iteration 17, loss = 0.12969524\n",
      "Iteration 18, loss = 0.12768176\n",
      "Iteration 19, loss = 0.12261944\n",
      "Iteration 20, loss = 0.11968651\n",
      "Iteration 21, loss = 0.12114854\n",
      "Iteration 22, loss = 0.12310863\n",
      "Iteration 23, loss = 0.12403805\n",
      "Iteration 24, loss = 0.12107663\n",
      "Iteration 25, loss = 0.12199663\n",
      "Iteration 26, loss = 0.11184237\n",
      "Iteration 27, loss = 0.11220169\n",
      "Iteration 28, loss = 0.11193331\n",
      "Iteration 29, loss = 0.10898876\n",
      "Iteration 30, loss = 0.10745840\n",
      "Iteration 31, loss = 0.10737562\n",
      "Iteration 32, loss = 0.10714768\n",
      "Iteration 33, loss = 0.10917937\n",
      "Iteration 34, loss = 0.10283271\n",
      "Iteration 35, loss = 0.10666144\n",
      "Iteration 36, loss = 0.10971380\n",
      "Iteration 37, loss = 0.10307921\n",
      "Iteration 38, loss = 0.09964225\n",
      "Iteration 39, loss = 0.10850620\n",
      "Iteration 40, loss = 0.10396669\n",
      "Iteration 41, loss = 0.10063060\n",
      "Iteration 42, loss = 0.10042495\n",
      "Iteration 43, loss = 0.10622336\n",
      "Iteration 44, loss = 0.09820771\n",
      "Iteration 45, loss = 0.10590313\n",
      "Iteration 46, loss = 0.09718213\n",
      "Iteration 47, loss = 0.09298337\n",
      "Iteration 48, loss = 0.10192841\n",
      "Iteration 49, loss = 0.09649269\n",
      "Iteration 50, loss = 0.09360400\n",
      "Iteration 51, loss = 0.09626199\n",
      "Iteration 52, loss = 0.09477176\n",
      "Iteration 53, loss = 0.09304361\n",
      "Iteration 54, loss = 0.09580164\n",
      "Iteration 55, loss = 0.09634425\n",
      "Iteration 56, loss = 0.09188349\n",
      "Iteration 57, loss = 0.08809390\n",
      "Iteration 58, loss = 0.09024679\n",
      "Iteration 59, loss = 0.08840600\n",
      "Iteration 60, loss = 0.09016141\n",
      "Iteration 61, loss = 0.09505102\n",
      "Iteration 62, loss = 0.08388558\n",
      "Iteration 63, loss = 0.08479608\n",
      "Iteration 64, loss = 0.09210695\n",
      "Iteration 65, loss = 0.09214816\n",
      "Iteration 66, loss = 0.08868502\n",
      "Iteration 67, loss = 0.08596820\n",
      "Iteration 68, loss = 0.08454571\n",
      "Iteration 69, loss = 0.08290937\n",
      "Iteration 70, loss = 0.07958931\n",
      "Iteration 71, loss = 0.07780990\n",
      "Iteration 72, loss = 0.07686014\n",
      "Iteration 73, loss = 0.07930896\n",
      "Iteration 74, loss = 0.08463624\n",
      "Iteration 75, loss = 0.07814608\n",
      "Iteration 76, loss = 0.08201246\n",
      "Iteration 77, loss = 0.08815924\n",
      "Iteration 78, loss = 0.07888803\n",
      "Iteration 79, loss = 0.08225847\n",
      "Iteration 80, loss = 0.08605176\n",
      "Iteration 81, loss = 0.07454362\n",
      "Iteration 82, loss = 0.07402161\n",
      "Iteration 83, loss = 0.08205302\n",
      "Iteration 84, loss = 0.08551149\n",
      "Iteration 85, loss = 0.07832948\n",
      "Iteration 86, loss = 0.07289576\n",
      "Iteration 87, loss = 0.07179678\n",
      "Iteration 88, loss = 0.07772203\n",
      "Iteration 89, loss = 0.07424342\n",
      "Iteration 90, loss = 0.07075850\n",
      "Iteration 91, loss = 0.07096969\n",
      "Iteration 92, loss = 0.07012419\n",
      "Iteration 93, loss = 0.08238165\n",
      "Iteration 94, loss = 0.07955512\n",
      "Iteration 95, loss = 0.06748735\n",
      "Iteration 96, loss = 0.06750472\n",
      "Iteration 97, loss = 0.06812685\n",
      "Iteration 98, loss = 0.06831214\n",
      "Iteration 99, loss = 0.07339119\n",
      "Iteration 100, loss = 0.06730835\n",
      "Iteration 101, loss = 0.06626743\n",
      "Iteration 102, loss = 0.07046671\n",
      "Iteration 103, loss = 0.07661577\n",
      "Iteration 104, loss = 0.06716950\n",
      "Iteration 105, loss = 0.06630931\n",
      "Iteration 106, loss = 0.06631022\n",
      "Iteration 107, loss = 0.06559096\n",
      "Iteration 108, loss = 0.07035526\n",
      "Iteration 109, loss = 0.06794023\n",
      "Iteration 110, loss = 0.06519405\n",
      "Iteration 111, loss = 0.06834982\n",
      "Iteration 112, loss = 0.07029806\n",
      "Iteration 113, loss = 0.06809078\n",
      "Iteration 114, loss = 0.06930114\n",
      "Iteration 115, loss = 0.06596367\n",
      "Iteration 116, loss = 0.06172171\n",
      "Iteration 117, loss = 0.06464577\n",
      "Iteration 118, loss = 0.06241470\n",
      "Iteration 119, loss = 0.06770860\n",
      "Iteration 120, loss = 0.07071227\n",
      "Iteration 121, loss = 0.06978612\n",
      "Iteration 122, loss = 0.06707184\n",
      "Iteration 123, loss = 0.06312559\n",
      "Iteration 124, loss = 0.06025097\n",
      "Iteration 125, loss = 0.05873500\n",
      "Iteration 126, loss = 0.05653404\n",
      "Iteration 127, loss = 0.05869600\n",
      "Iteration 128, loss = 0.05590644\n",
      "Iteration 129, loss = 0.06014544\n",
      "Iteration 130, loss = 0.06709455\n",
      "Iteration 131, loss = 0.05800775\n",
      "Iteration 132, loss = 0.05398841\n",
      "Iteration 133, loss = 0.05457874\n",
      "Iteration 134, loss = 0.06149604\n",
      "Iteration 135, loss = 0.05688020\n",
      "Iteration 136, loss = 0.06048474\n",
      "Iteration 137, loss = 0.05726353\n",
      "Iteration 138, loss = 0.05820323\n",
      "Iteration 139, loss = 0.05845081\n",
      "Iteration 140, loss = 0.05989192\n",
      "Iteration 141, loss = 0.05595586\n",
      "Iteration 142, loss = 0.05149265\n",
      "Iteration 143, loss = 0.05559547\n",
      "Iteration 144, loss = 0.05839010\n",
      "Iteration 145, loss = 0.05933544\n",
      "Iteration 146, loss = 0.05305795\n",
      "Iteration 147, loss = 0.05935451\n",
      "Iteration 148, loss = 0.05465379\n",
      "Iteration 149, loss = 0.04993322\n",
      "Iteration 150, loss = 0.05513060\n",
      "Iteration 151, loss = 0.05439208\n",
      "Iteration 152, loss = 0.05484642\n",
      "Iteration 153, loss = 0.05500443\n",
      "Iteration 154, loss = 0.06377554\n",
      "Iteration 155, loss = 0.04830252\n",
      "Iteration 156, loss = 0.05700588\n",
      "Iteration 157, loss = 0.05477101\n",
      "Iteration 158, loss = 0.05197509\n",
      "Iteration 159, loss = 0.05170530\n",
      "Iteration 160, loss = 0.05326384\n",
      "Iteration 161, loss = 0.05236906\n",
      "Iteration 162, loss = 0.04970820\n",
      "Iteration 163, loss = 0.06056227\n",
      "Iteration 164, loss = 0.05194976\n",
      "Iteration 165, loss = 0.04919517\n",
      "Iteration 166, loss = 0.04713687\n",
      "Iteration 167, loss = 0.05665348\n",
      "Iteration 168, loss = 0.05407026\n",
      "Iteration 169, loss = 0.04805715\n",
      "Iteration 170, loss = 0.04737743\n",
      "Iteration 171, loss = 0.05486203\n",
      "Iteration 172, loss = 0.05641270\n",
      "Iteration 173, loss = 0.04582339\n",
      "Iteration 174, loss = 0.04890083\n",
      "Iteration 175, loss = 0.05040636\n",
      "Iteration 176, loss = 0.04906697\n",
      "Iteration 177, loss = 0.04797566\n",
      "Iteration 178, loss = 0.04802122\n",
      "Iteration 179, loss = 0.06535335\n",
      "Iteration 180, loss = 0.05086756\n",
      "Iteration 181, loss = 0.04668497\n",
      "Iteration 182, loss = 0.04803918\n",
      "Iteration 183, loss = 0.04678483\n",
      "Iteration 184, loss = 0.05261840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 94.20401854714065\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.36065139\n",
      "Iteration 2, loss = 0.20896246\n",
      "Iteration 3, loss = 0.17785358\n",
      "Iteration 4, loss = 0.16571331\n",
      "Iteration 5, loss = 0.15806907\n",
      "Iteration 6, loss = 0.15289943\n",
      "Iteration 7, loss = 0.15270467\n",
      "Iteration 8, loss = 0.14680187\n",
      "Iteration 9, loss = 0.14135271\n",
      "Iteration 10, loss = 0.14696412\n",
      "Iteration 11, loss = 0.14358073\n",
      "Iteration 12, loss = 0.13669580\n",
      "Iteration 13, loss = 0.12807544\n",
      "Iteration 14, loss = 0.12624074\n",
      "Iteration 15, loss = 0.12550036\n",
      "Iteration 16, loss = 0.13597242\n",
      "Iteration 17, loss = 0.12824728\n",
      "Iteration 18, loss = 0.12505119\n",
      "Iteration 19, loss = 0.12818844\n",
      "Iteration 20, loss = 0.11847632\n",
      "Iteration 21, loss = 0.11481008\n",
      "Iteration 22, loss = 0.11504962\n",
      "Iteration 23, loss = 0.11382977\n",
      "Iteration 24, loss = 0.11373706\n",
      "Iteration 25, loss = 0.11499462\n",
      "Iteration 26, loss = 0.11938899\n",
      "Iteration 27, loss = 0.11214659\n",
      "Iteration 28, loss = 0.10823432\n",
      "Iteration 29, loss = 0.10788608\n",
      "Iteration 30, loss = 0.10839125\n",
      "Iteration 31, loss = 0.11893906\n",
      "Iteration 32, loss = 0.10950837\n",
      "Iteration 33, loss = 0.11096807\n",
      "Iteration 34, loss = 0.10426066\n",
      "Iteration 35, loss = 0.10222158\n",
      "Iteration 36, loss = 0.10107446\n",
      "Iteration 37, loss = 0.10046224\n",
      "Iteration 38, loss = 0.09963622\n",
      "Iteration 39, loss = 0.10091607\n",
      "Iteration 40, loss = 0.10280841\n",
      "Iteration 41, loss = 0.10087259\n",
      "Iteration 42, loss = 0.10044662\n",
      "Iteration 43, loss = 0.10161006\n",
      "Iteration 44, loss = 0.10335272\n",
      "Iteration 45, loss = 0.10310435\n",
      "Iteration 46, loss = 0.09964629\n",
      "Iteration 47, loss = 0.09475587\n",
      "Iteration 48, loss = 0.09945790\n",
      "Iteration 49, loss = 0.10275297\n",
      "Iteration 50, loss = 0.09213612\n",
      "Iteration 51, loss = 0.09282989\n",
      "Iteration 52, loss = 0.09595358\n",
      "Iteration 53, loss = 0.08947167\n",
      "Iteration 54, loss = 0.09670490\n",
      "Iteration 55, loss = 0.09357319\n",
      "Iteration 56, loss = 0.09228989\n",
      "Iteration 57, loss = 0.09663704\n",
      "Iteration 58, loss = 0.08851172\n",
      "Iteration 59, loss = 0.09270510\n",
      "Iteration 60, loss = 0.09520368\n",
      "Iteration 61, loss = 0.08579253\n",
      "Iteration 62, loss = 0.10840653\n",
      "Iteration 63, loss = 0.09191652\n",
      "Iteration 64, loss = 0.08899853\n",
      "Iteration 65, loss = 0.08613117\n",
      "Iteration 66, loss = 0.08743488\n",
      "Iteration 67, loss = 0.08360192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 0.08943896\n",
      "Iteration 69, loss = 0.08475920\n",
      "Iteration 70, loss = 0.08876705\n",
      "Iteration 71, loss = 0.09085463\n",
      "Iteration 72, loss = 0.07939229\n",
      "Iteration 73, loss = 0.08096552\n",
      "Iteration 74, loss = 0.07829114\n",
      "Iteration 75, loss = 0.08065821\n",
      "Iteration 76, loss = 0.07788832\n",
      "Iteration 77, loss = 0.08087782\n",
      "Iteration 78, loss = 0.08110012\n",
      "Iteration 79, loss = 0.07780745\n",
      "Iteration 80, loss = 0.08140508\n",
      "Iteration 81, loss = 0.08664764\n",
      "Iteration 82, loss = 0.08151386\n",
      "Iteration 83, loss = 0.07708595\n",
      "Iteration 84, loss = 0.07783908\n",
      "Iteration 85, loss = 0.07827617\n",
      "Iteration 86, loss = 0.07374880\n",
      "Iteration 87, loss = 0.07714086\n",
      "Iteration 88, loss = 0.08070334\n",
      "Iteration 89, loss = 0.07359222\n",
      "Iteration 90, loss = 0.07423508\n",
      "Iteration 91, loss = 0.07824375\n",
      "Iteration 92, loss = 0.07522846\n",
      "Iteration 93, loss = 0.07392128\n",
      "Iteration 94, loss = 0.08096487\n",
      "Iteration 95, loss = 0.08029955\n",
      "Iteration 96, loss = 0.07200145\n",
      "Iteration 97, loss = 0.06817234\n",
      "Iteration 98, loss = 0.06955148\n",
      "Iteration 99, loss = 0.07615538\n",
      "Iteration 100, loss = 0.07522455\n",
      "Iteration 101, loss = 0.06792183\n",
      "Iteration 102, loss = 0.07088666\n",
      "Iteration 103, loss = 0.06606546\n",
      "Iteration 104, loss = 0.06911730\n",
      "Iteration 105, loss = 0.07769720\n",
      "Iteration 106, loss = 0.06820075\n",
      "Iteration 107, loss = 0.07089440\n",
      "Iteration 108, loss = 0.07137255\n",
      "Iteration 109, loss = 0.07016912\n",
      "Iteration 110, loss = 0.06807434\n",
      "Iteration 111, loss = 0.06955098\n",
      "Iteration 112, loss = 0.06760351\n",
      "Iteration 113, loss = 0.06617134\n",
      "Iteration 114, loss = 0.06200880\n",
      "Iteration 115, loss = 0.06571088\n",
      "Iteration 116, loss = 0.06506492\n",
      "Iteration 117, loss = 0.06594132\n",
      "Iteration 118, loss = 0.06271283\n",
      "Iteration 119, loss = 0.06357403\n",
      "Iteration 120, loss = 0.06305942\n",
      "Iteration 121, loss = 0.05874162\n",
      "Iteration 122, loss = 0.06586786\n",
      "Iteration 123, loss = 0.06257437\n",
      "Iteration 124, loss = 0.06349706\n",
      "Iteration 125, loss = 0.06332685\n",
      "Iteration 126, loss = 0.06356522\n",
      "Iteration 127, loss = 0.06253197\n",
      "Iteration 128, loss = 0.05784278\n",
      "Iteration 129, loss = 0.06055784\n",
      "Iteration 130, loss = 0.05804417\n",
      "Iteration 131, loss = 0.05858962\n",
      "Iteration 132, loss = 0.05559964\n",
      "Iteration 133, loss = 0.05887835\n",
      "Iteration 134, loss = 0.06181779\n",
      "Iteration 135, loss = 0.05620991\n",
      "Iteration 136, loss = 0.05663256\n",
      "Iteration 137, loss = 0.06074635\n",
      "Iteration 138, loss = 0.05670109\n",
      "Iteration 139, loss = 0.05648847\n",
      "Iteration 140, loss = 0.05713184\n",
      "Iteration 141, loss = 0.05700570\n",
      "Iteration 142, loss = 0.06246475\n",
      "Iteration 143, loss = 0.05432496\n",
      "Iteration 144, loss = 0.05509072\n",
      "Iteration 145, loss = 0.05588268\n",
      "Iteration 146, loss = 0.05466115\n",
      "Iteration 147, loss = 0.05288120\n",
      "Iteration 148, loss = 0.05718986\n",
      "Iteration 149, loss = 0.05948193\n",
      "Iteration 150, loss = 0.06021334\n",
      "Iteration 151, loss = 0.05818342\n",
      "Iteration 152, loss = 0.05851660\n",
      "Iteration 153, loss = 0.05586118\n",
      "Iteration 154, loss = 0.05349154\n",
      "Iteration 155, loss = 0.05050623\n",
      "Iteration 156, loss = 0.04913238\n",
      "Iteration 157, loss = 0.05223628\n",
      "Iteration 158, loss = 0.05248346\n",
      "Iteration 159, loss = 0.05778499\n",
      "Iteration 160, loss = 0.05389315\n",
      "Iteration 161, loss = 0.04704805\n",
      "Iteration 162, loss = 0.05285850\n",
      "Iteration 163, loss = 0.05408851\n",
      "Iteration 164, loss = 0.04920637\n",
      "Iteration 165, loss = 0.04821002\n",
      "Iteration 166, loss = 0.04875303\n",
      "Iteration 167, loss = 0.05314522\n",
      "Iteration 168, loss = 0.04669781\n",
      "Iteration 169, loss = 0.04997191\n",
      "Iteration 170, loss = 0.04913407\n",
      "Iteration 171, loss = 0.04711458\n",
      "Iteration 172, loss = 0.05271811\n",
      "Iteration 173, loss = 0.04528867\n",
      "Iteration 174, loss = 0.04578741\n",
      "Iteration 175, loss = 0.04563868\n",
      "Iteration 176, loss = 0.04918788\n",
      "Iteration 177, loss = 0.04971375\n",
      "Iteration 178, loss = 0.04404158\n",
      "Iteration 179, loss = 0.04903106\n",
      "Iteration 180, loss = 0.05344100\n",
      "Iteration 181, loss = 0.05830250\n",
      "Iteration 182, loss = 0.04736998\n",
      "Iteration 183, loss = 0.04890019\n",
      "Iteration 184, loss = 0.04447066\n",
      "Iteration 185, loss = 0.04534219\n",
      "Iteration 186, loss = 0.04498892\n",
      "Iteration 187, loss = 0.04346267\n",
      "Iteration 188, loss = 0.04529613\n",
      "Iteration 189, loss = 0.04414139\n",
      "Iteration 190, loss = 0.04195596\n",
      "Iteration 191, loss = 0.04451919\n",
      "Iteration 192, loss = 0.04429124\n",
      "Iteration 193, loss = 0.04341749\n",
      "Iteration 194, loss = 0.04511367\n",
      "Iteration 195, loss = 0.04370893\n",
      "Iteration 196, loss = 0.04239170\n",
      "Iteration 197, loss = 0.04417498\n",
      "Iteration 198, loss = 0.04489826\n",
      "Iteration 199, loss = 0.04993888\n",
      "Iteration 200, loss = 0.05664901\n",
      "Iteration 201, loss = 0.05928193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 93.89489953632149\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.30977687\n",
      "Iteration 2, loss = 0.20322390\n",
      "Iteration 3, loss = 0.18132852\n",
      "Iteration 4, loss = 0.16956239\n",
      "Iteration 5, loss = 0.17034830\n",
      "Iteration 6, loss = 0.15794550\n",
      "Iteration 7, loss = 0.14952152\n",
      "Iteration 8, loss = 0.14398102\n",
      "Iteration 9, loss = 0.14057002\n",
      "Iteration 10, loss = 0.13967703\n",
      "Iteration 11, loss = 0.13617344\n",
      "Iteration 12, loss = 0.13724615\n",
      "Iteration 13, loss = 0.14072470\n",
      "Iteration 14, loss = 0.13594506\n",
      "Iteration 15, loss = 0.13694246\n",
      "Iteration 16, loss = 0.13549663\n",
      "Iteration 17, loss = 0.13055062\n",
      "Iteration 18, loss = 0.13130233\n",
      "Iteration 19, loss = 0.12529164\n",
      "Iteration 20, loss = 0.12480913\n",
      "Iteration 21, loss = 0.12744256\n",
      "Iteration 22, loss = 0.13359957\n",
      "Iteration 23, loss = 0.11926747\n",
      "Iteration 24, loss = 0.11528308\n",
      "Iteration 25, loss = 0.11705049\n",
      "Iteration 26, loss = 0.11486520\n",
      "Iteration 27, loss = 0.11727210\n",
      "Iteration 28, loss = 0.11155706\n",
      "Iteration 29, loss = 0.11247221\n",
      "Iteration 30, loss = 0.11119187\n",
      "Iteration 31, loss = 0.11139560\n",
      "Iteration 32, loss = 0.11259390\n",
      "Iteration 33, loss = 0.11070959\n",
      "Iteration 34, loss = 0.10636712\n",
      "Iteration 35, loss = 0.10677844\n",
      "Iteration 36, loss = 0.11167954\n",
      "Iteration 37, loss = 0.10931904\n",
      "Iteration 38, loss = 0.10441061\n",
      "Iteration 39, loss = 0.10595437\n",
      "Iteration 40, loss = 0.10878674\n",
      "Iteration 41, loss = 0.10110443\n",
      "Iteration 42, loss = 0.09872797\n",
      "Iteration 43, loss = 0.10076508\n",
      "Iteration 44, loss = 0.09936304\n",
      "Iteration 45, loss = 0.10163740\n",
      "Iteration 46, loss = 0.10504885\n",
      "Iteration 47, loss = 0.10344524\n",
      "Iteration 48, loss = 0.09657161\n",
      "Iteration 49, loss = 0.10271400\n",
      "Iteration 50, loss = 0.09508964\n",
      "Iteration 51, loss = 0.09906823\n",
      "Iteration 52, loss = 0.10082271\n",
      "Iteration 53, loss = 0.10167431\n",
      "Iteration 54, loss = 0.10369663\n",
      "Iteration 55, loss = 0.09554124\n",
      "Iteration 56, loss = 0.10133247\n",
      "Iteration 57, loss = 0.09519022\n",
      "Iteration 58, loss = 0.09241270\n",
      "Iteration 59, loss = 0.09027357\n",
      "Iteration 60, loss = 0.09122396\n",
      "Iteration 61, loss = 0.08626583\n",
      "Iteration 62, loss = 0.08854544\n",
      "Iteration 63, loss = 0.09960671\n",
      "Iteration 64, loss = 0.09320968\n",
      "Iteration 65, loss = 0.08752687\n",
      "Iteration 66, loss = 0.08650323\n",
      "Iteration 67, loss = 0.08787433\n",
      "Iteration 68, loss = 0.08591231\n",
      "Iteration 69, loss = 0.08504404\n",
      "Iteration 70, loss = 0.09828739\n",
      "Iteration 71, loss = 0.09001889\n",
      "Iteration 72, loss = 0.08551079\n",
      "Iteration 73, loss = 0.08749116\n",
      "Iteration 74, loss = 0.08701421\n",
      "Iteration 75, loss = 0.08258859\n",
      "Iteration 76, loss = 0.08079822\n",
      "Iteration 77, loss = 0.08121143\n",
      "Iteration 78, loss = 0.08711484\n",
      "Iteration 79, loss = 0.07650295\n",
      "Iteration 80, loss = 0.07755840\n",
      "Iteration 81, loss = 0.08043832\n",
      "Iteration 82, loss = 0.07871953\n",
      "Iteration 83, loss = 0.08113874\n",
      "Iteration 84, loss = 0.08387617\n",
      "Iteration 85, loss = 0.07549530\n",
      "Iteration 86, loss = 0.07391498\n",
      "Iteration 87, loss = 0.07453044\n",
      "Iteration 88, loss = 0.07518027\n",
      "Iteration 89, loss = 0.07602700\n",
      "Iteration 90, loss = 0.07362403\n",
      "Iteration 91, loss = 0.07693150\n",
      "Iteration 92, loss = 0.07486767\n",
      "Iteration 93, loss = 0.08569582\n",
      "Iteration 94, loss = 0.07586849\n",
      "Iteration 95, loss = 0.07407619\n",
      "Iteration 96, loss = 0.08074825\n",
      "Iteration 97, loss = 0.07560555\n",
      "Iteration 98, loss = 0.07142074\n",
      "Iteration 99, loss = 0.07033015\n",
      "Iteration 100, loss = 0.07024796\n",
      "Iteration 101, loss = 0.07331338\n",
      "Iteration 102, loss = 0.06894819\n",
      "Iteration 103, loss = 0.06881047\n",
      "Iteration 104, loss = 0.06830537\n",
      "Iteration 105, loss = 0.06985350\n",
      "Iteration 106, loss = 0.06991461\n",
      "Iteration 107, loss = 0.06816353\n",
      "Iteration 108, loss = 0.07186750\n",
      "Iteration 109, loss = 0.06418266\n",
      "Iteration 110, loss = 0.06501894\n",
      "Iteration 111, loss = 0.06758334\n",
      "Iteration 112, loss = 0.06627198\n",
      "Iteration 113, loss = 0.06573443\n",
      "Iteration 114, loss = 0.06566226\n",
      "Iteration 115, loss = 0.06513113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 116, loss = 0.06707121\n",
      "Iteration 117, loss = 0.07379792\n",
      "Iteration 118, loss = 0.06270826\n",
      "Iteration 119, loss = 0.06349675\n",
      "Iteration 120, loss = 0.06971230\n",
      "Iteration 121, loss = 0.06331480\n",
      "Iteration 122, loss = 0.06647188\n",
      "Iteration 123, loss = 0.06446644\n",
      "Iteration 124, loss = 0.06050424\n",
      "Iteration 125, loss = 0.06711812\n",
      "Iteration 126, loss = 0.06496457\n",
      "Iteration 127, loss = 0.06008789\n",
      "Iteration 128, loss = 0.06109690\n",
      "Iteration 129, loss = 0.05935318\n",
      "Iteration 130, loss = 0.05913989\n",
      "Iteration 131, loss = 0.05892381\n",
      "Iteration 132, loss = 0.05958508\n",
      "Iteration 133, loss = 0.06431709\n",
      "Iteration 134, loss = 0.05696596\n",
      "Iteration 135, loss = 0.05679530\n",
      "Iteration 136, loss = 0.05819294\n",
      "Iteration 137, loss = 0.06470650\n",
      "Iteration 138, loss = 0.05982813\n",
      "Iteration 139, loss = 0.05643595\n",
      "Iteration 140, loss = 0.05728558\n",
      "Iteration 141, loss = 0.05935956\n",
      "Iteration 142, loss = 0.05290159\n",
      "Iteration 143, loss = 0.05710593\n",
      "Iteration 144, loss = 0.06922155\n",
      "Iteration 145, loss = 0.06233369\n",
      "Iteration 146, loss = 0.06498383\n",
      "Iteration 147, loss = 0.07857931\n",
      "Iteration 148, loss = 0.05642979\n",
      "Iteration 149, loss = 0.05510082\n",
      "Iteration 150, loss = 0.05460423\n",
      "Iteration 151, loss = 0.05300449\n",
      "Iteration 152, loss = 0.05471460\n",
      "Iteration 153, loss = 0.06252657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 93.66306027820711\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.31706913\n",
      "Iteration 2, loss = 0.20209115\n",
      "Iteration 3, loss = 0.18044900\n",
      "Iteration 4, loss = 0.16603060\n",
      "Iteration 5, loss = 0.16586056\n",
      "Iteration 6, loss = 0.16696306\n",
      "Iteration 7, loss = 0.14740698\n",
      "Iteration 8, loss = 0.14505229\n",
      "Iteration 9, loss = 0.14250685\n",
      "Iteration 10, loss = 0.14095933\n",
      "Iteration 11, loss = 0.14065575\n",
      "Iteration 12, loss = 0.13392712\n",
      "Iteration 13, loss = 0.13032053\n",
      "Iteration 14, loss = 0.13256112\n",
      "Iteration 15, loss = 0.13200252\n",
      "Iteration 16, loss = 0.12313554\n",
      "Iteration 17, loss = 0.12229711\n",
      "Iteration 18, loss = 0.12522663\n",
      "Iteration 19, loss = 0.12339625\n",
      "Iteration 20, loss = 0.12717506\n",
      "Iteration 21, loss = 0.12492564\n",
      "Iteration 22, loss = 0.12959872\n",
      "Iteration 23, loss = 0.13499781\n",
      "Iteration 24, loss = 0.12032778\n",
      "Iteration 25, loss = 0.11372382\n",
      "Iteration 26, loss = 0.12000138\n",
      "Iteration 27, loss = 0.11621325\n",
      "Iteration 28, loss = 0.11688569\n",
      "Iteration 29, loss = 0.12067812\n",
      "Iteration 30, loss = 0.11241356\n",
      "Iteration 31, loss = 0.11156005\n",
      "Iteration 32, loss = 0.10450668\n",
      "Iteration 33, loss = 0.10680688\n",
      "Iteration 34, loss = 0.10406608\n",
      "Iteration 35, loss = 0.10325727\n",
      "Iteration 36, loss = 0.10657164\n",
      "Iteration 37, loss = 0.10409219\n",
      "Iteration 38, loss = 0.09862038\n",
      "Iteration 39, loss = 0.10293770\n",
      "Iteration 40, loss = 0.09985570\n",
      "Iteration 41, loss = 0.09997564\n",
      "Iteration 42, loss = 0.10038450\n",
      "Iteration 43, loss = 0.09595425\n",
      "Iteration 44, loss = 0.09643390\n",
      "Iteration 45, loss = 0.09580258\n",
      "Iteration 46, loss = 0.09729406\n",
      "Iteration 47, loss = 0.09778492\n",
      "Iteration 48, loss = 0.10571335\n",
      "Iteration 49, loss = 0.09531223\n",
      "Iteration 50, loss = 0.09487074\n",
      "Iteration 51, loss = 0.09712697\n",
      "Iteration 52, loss = 0.09195073\n",
      "Iteration 53, loss = 0.09373079\n",
      "Iteration 54, loss = 0.09421730\n",
      "Iteration 55, loss = 0.08834917\n",
      "Iteration 56, loss = 0.08938149\n",
      "Iteration 57, loss = 0.09838744\n",
      "Iteration 58, loss = 0.09569750\n",
      "Iteration 59, loss = 0.08814192\n",
      "Iteration 60, loss = 0.08820408\n",
      "Iteration 61, loss = 0.09311689\n",
      "Iteration 62, loss = 0.09246775\n",
      "Iteration 63, loss = 0.08614666\n",
      "Iteration 64, loss = 0.08801093\n",
      "Iteration 65, loss = 0.09163276\n",
      "Iteration 66, loss = 0.08293249\n",
      "Iteration 67, loss = 0.08000720\n",
      "Iteration 68, loss = 0.08365867\n",
      "Iteration 69, loss = 0.08151638\n",
      "Iteration 70, loss = 0.08170513\n",
      "Iteration 71, loss = 0.08237666\n",
      "Iteration 72, loss = 0.08432395\n",
      "Iteration 73, loss = 0.08381729\n",
      "Iteration 74, loss = 0.08249824\n",
      "Iteration 75, loss = 0.07945586\n",
      "Iteration 76, loss = 0.07945150\n",
      "Iteration 77, loss = 0.07951734\n",
      "Iteration 78, loss = 0.08026070\n",
      "Iteration 79, loss = 0.07507121\n",
      "Iteration 80, loss = 0.07578731\n",
      "Iteration 81, loss = 0.08014882\n",
      "Iteration 82, loss = 0.08287770\n",
      "Iteration 83, loss = 0.07289635\n",
      "Iteration 84, loss = 0.07950612\n",
      "Iteration 85, loss = 0.07783267\n",
      "Iteration 86, loss = 0.07197554\n",
      "Iteration 87, loss = 0.07266683\n",
      "Iteration 88, loss = 0.07237230\n",
      "Iteration 89, loss = 0.07349150\n",
      "Iteration 90, loss = 0.07220536\n",
      "Iteration 91, loss = 0.07167730\n",
      "Iteration 92, loss = 0.06804750\n",
      "Iteration 93, loss = 0.07744133\n",
      "Iteration 94, loss = 0.07223122\n",
      "Iteration 95, loss = 0.07226941\n",
      "Iteration 96, loss = 0.07228622\n",
      "Iteration 97, loss = 0.07323257\n",
      "Iteration 98, loss = 0.06807124\n",
      "Iteration 99, loss = 0.06761855\n",
      "Iteration 100, loss = 0.06799373\n",
      "Iteration 101, loss = 0.07181764\n",
      "Iteration 102, loss = 0.06704655\n",
      "Iteration 103, loss = 0.07060807\n",
      "Iteration 104, loss = 0.06834526\n",
      "Iteration 105, loss = 0.06612051\n",
      "Iteration 106, loss = 0.06876751\n",
      "Iteration 107, loss = 0.06602280\n",
      "Iteration 108, loss = 0.06563958\n",
      "Iteration 109, loss = 0.06417949\n",
      "Iteration 110, loss = 0.06729586\n",
      "Iteration 111, loss = 0.06231279\n",
      "Iteration 112, loss = 0.06681236\n",
      "Iteration 113, loss = 0.06391664\n",
      "Iteration 114, loss = 0.06139298\n",
      "Iteration 115, loss = 0.06348477\n",
      "Iteration 116, loss = 0.06151906\n",
      "Iteration 117, loss = 0.06229391\n",
      "Iteration 118, loss = 0.06220120\n",
      "Iteration 119, loss = 0.06379396\n",
      "Iteration 120, loss = 0.06484074\n",
      "Iteration 121, loss = 0.06422720\n",
      "Iteration 122, loss = 0.06081358\n",
      "Iteration 123, loss = 0.06164457\n",
      "Iteration 124, loss = 0.06067504\n",
      "Iteration 125, loss = 0.05924252\n",
      "Iteration 126, loss = 0.05961009\n",
      "Iteration 127, loss = 0.06353842\n",
      "Iteration 128, loss = 0.05836117\n",
      "Iteration 129, loss = 0.06140338\n",
      "Iteration 130, loss = 0.06034998\n",
      "Iteration 131, loss = 0.05809295\n",
      "Iteration 132, loss = 0.06133590\n",
      "Iteration 133, loss = 0.06884328\n",
      "Iteration 134, loss = 0.06579911\n",
      "Iteration 135, loss = 0.06008463\n",
      "Iteration 136, loss = 0.05867063\n",
      "Iteration 137, loss = 0.06624321\n",
      "Iteration 138, loss = 0.06106763\n",
      "Iteration 139, loss = 0.05530354\n",
      "Iteration 140, loss = 0.05427893\n",
      "Iteration 141, loss = 0.05434688\n",
      "Iteration 142, loss = 0.05271509\n",
      "Iteration 143, loss = 0.05323747\n",
      "Iteration 144, loss = 0.05351842\n",
      "Iteration 145, loss = 0.06068139\n",
      "Iteration 146, loss = 0.05871168\n",
      "Iteration 147, loss = 0.05248019\n",
      "Iteration 148, loss = 0.05588257\n",
      "Iteration 149, loss = 0.05678511\n",
      "Iteration 150, loss = 0.05631760\n",
      "Iteration 151, loss = 0.05238183\n",
      "Iteration 152, loss = 0.05259213\n",
      "Iteration 153, loss = 0.05844951\n",
      "Iteration 154, loss = 0.06408936\n",
      "Iteration 155, loss = 0.05403979\n",
      "Iteration 156, loss = 0.05388875\n",
      "Iteration 157, loss = 0.04981515\n",
      "Iteration 158, loss = 0.05243075\n",
      "Iteration 159, loss = 0.06177041\n",
      "Iteration 160, loss = 0.06000689\n",
      "Iteration 161, loss = 0.05288993\n",
      "Iteration 162, loss = 0.04817215\n",
      "Iteration 163, loss = 0.04925940\n",
      "Iteration 164, loss = 0.04852294\n",
      "Iteration 165, loss = 0.05307298\n",
      "Iteration 166, loss = 0.04895330\n",
      "Iteration 167, loss = 0.05114686\n",
      "Iteration 168, loss = 0.05549619\n",
      "Iteration 169, loss = 0.05321422\n",
      "Iteration 170, loss = 0.05595370\n",
      "Iteration 171, loss = 0.05448090\n",
      "Iteration 172, loss = 0.05382447\n",
      "Iteration 173, loss = 0.05406209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 93.58578052550232\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.30391144\n",
      "Iteration 2, loss = 0.19600707\n",
      "Iteration 3, loss = 0.18782142\n",
      "Iteration 4, loss = 0.17053864\n",
      "Iteration 5, loss = 0.16123840\n",
      "Iteration 6, loss = 0.15391030\n",
      "Iteration 7, loss = 0.14771590\n",
      "Iteration 8, loss = 0.14488511\n",
      "Iteration 9, loss = 0.14127984\n",
      "Iteration 10, loss = 0.14220292\n",
      "Iteration 11, loss = 0.14040596\n",
      "Iteration 12, loss = 0.13980290\n",
      "Iteration 13, loss = 0.13464425\n",
      "Iteration 14, loss = 0.14265157\n",
      "Iteration 15, loss = 0.12823518\n",
      "Iteration 16, loss = 0.12627164\n",
      "Iteration 17, loss = 0.12100041\n",
      "Iteration 18, loss = 0.12106644\n",
      "Iteration 19, loss = 0.12040027\n",
      "Iteration 20, loss = 0.12232903\n",
      "Iteration 21, loss = 0.12944251\n",
      "Iteration 22, loss = 0.12093625\n",
      "Iteration 23, loss = 0.11709742\n",
      "Iteration 24, loss = 0.12060601\n",
      "Iteration 25, loss = 0.11485899\n",
      "Iteration 26, loss = 0.11235241\n",
      "Iteration 27, loss = 0.11339932\n",
      "Iteration 28, loss = 0.10737175\n",
      "Iteration 29, loss = 0.10607327\n",
      "Iteration 30, loss = 0.10658890\n",
      "Iteration 31, loss = 0.10501629\n",
      "Iteration 32, loss = 0.10163989\n",
      "Iteration 33, loss = 0.10590772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.10422649\n",
      "Iteration 35, loss = 0.10364475\n",
      "Iteration 36, loss = 0.10474774\n",
      "Iteration 37, loss = 0.10771299\n",
      "Iteration 38, loss = 0.09805060\n",
      "Iteration 39, loss = 0.09865118\n",
      "Iteration 40, loss = 0.10027133\n",
      "Iteration 41, loss = 0.10142365\n",
      "Iteration 42, loss = 0.11538759\n",
      "Iteration 43, loss = 0.10822831\n",
      "Iteration 44, loss = 0.09369318\n",
      "Iteration 45, loss = 0.09939876\n",
      "Iteration 46, loss = 0.09333098\n",
      "Iteration 47, loss = 0.09199236\n",
      "Iteration 48, loss = 0.09082174\n",
      "Iteration 49, loss = 0.09075206\n",
      "Iteration 50, loss = 0.09572603\n",
      "Iteration 51, loss = 0.09250350\n",
      "Iteration 52, loss = 0.09029694\n",
      "Iteration 53, loss = 0.08885084\n",
      "Iteration 54, loss = 0.09365565\n",
      "Iteration 55, loss = 0.09494673\n",
      "Iteration 56, loss = 0.09112209\n",
      "Iteration 57, loss = 0.08616002\n",
      "Iteration 58, loss = 0.08613155\n",
      "Iteration 59, loss = 0.08585060\n",
      "Iteration 60, loss = 0.08676776\n",
      "Iteration 61, loss = 0.09441012\n",
      "Iteration 62, loss = 0.08415281\n",
      "Iteration 63, loss = 0.08337288\n",
      "Iteration 64, loss = 0.09263703\n",
      "Iteration 65, loss = 0.09514627\n",
      "Iteration 66, loss = 0.08790980\n",
      "Iteration 67, loss = 0.07840057\n",
      "Iteration 68, loss = 0.07774502\n",
      "Iteration 69, loss = 0.07931428\n",
      "Iteration 70, loss = 0.08118461\n",
      "Iteration 71, loss = 0.07452974\n",
      "Iteration 72, loss = 0.07839646\n",
      "Iteration 73, loss = 0.07654651\n",
      "Iteration 74, loss = 0.07441816\n",
      "Iteration 75, loss = 0.07699012\n",
      "Iteration 76, loss = 0.07449075\n",
      "Iteration 77, loss = 0.07392920\n",
      "Iteration 78, loss = 0.07895958\n",
      "Iteration 79, loss = 0.09210573\n",
      "Iteration 80, loss = 0.07735263\n",
      "Iteration 81, loss = 0.07929269\n",
      "Iteration 82, loss = 0.06988750\n",
      "Iteration 83, loss = 0.07162043\n",
      "Iteration 84, loss = 0.06831941\n",
      "Iteration 85, loss = 0.06898459\n",
      "Iteration 86, loss = 0.06954148\n",
      "Iteration 87, loss = 0.07082038\n",
      "Iteration 88, loss = 0.06988088\n",
      "Iteration 89, loss = 0.07601954\n",
      "Iteration 90, loss = 0.07119206\n",
      "Iteration 91, loss = 0.06711915\n",
      "Iteration 92, loss = 0.06538803\n",
      "Iteration 93, loss = 0.06551153\n",
      "Iteration 94, loss = 0.06733508\n",
      "Iteration 95, loss = 0.07096957\n",
      "Iteration 96, loss = 0.06602403\n",
      "Iteration 97, loss = 0.06627750\n",
      "Iteration 98, loss = 0.06689109\n",
      "Iteration 99, loss = 0.06682489\n",
      "Iteration 100, loss = 0.07243553\n",
      "Iteration 101, loss = 0.07161419\n",
      "Iteration 102, loss = 0.07252746\n",
      "Iteration 103, loss = 0.06649533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 94.20401854714065\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 94.20401854714065 %\n",
      "Fold 1: 93.89489953632149 %\n",
      "Fold 2: 93.66306027820711 %\n",
      "Fold 3: 93.58578052550232 %\n",
      "Fold 4: 94.20401854714065 %\n",
      "Average: 93.91035548686246 %\n",
      "Accuracy:  0.9177997527812114\n",
      "Precision:  0.9223181257706535\n",
      "Recall:  0.9144254278728606\n",
      "F1-Score:  0.9183548189073051\n",
      "AUC:  0.9178377139364304\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_mlp(X_Train,Y_Train.ravel())\n",
    "y_pred=clf.predict(X_Test)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test,y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test,y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test,y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test,y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cumulative explained variance')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kpca = KernelPCA(kernel = 'rbf')\n",
    "kpca_transform = kpca.fit_transform(X_Train_FeatureMap)\n",
    "explained_variance = np.var(kpca_transform, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "plt.yticks([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "plt.plot(np.cumsum(explained_variance_ratio))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6470, 3000)\n"
     ]
    }
   ],
   "source": [
    "kpca = KernelPCA(kernel = 'rbf',n_components=3000)\n",
    "X_Train_Transformed_FeatureMap = kpca.fit_transform(X_Train_FeatureMap)\n",
    "print(X_Train_Transformed_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1618, 3136) (1618, 3000)\n"
     ]
    }
   ],
   "source": [
    "X_Test_Transformed_FeatureMap = kpca.transform(X_Test_FeatureMap)\n",
    "\n",
    "print(X_Test_FeatureMap.shape,X_Test_Transformed_FeatureMap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 1/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.944 total time=  18.7s\n",
      "[CV 2/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 2/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.930 total time=  18.0s\n",
      "[CV 3/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 3/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.940 total time=  18.0s\n",
      "[CV 4/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 4/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.937 total time=  18.1s\n",
      "[CV 5/5; 1/2] START C=10, gamma=10, kernel=rbf..................................\n",
      "[CV 5/5; 1/2] END ...C=10, gamma=10, kernel=rbf;, score=0.940 total time=  18.0s\n",
      "[CV 1/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 1/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.933 total time=  30.8s\n",
      "[CV 2/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 2/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.925 total time=  30.9s\n",
      "[CV 3/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 3/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.940 total time=  29.7s\n",
      "[CV 4/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 4/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.944 total time=  30.4s\n",
      "[CV 5/5; 2/2] START C=10, gamma=100, kernel=rbf.................................\n",
      "[CV 5/5; 2/2] END ..C=10, gamma=100, kernel=rbf;, score=0.934 total time=  30.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [10], 'gamma': [10, 100], 'kernel': ['rbf']},\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% notify -m \"GridCV\"\n",
    "tuned_parameters = {'kernel': ['rbf'], 'gamma': [10,100],\n",
    "                     'C': [10],\n",
    "}\n",
    "clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring= 'accuracy',verbose=10\n",
    "    )\n",
    "clf.fit(X_Train_Transformed_FeatureMap, Y_Train_FeatureMap.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 49.1499227202473\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 48.222565687789796\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 48.99536321483771\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 49.22720247295209\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 49.53632148377125\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 49.1499227202473 %\n",
      "Fold 1: 48.222565687789796 %\n",
      "Fold 2: 48.99536321483771 %\n",
      "Fold 3: 49.22720247295209 %\n",
      "Fold 4: 49.53632148377125 %\n",
      "Average: 49.02627511591963 %\n",
      "Accuracy:  0.49443757725587145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1-Score:  0.0\n",
      "AUC:  0.5\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_svm(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=clf.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 81.76197836166924\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 81.99381761978361\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 83.30757341576506\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 79.98454404945903\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 79.75270479134467\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 81.76197836166924 %\n",
      "Fold 1: 81.99381761978361 %\n",
      "Fold 2: 83.30757341576506 %\n",
      "Fold 3: 79.98454404945903 %\n",
      "Fold 4: 79.75270479134467 %\n",
      "Average: 81.36012364760433 %\n",
      "Accuracy:  0.7855377008652658\n",
      "Precision:  0.7882496940024479\n",
      "Recall:  0.7872860635696821\n",
      "F1-Score:  0.7877675840978593\n",
      "AUC:  0.785518031784841\n"
     ]
    }
   ],
   "source": [
    "dtree=k_fold_cv_dtree(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=dtree.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "[20:50:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 89.56723338485317\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "[20:51:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 90.57187017001546\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "[20:53:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 91.19010819165379\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "[20:55:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 91.80834621329211\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "[20:57:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 91.112828438949\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 89.56723338485317 %\n",
      "Fold 1: 90.57187017001546 %\n",
      "Fold 2: 91.19010819165379 %\n",
      "Fold 3: 91.80834621329211 %\n",
      "Fold 4: 91.112828438949 %\n",
      "Average: 90.85007727975271 %\n",
      "Accuracy:  0.9011124845488258\n",
      "Precision:  0.8825581395348837\n",
      "Recall:  0.9278728606356969\n",
      "F1-Score:  0.9046483909415971\n",
      "AUC:  0.9008114303178484\n"
     ]
    }
   ],
   "source": [
    "xg=k_fold_cv_xgb(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=xg.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69003805\n",
      "Iteration 2, loss = 0.67958354\n",
      "Iteration 3, loss = 0.66436373\n",
      "Iteration 4, loss = 0.64329261\n",
      "Iteration 5, loss = 0.61585108\n",
      "Iteration 6, loss = 0.58283988\n",
      "Iteration 7, loss = 0.54677571\n",
      "Iteration 8, loss = 0.50892058\n",
      "Iteration 9, loss = 0.47191022\n",
      "Iteration 10, loss = 0.43669568\n",
      "Iteration 11, loss = 0.40469120\n",
      "Iteration 12, loss = 0.37584757\n",
      "Iteration 13, loss = 0.34939670\n",
      "Iteration 14, loss = 0.32635130\n",
      "Iteration 15, loss = 0.30633221\n",
      "Iteration 16, loss = 0.28877317\n",
      "Iteration 17, loss = 0.27392091\n",
      "Iteration 18, loss = 0.26026939\n",
      "Iteration 19, loss = 0.24841616\n",
      "Iteration 20, loss = 0.23802523\n",
      "Iteration 21, loss = 0.22896573\n",
      "Iteration 22, loss = 0.22081322\n",
      "Iteration 23, loss = 0.21295069\n",
      "Iteration 24, loss = 0.20669409\n",
      "Iteration 25, loss = 0.20019163\n",
      "Iteration 26, loss = 0.19462581\n",
      "Iteration 27, loss = 0.18958894\n",
      "Iteration 28, loss = 0.18489886\n",
      "Iteration 29, loss = 0.18013234\n",
      "Iteration 30, loss = 0.17612004\n",
      "Iteration 31, loss = 0.17236614\n",
      "Iteration 32, loss = 0.16909289\n",
      "Iteration 33, loss = 0.16514655\n",
      "Iteration 34, loss = 0.16238917\n",
      "Iteration 35, loss = 0.15932271\n",
      "Iteration 36, loss = 0.15641641\n",
      "Iteration 37, loss = 0.15356511\n",
      "Iteration 38, loss = 0.15094876\n",
      "Iteration 39, loss = 0.14865110\n",
      "Iteration 40, loss = 0.14662507\n",
      "Iteration 41, loss = 0.14395282\n",
      "Iteration 42, loss = 0.14247573\n",
      "Iteration 43, loss = 0.14000682\n",
      "Iteration 44, loss = 0.13828078\n",
      "Iteration 45, loss = 0.13636394\n",
      "Iteration 46, loss = 0.13449240\n",
      "Iteration 47, loss = 0.13265356\n",
      "Iteration 48, loss = 0.13096967\n",
      "Iteration 49, loss = 0.12960501\n",
      "Iteration 50, loss = 0.12795148\n",
      "Iteration 51, loss = 0.12641370\n",
      "Iteration 52, loss = 0.12529393\n",
      "Iteration 53, loss = 0.12377754\n",
      "Iteration 54, loss = 0.12227781\n",
      "Iteration 55, loss = 0.12137429\n",
      "Iteration 56, loss = 0.11975755\n",
      "Iteration 57, loss = 0.11931016\n",
      "Iteration 58, loss = 0.11772029\n",
      "Iteration 59, loss = 0.11632213\n",
      "Iteration 60, loss = 0.11486225\n",
      "Iteration 61, loss = 0.11403371\n",
      "Iteration 62, loss = 0.11278299\n",
      "Iteration 63, loss = 0.11162194\n",
      "Iteration 64, loss = 0.11063909\n",
      "Iteration 65, loss = 0.10976791\n",
      "Iteration 66, loss = 0.10868618\n",
      "Iteration 67, loss = 0.10776286\n",
      "Iteration 68, loss = 0.10696948\n",
      "Iteration 69, loss = 0.10585721\n",
      "Iteration 70, loss = 0.10502474\n",
      "Iteration 71, loss = 0.10426362\n",
      "Iteration 72, loss = 0.10320252\n",
      "Iteration 73, loss = 0.10251840\n",
      "Iteration 74, loss = 0.10170529\n",
      "Iteration 75, loss = 0.10079230\n",
      "Iteration 76, loss = 0.10008621\n",
      "Iteration 77, loss = 0.09912367\n",
      "Iteration 78, loss = 0.09836450\n",
      "Iteration 79, loss = 0.09764772\n",
      "Iteration 80, loss = 0.09717037\n",
      "Iteration 81, loss = 0.09627155\n",
      "Iteration 82, loss = 0.09571638\n",
      "Iteration 83, loss = 0.09481053\n",
      "Iteration 84, loss = 0.09452595\n",
      "Iteration 85, loss = 0.09344676\n",
      "Iteration 86, loss = 0.09289482\n",
      "Iteration 87, loss = 0.09231181\n",
      "Iteration 88, loss = 0.09148760\n",
      "Iteration 89, loss = 0.09097815\n",
      "Iteration 90, loss = 0.09029904\n",
      "Iteration 91, loss = 0.08964779\n",
      "Iteration 92, loss = 0.08909250\n",
      "Iteration 93, loss = 0.08856839\n",
      "Iteration 94, loss = 0.08824401\n",
      "Iteration 95, loss = 0.08767858\n",
      "Iteration 96, loss = 0.08756556\n",
      "Iteration 97, loss = 0.08635352\n",
      "Iteration 98, loss = 0.08610257\n",
      "Iteration 99, loss = 0.08541274\n",
      "Iteration 100, loss = 0.08456282\n",
      "Iteration 101, loss = 0.08436560\n",
      "Iteration 102, loss = 0.08374628\n",
      "Iteration 103, loss = 0.08311633\n",
      "Iteration 104, loss = 0.08302430\n",
      "Iteration 105, loss = 0.08222811\n",
      "Iteration 106, loss = 0.08170662\n",
      "Iteration 107, loss = 0.08116467\n",
      "Iteration 108, loss = 0.08069354\n",
      "Iteration 109, loss = 0.08032958\n",
      "Iteration 110, loss = 0.07995107\n",
      "Iteration 111, loss = 0.07927049\n",
      "Iteration 112, loss = 0.07882939\n",
      "Iteration 113, loss = 0.07838385\n",
      "Iteration 114, loss = 0.07795050\n",
      "Iteration 115, loss = 0.07765330\n",
      "Iteration 116, loss = 0.07736608\n",
      "Iteration 117, loss = 0.07711451\n",
      "Iteration 118, loss = 0.07630033\n",
      "Iteration 119, loss = 0.07588959\n",
      "Iteration 120, loss = 0.07547464\n",
      "Iteration 121, loss = 0.07511543\n",
      "Iteration 122, loss = 0.07492087\n",
      "Iteration 123, loss = 0.07448135\n",
      "Iteration 124, loss = 0.07387582\n",
      "Iteration 125, loss = 0.07374796\n",
      "Iteration 126, loss = 0.07376782\n",
      "Iteration 127, loss = 0.07291639\n",
      "Iteration 128, loss = 0.07282301\n",
      "Iteration 129, loss = 0.07269254\n",
      "Iteration 130, loss = 0.07212002\n",
      "Iteration 131, loss = 0.07156166\n",
      "Iteration 132, loss = 0.07097527\n",
      "Iteration 133, loss = 0.07120152\n",
      "Iteration 134, loss = 0.07106157\n",
      "Iteration 135, loss = 0.07009972\n",
      "Iteration 136, loss = 0.07004963\n",
      "Iteration 137, loss = 0.06997138\n",
      "Iteration 138, loss = 0.06907536\n",
      "Iteration 139, loss = 0.06892225\n",
      "Iteration 140, loss = 0.06848761\n",
      "Iteration 141, loss = 0.06819854\n",
      "Iteration 142, loss = 0.06795058\n",
      "Iteration 143, loss = 0.06755813\n",
      "Iteration 144, loss = 0.06751664\n",
      "Iteration 145, loss = 0.06691577\n",
      "Iteration 146, loss = 0.06673546\n",
      "Iteration 147, loss = 0.06648683\n",
      "Iteration 148, loss = 0.06628996\n",
      "Iteration 149, loss = 0.06591123\n",
      "Iteration 150, loss = 0.06594587\n",
      "Iteration 151, loss = 0.06558599\n",
      "Iteration 152, loss = 0.06510072\n",
      "Iteration 153, loss = 0.06515207\n",
      "Iteration 154, loss = 0.06495751\n",
      "Iteration 155, loss = 0.06425029\n",
      "Iteration 156, loss = 0.06434940\n",
      "Iteration 157, loss = 0.06356120\n",
      "Iteration 158, loss = 0.06357033\n",
      "Iteration 159, loss = 0.06331816\n",
      "Iteration 160, loss = 0.06326009\n",
      "Iteration 161, loss = 0.06308326\n",
      "Iteration 162, loss = 0.06279546\n",
      "Iteration 163, loss = 0.06224462\n",
      "Iteration 164, loss = 0.06223807\n",
      "Iteration 165, loss = 0.06197365\n",
      "Iteration 166, loss = 0.06143907\n",
      "Iteration 167, loss = 0.06151258\n",
      "Iteration 168, loss = 0.06139540\n",
      "Iteration 169, loss = 0.06083039\n",
      "Iteration 170, loss = 0.06049825\n",
      "Iteration 171, loss = 0.06073473\n",
      "Iteration 172, loss = 0.06034228\n",
      "Iteration 173, loss = 0.05984633\n",
      "Iteration 174, loss = 0.05963219\n",
      "Iteration 175, loss = 0.05953384\n",
      "Iteration 176, loss = 0.05924123\n",
      "Iteration 177, loss = 0.05905731\n",
      "Iteration 178, loss = 0.05903087\n",
      "Iteration 179, loss = 0.05857969\n",
      "Iteration 180, loss = 0.05866702\n",
      "Iteration 181, loss = 0.05837039\n",
      "Iteration 182, loss = 0.05867853\n",
      "Iteration 183, loss = 0.05791862\n",
      "Iteration 184, loss = 0.05757316\n",
      "Iteration 185, loss = 0.05726232\n",
      "Iteration 186, loss = 0.05698561\n",
      "Iteration 187, loss = 0.05709205\n",
      "Iteration 188, loss = 0.05662088\n",
      "Iteration 189, loss = 0.05670214\n",
      "Iteration 190, loss = 0.05629037\n",
      "Iteration 191, loss = 0.05613553\n",
      "Iteration 192, loss = 0.05585526\n",
      "Iteration 193, loss = 0.05567892\n",
      "Iteration 194, loss = 0.05571834\n",
      "Iteration 195, loss = 0.05561244\n",
      "Iteration 196, loss = 0.05541508\n",
      "Iteration 197, loss = 0.05504302\n",
      "Iteration 198, loss = 0.05471133\n",
      "Iteration 199, loss = 0.05462653\n",
      "Iteration 200, loss = 0.05458359\n",
      "Iteration 201, loss = 0.05417757\n",
      "Iteration 202, loss = 0.05394408\n",
      "Iteration 203, loss = 0.05387271\n",
      "Iteration 204, loss = 0.05385162\n",
      "Iteration 205, loss = 0.05358728\n",
      "Iteration 206, loss = 0.05348422\n",
      "Iteration 207, loss = 0.05356742\n",
      "Iteration 208, loss = 0.05376078\n",
      "Iteration 209, loss = 0.05305228\n",
      "Iteration 210, loss = 0.05281281\n",
      "Iteration 211, loss = 0.05262152\n",
      "Iteration 212, loss = 0.05272689\n",
      "Iteration 213, loss = 0.05191432\n",
      "Iteration 214, loss = 0.05195623\n",
      "Iteration 215, loss = 0.05201896\n",
      "Iteration 216, loss = 0.05180622\n",
      "Iteration 217, loss = 0.05155836\n",
      "Iteration 218, loss = 0.05239060\n",
      "Iteration 219, loss = 0.05156861\n",
      "Iteration 220, loss = 0.05101038\n",
      "Iteration 221, loss = 0.05104873\n",
      "Iteration 222, loss = 0.05101883\n",
      "Iteration 223, loss = 0.05083285\n",
      "Iteration 224, loss = 0.05071411\n",
      "Iteration 225, loss = 0.05031090\n",
      "Iteration 226, loss = 0.05022469\n",
      "Iteration 227, loss = 0.05013372\n",
      "Iteration 228, loss = 0.04980118\n",
      "Iteration 229, loss = 0.05075180\n",
      "Iteration 230, loss = 0.04961377\n",
      "Iteration 231, loss = 0.04944114\n",
      "Iteration 232, loss = 0.04929734\n",
      "Iteration 233, loss = 0.04959683\n",
      "Iteration 234, loss = 0.04896754\n",
      "Iteration 235, loss = 0.04913775\n",
      "Iteration 236, loss = 0.04865399\n",
      "Iteration 237, loss = 0.04847547\n",
      "Iteration 238, loss = 0.04828130\n",
      "Iteration 239, loss = 0.04804987\n",
      "Iteration 240, loss = 0.04896557\n",
      "Iteration 241, loss = 0.04813711\n",
      "Iteration 242, loss = 0.04796568\n",
      "Iteration 243, loss = 0.04774318\n",
      "Iteration 244, loss = 0.04784359\n",
      "Iteration 245, loss = 0.04745639\n",
      "Iteration 246, loss = 0.04739671\n",
      "Iteration 247, loss = 0.04729036\n",
      "Iteration 248, loss = 0.04715892\n",
      "Iteration 249, loss = 0.04737842\n",
      "Iteration 250, loss = 0.04716534\n",
      "Iteration 251, loss = 0.04685770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 0.04761136\n",
      "Iteration 253, loss = 0.04768001\n",
      "Iteration 254, loss = 0.04664948\n",
      "Iteration 255, loss = 0.04656224\n",
      "Iteration 256, loss = 0.04605425\n",
      "Iteration 257, loss = 0.04671705\n",
      "Iteration 258, loss = 0.04606849\n",
      "Iteration 259, loss = 0.04588973\n",
      "Iteration 260, loss = 0.04588769\n",
      "Iteration 261, loss = 0.04554307\n",
      "Iteration 262, loss = 0.04567879\n",
      "Iteration 263, loss = 0.04538215\n",
      "Iteration 264, loss = 0.04619766\n",
      "Iteration 265, loss = 0.04531383\n",
      "Iteration 266, loss = 0.04537359\n",
      "Iteration 267, loss = 0.04535314\n",
      "Iteration 268, loss = 0.04527481\n",
      "Iteration 269, loss = 0.04516406\n",
      "Iteration 270, loss = 0.04494439\n",
      "Iteration 271, loss = 0.04466198\n",
      "Iteration 272, loss = 0.04450148\n",
      "Iteration 273, loss = 0.04452660\n",
      "Iteration 274, loss = 0.04442022\n",
      "Iteration 275, loss = 0.04421781\n",
      "Iteration 276, loss = 0.04409319\n",
      "Iteration 277, loss = 0.04401016\n",
      "Iteration 278, loss = 0.04396411\n",
      "Iteration 279, loss = 0.04370507\n",
      "Iteration 280, loss = 0.04349130\n",
      "Iteration 281, loss = 0.04348753\n",
      "Iteration 282, loss = 0.04354196\n",
      "Iteration 283, loss = 0.04344973\n",
      "Iteration 284, loss = 0.04348413\n",
      "Iteration 285, loss = 0.04367897\n",
      "Iteration 286, loss = 0.04412912\n",
      "Iteration 287, loss = 0.04336347\n",
      "Iteration 288, loss = 0.04292460\n",
      "Iteration 289, loss = 0.04296696\n",
      "Iteration 290, loss = 0.04277816\n",
      "Iteration 291, loss = 0.04311665\n",
      "Iteration 292, loss = 0.04366231\n",
      "Iteration 293, loss = 0.04235053\n",
      "Iteration 294, loss = 0.04241944\n",
      "Iteration 295, loss = 0.04237894\n",
      "Iteration 296, loss = 0.04221155\n",
      "Iteration 297, loss = 0.04199088\n",
      "Iteration 298, loss = 0.04222414\n",
      "Iteration 299, loss = 0.04195663\n",
      "Iteration 300, loss = 0.04180606\n",
      "Iteration 301, loss = 0.04197045\n",
      "Iteration 302, loss = 0.04171723\n",
      "Iteration 303, loss = 0.04187057\n",
      "Iteration 304, loss = 0.04165914\n",
      "Iteration 305, loss = 0.04146387\n",
      "Iteration 306, loss = 0.04130619\n",
      "Iteration 307, loss = 0.04120423\n",
      "Iteration 308, loss = 0.04175780\n",
      "Iteration 309, loss = 0.04156436\n",
      "Iteration 310, loss = 0.04103726\n",
      "Iteration 311, loss = 0.04096942\n",
      "Iteration 312, loss = 0.04077420\n",
      "Iteration 313, loss = 0.04117790\n",
      "Iteration 314, loss = 0.04128758\n",
      "Iteration 315, loss = 0.04064372\n",
      "Iteration 316, loss = 0.04053452\n",
      "Iteration 317, loss = 0.04074119\n",
      "Iteration 318, loss = 0.04055964\n",
      "Iteration 319, loss = 0.04065929\n",
      "Iteration 320, loss = 0.04029101\n",
      "Iteration 321, loss = 0.04006597\n",
      "Iteration 322, loss = 0.04004525\n",
      "Iteration 323, loss = 0.04018301\n",
      "Iteration 324, loss = 0.04000046\n",
      "Iteration 325, loss = 0.03984903\n",
      "Iteration 326, loss = 0.03986278\n",
      "Iteration 327, loss = 0.03948802\n",
      "Iteration 328, loss = 0.04100541\n",
      "Iteration 329, loss = 0.03969571\n",
      "Iteration 330, loss = 0.03954818\n",
      "Iteration 331, loss = 0.03955237\n",
      "Iteration 332, loss = 0.03928535\n",
      "Iteration 333, loss = 0.03958468\n",
      "Iteration 334, loss = 0.03912024\n",
      "Iteration 335, loss = 0.04005426\n",
      "Iteration 336, loss = 0.03930546\n",
      "Iteration 337, loss = 0.03914313\n",
      "Iteration 338, loss = 0.03993396\n",
      "Iteration 339, loss = 0.03913971\n",
      "Iteration 340, loss = 0.03944250\n",
      "Iteration 341, loss = 0.03927553\n",
      "Iteration 342, loss = 0.03882222\n",
      "Iteration 343, loss = 0.03887312\n",
      "Iteration 344, loss = 0.03895836\n",
      "Iteration 345, loss = 0.03865719\n",
      "Iteration 346, loss = 0.03840694\n",
      "Iteration 347, loss = 0.03851085\n",
      "Iteration 348, loss = 0.03949697\n",
      "Iteration 349, loss = 0.03898845\n",
      "Iteration 350, loss = 0.03845668\n",
      "Iteration 351, loss = 0.03841140\n",
      "Iteration 352, loss = 0.03814452\n",
      "Iteration 353, loss = 0.03855389\n",
      "Iteration 354, loss = 0.03806110\n",
      "Iteration 355, loss = 0.03770165\n",
      "Iteration 356, loss = 0.03875847\n",
      "Iteration 357, loss = 0.03792495\n",
      "Iteration 358, loss = 0.03768584\n",
      "Iteration 359, loss = 0.03775598\n",
      "Iteration 360, loss = 0.03769086\n",
      "Iteration 361, loss = 0.03776418\n",
      "Iteration 362, loss = 0.03759552\n",
      "Iteration 363, loss = 0.03727454\n",
      "Iteration 364, loss = 0.03854473\n",
      "Iteration 365, loss = 0.03747513\n",
      "Iteration 366, loss = 0.03761427\n",
      "Iteration 367, loss = 0.03748764\n",
      "Iteration 368, loss = 0.03727226\n",
      "Iteration 369, loss = 0.03711094\n",
      "Iteration 370, loss = 0.03692901\n",
      "Iteration 371, loss = 0.03757587\n",
      "Iteration 372, loss = 0.03724526\n",
      "Iteration 373, loss = 0.03696218\n",
      "Iteration 374, loss = 0.03690075\n",
      "Iteration 375, loss = 0.03695837\n",
      "Iteration 376, loss = 0.03678555\n",
      "Iteration 377, loss = 0.03682147\n",
      "Iteration 378, loss = 0.03665392\n",
      "Iteration 379, loss = 0.03658683\n",
      "Iteration 380, loss = 0.03655332\n",
      "Iteration 381, loss = 0.03699579\n",
      "Iteration 382, loss = 0.03633203\n",
      "Iteration 383, loss = 0.03643274\n",
      "Iteration 384, loss = 0.03614732\n",
      "Iteration 385, loss = 0.03643894\n",
      "Iteration 386, loss = 0.03624740\n",
      "Iteration 387, loss = 0.03619573\n",
      "Iteration 388, loss = 0.03632910\n",
      "Iteration 389, loss = 0.03632710\n",
      "Iteration 390, loss = 0.03632953\n",
      "Iteration 391, loss = 0.03616061\n",
      "Iteration 392, loss = 0.03600446\n",
      "Iteration 393, loss = 0.03640197\n",
      "Iteration 394, loss = 0.03645382\n",
      "Iteration 395, loss = 0.03577174\n",
      "Iteration 396, loss = 0.03584317\n",
      "Iteration 397, loss = 0.03556385\n",
      "Iteration 398, loss = 0.03570025\n",
      "Iteration 399, loss = 0.03560525\n",
      "Iteration 400, loss = 0.03580939\n",
      "Iteration 401, loss = 0.03562657\n",
      "Iteration 402, loss = 0.03585030\n",
      "Iteration 403, loss = 0.03532992\n",
      "Iteration 404, loss = 0.03553520\n",
      "Iteration 405, loss = 0.03567717\n",
      "Iteration 406, loss = 0.03540072\n",
      "Iteration 407, loss = 0.03540159\n",
      "Iteration 408, loss = 0.03537184\n",
      "Iteration 409, loss = 0.03516693\n",
      "Iteration 410, loss = 0.03514719\n",
      "Iteration 411, loss = 0.03499333\n",
      "Iteration 412, loss = 0.03491780\n",
      "Iteration 413, loss = 0.03539324\n",
      "Iteration 414, loss = 0.03555446\n",
      "Iteration 415, loss = 0.03501022\n",
      "Iteration 416, loss = 0.03506540\n",
      "Iteration 417, loss = 0.03473923\n",
      "Iteration 418, loss = 0.03459815\n",
      "Iteration 419, loss = 0.03516727\n",
      "Iteration 420, loss = 0.03472597\n",
      "Iteration 421, loss = 0.03481959\n",
      "Iteration 422, loss = 0.03443524\n",
      "Iteration 423, loss = 0.03473083\n",
      "Iteration 424, loss = 0.03476899\n",
      "Iteration 425, loss = 0.03528480\n",
      "Iteration 426, loss = 0.03479333\n",
      "Iteration 427, loss = 0.03482495\n",
      "Iteration 428, loss = 0.03457580\n",
      "Iteration 429, loss = 0.03440788\n",
      "Iteration 430, loss = 0.03471548\n",
      "Iteration 431, loss = 0.03423786\n",
      "Iteration 432, loss = 0.03417249\n",
      "Iteration 433, loss = 0.03413037\n",
      "Iteration 434, loss = 0.03392475\n",
      "Iteration 435, loss = 0.03568525\n",
      "Iteration 436, loss = 0.03437286\n",
      "Iteration 437, loss = 0.03398831\n",
      "Iteration 438, loss = 0.03396495\n",
      "Iteration 439, loss = 0.03449622\n",
      "Iteration 440, loss = 0.03373414\n",
      "Iteration 441, loss = 0.03394658\n",
      "Iteration 442, loss = 0.03376166\n",
      "Iteration 443, loss = 0.03420067\n",
      "Iteration 444, loss = 0.03437636\n",
      "Iteration 445, loss = 0.03373635\n",
      "Iteration 446, loss = 0.03378708\n",
      "Iteration 447, loss = 0.03442833\n",
      "Iteration 448, loss = 0.03367994\n",
      "Iteration 449, loss = 0.03368391\n",
      "Iteration 450, loss = 0.03362221\n",
      "Iteration 451, loss = 0.03338837\n",
      "Iteration 452, loss = 0.03365654\n",
      "Iteration 453, loss = 0.03421229\n",
      "Iteration 454, loss = 0.03392490\n",
      "Iteration 455, loss = 0.03354719\n",
      "Iteration 456, loss = 0.03347161\n",
      "Iteration 457, loss = 0.03372080\n",
      "Iteration 458, loss = 0.03352082\n",
      "Iteration 459, loss = 0.03348149\n",
      "Iteration 460, loss = 0.03327401\n",
      "Iteration 461, loss = 0.03320366\n",
      "Iteration 462, loss = 0.03327871\n",
      "Iteration 463, loss = 0.03339865\n",
      "Iteration 464, loss = 0.03297443\n",
      "Iteration 465, loss = 0.03354346\n",
      "Iteration 466, loss = 0.03299169\n",
      "Iteration 467, loss = 0.03306654\n",
      "Iteration 468, loss = 0.03339281\n",
      "Iteration 469, loss = 0.03303956\n",
      "Iteration 470, loss = 0.03283744\n",
      "Iteration 471, loss = 0.03302532\n",
      "Iteration 472, loss = 0.03252624\n",
      "Iteration 473, loss = 0.03288277\n",
      "Iteration 474, loss = 0.03258052\n",
      "Iteration 475, loss = 0.03279309\n",
      "Iteration 476, loss = 0.03277319\n",
      "Iteration 477, loss = 0.03264980\n",
      "Iteration 478, loss = 0.03294787\n",
      "Iteration 479, loss = 0.03264488\n",
      "Iteration 480, loss = 0.03264728\n",
      "Iteration 481, loss = 0.03236568\n",
      "Iteration 482, loss = 0.03281868\n",
      "Iteration 483, loss = 0.03352634\n",
      "Iteration 484, loss = 0.03227381\n",
      "Iteration 485, loss = 0.03238990\n",
      "Iteration 486, loss = 0.03228759\n",
      "Iteration 487, loss = 0.03242650\n",
      "Iteration 488, loss = 0.03237900\n",
      "Iteration 489, loss = 0.03294225\n",
      "Iteration 490, loss = 0.03250073\n",
      "Iteration 491, loss = 0.03224904\n",
      "Iteration 492, loss = 0.03279867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 90.72642967542504\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.69006832\n",
      "Iteration 2, loss = 0.67973763\n",
      "Iteration 3, loss = 0.66501800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.64419808\n",
      "Iteration 5, loss = 0.61710777\n",
      "Iteration 6, loss = 0.58436404\n",
      "Iteration 7, loss = 0.54832427\n",
      "Iteration 8, loss = 0.51134673\n",
      "Iteration 9, loss = 0.47458388\n",
      "Iteration 10, loss = 0.43934336\n",
      "Iteration 11, loss = 0.40660200\n",
      "Iteration 12, loss = 0.37697557\n",
      "Iteration 13, loss = 0.35086453\n",
      "Iteration 14, loss = 0.32783120\n",
      "Iteration 15, loss = 0.30773122\n",
      "Iteration 16, loss = 0.29028374\n",
      "Iteration 17, loss = 0.27494242\n",
      "Iteration 18, loss = 0.26196229\n",
      "Iteration 19, loss = 0.25024392\n",
      "Iteration 20, loss = 0.23906378\n",
      "Iteration 21, loss = 0.22986891\n",
      "Iteration 22, loss = 0.22135449\n",
      "Iteration 23, loss = 0.21370793\n",
      "Iteration 24, loss = 0.20702229\n",
      "Iteration 25, loss = 0.20076098\n",
      "Iteration 26, loss = 0.19484183\n",
      "Iteration 27, loss = 0.18952479\n",
      "Iteration 28, loss = 0.18464665\n",
      "Iteration 29, loss = 0.18040595\n",
      "Iteration 30, loss = 0.17575751\n",
      "Iteration 31, loss = 0.17247170\n",
      "Iteration 32, loss = 0.16846962\n",
      "Iteration 33, loss = 0.16503694\n",
      "Iteration 34, loss = 0.16175746\n",
      "Iteration 35, loss = 0.15871374\n",
      "Iteration 36, loss = 0.15566068\n",
      "Iteration 37, loss = 0.15269909\n",
      "Iteration 38, loss = 0.15067279\n",
      "Iteration 39, loss = 0.14789965\n",
      "Iteration 40, loss = 0.14543992\n",
      "Iteration 41, loss = 0.14321321\n",
      "Iteration 42, loss = 0.14096692\n",
      "Iteration 43, loss = 0.13903497\n",
      "Iteration 44, loss = 0.13696018\n",
      "Iteration 45, loss = 0.13491917\n",
      "Iteration 46, loss = 0.13317515\n",
      "Iteration 47, loss = 0.13121103\n",
      "Iteration 48, loss = 0.12954588\n",
      "Iteration 49, loss = 0.12806605\n",
      "Iteration 50, loss = 0.12638205\n",
      "Iteration 51, loss = 0.12483183\n",
      "Iteration 52, loss = 0.12326241\n",
      "Iteration 53, loss = 0.12189670\n",
      "Iteration 54, loss = 0.12046115\n",
      "Iteration 55, loss = 0.11912038\n",
      "Iteration 56, loss = 0.11794869\n",
      "Iteration 57, loss = 0.11639804\n",
      "Iteration 58, loss = 0.11521232\n",
      "Iteration 59, loss = 0.11391399\n",
      "Iteration 60, loss = 0.11283503\n",
      "Iteration 61, loss = 0.11170799\n",
      "Iteration 62, loss = 0.11051334\n",
      "Iteration 63, loss = 0.10938700\n",
      "Iteration 64, loss = 0.10862997\n",
      "Iteration 65, loss = 0.10734165\n",
      "Iteration 66, loss = 0.10632672\n",
      "Iteration 67, loss = 0.10537134\n",
      "Iteration 68, loss = 0.10434668\n",
      "Iteration 69, loss = 0.10341374\n",
      "Iteration 70, loss = 0.10291060\n",
      "Iteration 71, loss = 0.10190554\n",
      "Iteration 72, loss = 0.10065786\n",
      "Iteration 73, loss = 0.10023286\n",
      "Iteration 74, loss = 0.09909712\n",
      "Iteration 75, loss = 0.09847632\n",
      "Iteration 76, loss = 0.09743063\n",
      "Iteration 77, loss = 0.09647999\n",
      "Iteration 78, loss = 0.09574747\n",
      "Iteration 79, loss = 0.09534853\n",
      "Iteration 80, loss = 0.09439465\n",
      "Iteration 81, loss = 0.09389413\n",
      "Iteration 82, loss = 0.09301324\n",
      "Iteration 83, loss = 0.09218669\n",
      "Iteration 84, loss = 0.09144457\n",
      "Iteration 85, loss = 0.09144121\n",
      "Iteration 86, loss = 0.09001330\n",
      "Iteration 87, loss = 0.08962434\n",
      "Iteration 88, loss = 0.08887630\n",
      "Iteration 89, loss = 0.08833394\n",
      "Iteration 90, loss = 0.08803858\n",
      "Iteration 91, loss = 0.08734097\n",
      "Iteration 92, loss = 0.08665006\n",
      "Iteration 93, loss = 0.08612206\n",
      "Iteration 94, loss = 0.08528657\n",
      "Iteration 95, loss = 0.08512953\n",
      "Iteration 96, loss = 0.08443658\n",
      "Iteration 97, loss = 0.08371531\n",
      "Iteration 98, loss = 0.08341219\n",
      "Iteration 99, loss = 0.08267894\n",
      "Iteration 100, loss = 0.08243378\n",
      "Iteration 101, loss = 0.08185508\n",
      "Iteration 102, loss = 0.08139647\n",
      "Iteration 103, loss = 0.08068561\n",
      "Iteration 104, loss = 0.08052416\n",
      "Iteration 105, loss = 0.08010692\n",
      "Iteration 106, loss = 0.07964947\n",
      "Iteration 107, loss = 0.07940748\n",
      "Iteration 108, loss = 0.07850512\n",
      "Iteration 109, loss = 0.07791477\n",
      "Iteration 110, loss = 0.07747131\n",
      "Iteration 111, loss = 0.07691331\n",
      "Iteration 112, loss = 0.07656669\n",
      "Iteration 113, loss = 0.07625969\n",
      "Iteration 114, loss = 0.07569791\n",
      "Iteration 115, loss = 0.07539730\n",
      "Iteration 116, loss = 0.07505996\n",
      "Iteration 117, loss = 0.07438694\n",
      "Iteration 118, loss = 0.07460091\n",
      "Iteration 119, loss = 0.07401971\n",
      "Iteration 120, loss = 0.07348249\n",
      "Iteration 121, loss = 0.07331172\n",
      "Iteration 122, loss = 0.07252902\n",
      "Iteration 123, loss = 0.07214245\n",
      "Iteration 124, loss = 0.07174681\n",
      "Iteration 125, loss = 0.07144434\n",
      "Iteration 126, loss = 0.07141605\n",
      "Iteration 127, loss = 0.07092834\n",
      "Iteration 128, loss = 0.07060443\n",
      "Iteration 129, loss = 0.07064008\n",
      "Iteration 130, loss = 0.06985514\n",
      "Iteration 131, loss = 0.06949663\n",
      "Iteration 132, loss = 0.06892545\n",
      "Iteration 133, loss = 0.06934928\n",
      "Iteration 134, loss = 0.06884798\n",
      "Iteration 135, loss = 0.06825510\n",
      "Iteration 136, loss = 0.06789874\n",
      "Iteration 137, loss = 0.06762166\n",
      "Iteration 138, loss = 0.06727204\n",
      "Iteration 139, loss = 0.06701029\n",
      "Iteration 140, loss = 0.06660707\n",
      "Iteration 141, loss = 0.06633885\n",
      "Iteration 142, loss = 0.06637606\n",
      "Iteration 143, loss = 0.06569226\n",
      "Iteration 144, loss = 0.06560898\n",
      "Iteration 145, loss = 0.06518282\n",
      "Iteration 146, loss = 0.06487453\n",
      "Iteration 147, loss = 0.06477348\n",
      "Iteration 148, loss = 0.06418704\n",
      "Iteration 149, loss = 0.06404873\n",
      "Iteration 150, loss = 0.06389265\n",
      "Iteration 151, loss = 0.06406893\n",
      "Iteration 152, loss = 0.06325575\n",
      "Iteration 153, loss = 0.06322843\n",
      "Iteration 154, loss = 0.06313291\n",
      "Iteration 155, loss = 0.06253020\n",
      "Iteration 156, loss = 0.06229191\n",
      "Iteration 157, loss = 0.06205837\n",
      "Iteration 158, loss = 0.06185907\n",
      "Iteration 159, loss = 0.06159009\n",
      "Iteration 160, loss = 0.06135557\n",
      "Iteration 161, loss = 0.06084734\n",
      "Iteration 162, loss = 0.06065359\n",
      "Iteration 163, loss = 0.06044105\n",
      "Iteration 164, loss = 0.06063506\n",
      "Iteration 165, loss = 0.06013194\n",
      "Iteration 166, loss = 0.05972864\n",
      "Iteration 167, loss = 0.05988792\n",
      "Iteration 168, loss = 0.05937431\n",
      "Iteration 169, loss = 0.05949173\n",
      "Iteration 170, loss = 0.05878388\n",
      "Iteration 171, loss = 0.05874795\n",
      "Iteration 172, loss = 0.05867422\n",
      "Iteration 173, loss = 0.05825798\n",
      "Iteration 174, loss = 0.05802242\n",
      "Iteration 175, loss = 0.05797554\n",
      "Iteration 176, loss = 0.05760551\n",
      "Iteration 177, loss = 0.05763077\n",
      "Iteration 178, loss = 0.05729906\n",
      "Iteration 179, loss = 0.05722445\n",
      "Iteration 180, loss = 0.05677844\n",
      "Iteration 181, loss = 0.05694103\n",
      "Iteration 182, loss = 0.05641558\n",
      "Iteration 183, loss = 0.05655489\n",
      "Iteration 184, loss = 0.05701959\n",
      "Iteration 185, loss = 0.05581240\n",
      "Iteration 186, loss = 0.05566864\n",
      "Iteration 187, loss = 0.05581225\n",
      "Iteration 188, loss = 0.05554776\n",
      "Iteration 189, loss = 0.05579160\n",
      "Iteration 190, loss = 0.05502823\n",
      "Iteration 191, loss = 0.05494010\n",
      "Iteration 192, loss = 0.05472994\n",
      "Iteration 193, loss = 0.05510350\n",
      "Iteration 194, loss = 0.05503947\n",
      "Iteration 195, loss = 0.05426131\n",
      "Iteration 196, loss = 0.05389678\n",
      "Iteration 197, loss = 0.05418718\n",
      "Iteration 198, loss = 0.05372742\n",
      "Iteration 199, loss = 0.05364691\n",
      "Iteration 200, loss = 0.05318208\n",
      "Iteration 201, loss = 0.05326657\n",
      "Iteration 202, loss = 0.05291705\n",
      "Iteration 203, loss = 0.05264875\n",
      "Iteration 204, loss = 0.05259630\n",
      "Iteration 205, loss = 0.05265079\n",
      "Iteration 206, loss = 0.05251016\n",
      "Iteration 207, loss = 0.05207698\n",
      "Iteration 208, loss = 0.05169398\n",
      "Iteration 209, loss = 0.05218708\n",
      "Iteration 210, loss = 0.05174995\n",
      "Iteration 211, loss = 0.05134549\n",
      "Iteration 212, loss = 0.05128491\n",
      "Iteration 213, loss = 0.05144868\n",
      "Iteration 214, loss = 0.05084499\n",
      "Iteration 215, loss = 0.05069904\n",
      "Iteration 216, loss = 0.05049173\n",
      "Iteration 217, loss = 0.05047991\n",
      "Iteration 218, loss = 0.05038298\n",
      "Iteration 219, loss = 0.05043925\n",
      "Iteration 220, loss = 0.05016146\n",
      "Iteration 221, loss = 0.04980208\n",
      "Iteration 222, loss = 0.04971175\n",
      "Iteration 223, loss = 0.05000318\n",
      "Iteration 224, loss = 0.04966479\n",
      "Iteration 225, loss = 0.04942735\n",
      "Iteration 226, loss = 0.04953082\n",
      "Iteration 227, loss = 0.04932636\n",
      "Iteration 228, loss = 0.04903525\n",
      "Iteration 229, loss = 0.04874681\n",
      "Iteration 230, loss = 0.04865886\n",
      "Iteration 231, loss = 0.04867786\n",
      "Iteration 232, loss = 0.04847984\n",
      "Iteration 233, loss = 0.04814759\n",
      "Iteration 234, loss = 0.04821743\n",
      "Iteration 235, loss = 0.04823904\n",
      "Iteration 236, loss = 0.04778430\n",
      "Iteration 237, loss = 0.04813997\n",
      "Iteration 238, loss = 0.04753788\n",
      "Iteration 239, loss = 0.04811176\n",
      "Iteration 240, loss = 0.04720287\n",
      "Iteration 241, loss = 0.04734986\n",
      "Iteration 242, loss = 0.04716884\n",
      "Iteration 243, loss = 0.04698915\n",
      "Iteration 244, loss = 0.04688544\n",
      "Iteration 245, loss = 0.04702660\n",
      "Iteration 246, loss = 0.04667674\n",
      "Iteration 247, loss = 0.04645497\n",
      "Iteration 248, loss = 0.04682292\n",
      "Iteration 249, loss = 0.04629709\n",
      "Iteration 250, loss = 0.04617825\n",
      "Iteration 251, loss = 0.04611538\n",
      "Iteration 252, loss = 0.04615789\n",
      "Iteration 253, loss = 0.04583279\n",
      "Iteration 254, loss = 0.04588129\n",
      "Iteration 255, loss = 0.04606712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.04556527\n",
      "Iteration 257, loss = 0.04523612\n",
      "Iteration 258, loss = 0.04540787\n",
      "Iteration 259, loss = 0.04560345\n",
      "Iteration 260, loss = 0.04524303\n",
      "Iteration 261, loss = 0.04491404\n",
      "Iteration 262, loss = 0.04482670\n",
      "Iteration 263, loss = 0.04478674\n",
      "Iteration 264, loss = 0.04511019\n",
      "Iteration 265, loss = 0.04512405\n",
      "Iteration 266, loss = 0.04534345\n",
      "Iteration 267, loss = 0.04443269\n",
      "Iteration 268, loss = 0.04463608\n",
      "Iteration 269, loss = 0.04428257\n",
      "Iteration 270, loss = 0.04378629\n",
      "Iteration 271, loss = 0.04415520\n",
      "Iteration 272, loss = 0.04393999\n",
      "Iteration 273, loss = 0.04373090\n",
      "Iteration 274, loss = 0.04365087\n",
      "Iteration 275, loss = 0.04357150\n",
      "Iteration 276, loss = 0.04397242\n",
      "Iteration 277, loss = 0.04365153\n",
      "Iteration 278, loss = 0.04321872\n",
      "Iteration 279, loss = 0.04335284\n",
      "Iteration 280, loss = 0.04329107\n",
      "Iteration 281, loss = 0.04294182\n",
      "Iteration 282, loss = 0.04290994\n",
      "Iteration 283, loss = 0.04296267\n",
      "Iteration 284, loss = 0.04288332\n",
      "Iteration 285, loss = 0.04266946\n",
      "Iteration 286, loss = 0.04243551\n",
      "Iteration 287, loss = 0.04237671\n",
      "Iteration 288, loss = 0.04305252\n",
      "Iteration 289, loss = 0.04217890\n",
      "Iteration 290, loss = 0.04225402\n",
      "Iteration 291, loss = 0.04215281\n",
      "Iteration 292, loss = 0.04219531\n",
      "Iteration 293, loss = 0.04186264\n",
      "Iteration 294, loss = 0.04183484\n",
      "Iteration 295, loss = 0.04159409\n",
      "Iteration 296, loss = 0.04164002\n",
      "Iteration 297, loss = 0.04153772\n",
      "Iteration 298, loss = 0.04140568\n",
      "Iteration 299, loss = 0.04129638\n",
      "Iteration 300, loss = 0.04145767\n",
      "Iteration 301, loss = 0.04122191\n",
      "Iteration 302, loss = 0.04125563\n",
      "Iteration 303, loss = 0.04102958\n",
      "Iteration 304, loss = 0.04111611\n",
      "Iteration 305, loss = 0.04102761\n",
      "Iteration 306, loss = 0.04067918\n",
      "Iteration 307, loss = 0.04082653\n",
      "Iteration 308, loss = 0.04087948\n",
      "Iteration 309, loss = 0.04099514\n",
      "Iteration 310, loss = 0.04090150\n",
      "Iteration 311, loss = 0.04043759\n",
      "Iteration 312, loss = 0.04028166\n",
      "Iteration 313, loss = 0.04092349\n",
      "Iteration 314, loss = 0.04040299\n",
      "Iteration 315, loss = 0.04075090\n",
      "Iteration 316, loss = 0.04024776\n",
      "Iteration 317, loss = 0.03991690\n",
      "Iteration 318, loss = 0.04005261\n",
      "Iteration 319, loss = 0.03991709\n",
      "Iteration 320, loss = 0.03966446\n",
      "Iteration 321, loss = 0.03973050\n",
      "Iteration 322, loss = 0.03967691\n",
      "Iteration 323, loss = 0.03950813\n",
      "Iteration 324, loss = 0.03987569\n",
      "Iteration 325, loss = 0.04035092\n",
      "Iteration 326, loss = 0.03973907\n",
      "Iteration 327, loss = 0.03954723\n",
      "Iteration 328, loss = 0.03954103\n",
      "Iteration 329, loss = 0.03933261\n",
      "Iteration 330, loss = 0.03901756\n",
      "Iteration 331, loss = 0.03899726\n",
      "Iteration 332, loss = 0.03915826\n",
      "Iteration 333, loss = 0.03911144\n",
      "Iteration 334, loss = 0.03839422\n",
      "Iteration 335, loss = 0.03884794\n",
      "Iteration 336, loss = 0.03883340\n",
      "Iteration 337, loss = 0.03886297\n",
      "Iteration 338, loss = 0.03841431\n",
      "Iteration 339, loss = 0.03837800\n",
      "Iteration 340, loss = 0.03883879\n",
      "Iteration 341, loss = 0.03856390\n",
      "Iteration 342, loss = 0.03813045\n",
      "Iteration 343, loss = 0.03865906\n",
      "Iteration 344, loss = 0.03853730\n",
      "Iteration 345, loss = 0.03846647\n",
      "Iteration 346, loss = 0.03866656\n",
      "Iteration 347, loss = 0.03821894\n",
      "Iteration 348, loss = 0.03813921\n",
      "Iteration 349, loss = 0.03805792\n",
      "Iteration 350, loss = 0.03796278\n",
      "Iteration 351, loss = 0.03824422\n",
      "Iteration 352, loss = 0.03793172\n",
      "Iteration 353, loss = 0.03769096\n",
      "Iteration 354, loss = 0.03741843\n",
      "Iteration 355, loss = 0.03754157\n",
      "Iteration 356, loss = 0.03735814\n",
      "Iteration 357, loss = 0.03768848\n",
      "Iteration 358, loss = 0.03742231\n",
      "Iteration 359, loss = 0.03722727\n",
      "Iteration 360, loss = 0.03715322\n",
      "Iteration 361, loss = 0.03785567\n",
      "Iteration 362, loss = 0.03764253\n",
      "Iteration 363, loss = 0.03723064\n",
      "Iteration 364, loss = 0.03725727\n",
      "Iteration 365, loss = 0.03744118\n",
      "Iteration 366, loss = 0.03704041\n",
      "Iteration 367, loss = 0.03681119\n",
      "Iteration 368, loss = 0.03686239\n",
      "Iteration 369, loss = 0.03695084\n",
      "Iteration 370, loss = 0.03685666\n",
      "Iteration 371, loss = 0.03654699\n",
      "Iteration 372, loss = 0.03692802\n",
      "Iteration 373, loss = 0.03661832\n",
      "Iteration 374, loss = 0.03666317\n",
      "Iteration 375, loss = 0.03635157\n",
      "Iteration 376, loss = 0.03667105\n",
      "Iteration 377, loss = 0.03640973\n",
      "Iteration 378, loss = 0.03629416\n",
      "Iteration 379, loss = 0.03711481\n",
      "Iteration 380, loss = 0.03619115\n",
      "Iteration 381, loss = 0.03622532\n",
      "Iteration 382, loss = 0.03607833\n",
      "Iteration 383, loss = 0.03601121\n",
      "Iteration 384, loss = 0.03610126\n",
      "Iteration 385, loss = 0.03596512\n",
      "Iteration 386, loss = 0.03595028\n",
      "Iteration 387, loss = 0.03703621\n",
      "Iteration 388, loss = 0.03595510\n",
      "Iteration 389, loss = 0.03599495\n",
      "Iteration 390, loss = 0.03597894\n",
      "Iteration 391, loss = 0.03586849\n",
      "Iteration 392, loss = 0.03566348\n",
      "Iteration 393, loss = 0.03573063\n",
      "Iteration 394, loss = 0.03571385\n",
      "Iteration 395, loss = 0.03574223\n",
      "Iteration 396, loss = 0.03575177\n",
      "Iteration 397, loss = 0.03562899\n",
      "Iteration 398, loss = 0.03614666\n",
      "Iteration 399, loss = 0.03529904\n",
      "Iteration 400, loss = 0.03559961\n",
      "Iteration 401, loss = 0.03565921\n",
      "Iteration 402, loss = 0.03532598\n",
      "Iteration 403, loss = 0.03512479\n",
      "Iteration 404, loss = 0.03563530\n",
      "Iteration 405, loss = 0.03480058\n",
      "Iteration 406, loss = 0.03518736\n",
      "Iteration 407, loss = 0.03518653\n",
      "Iteration 408, loss = 0.03538146\n",
      "Iteration 409, loss = 0.03511075\n",
      "Iteration 410, loss = 0.03491409\n",
      "Iteration 411, loss = 0.03477779\n",
      "Iteration 412, loss = 0.03472772\n",
      "Iteration 413, loss = 0.03489610\n",
      "Iteration 414, loss = 0.03467353\n",
      "Iteration 415, loss = 0.03492131\n",
      "Iteration 416, loss = 0.03484876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 91.26738794435857\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.68996374\n",
      "Iteration 2, loss = 0.67962963\n",
      "Iteration 3, loss = 0.66466274\n",
      "Iteration 4, loss = 0.64341090\n",
      "Iteration 5, loss = 0.61609528\n",
      "Iteration 6, loss = 0.58346432\n",
      "Iteration 7, loss = 0.54722617\n",
      "Iteration 8, loss = 0.50948396\n",
      "Iteration 9, loss = 0.47213981\n",
      "Iteration 10, loss = 0.43648648\n",
      "Iteration 11, loss = 0.40399564\n",
      "Iteration 12, loss = 0.37452941\n",
      "Iteration 13, loss = 0.34800932\n",
      "Iteration 14, loss = 0.32482428\n",
      "Iteration 15, loss = 0.30473430\n",
      "Iteration 16, loss = 0.28779552\n",
      "Iteration 17, loss = 0.27238077\n",
      "Iteration 18, loss = 0.25865031\n",
      "Iteration 19, loss = 0.24683134\n",
      "Iteration 20, loss = 0.23763728\n",
      "Iteration 21, loss = 0.22768734\n",
      "Iteration 22, loss = 0.21943381\n",
      "Iteration 23, loss = 0.21218861\n",
      "Iteration 24, loss = 0.20549937\n",
      "Iteration 25, loss = 0.19948129\n",
      "Iteration 26, loss = 0.19384725\n",
      "Iteration 27, loss = 0.18876785\n",
      "Iteration 28, loss = 0.18436528\n",
      "Iteration 29, loss = 0.18005036\n",
      "Iteration 30, loss = 0.17577629\n",
      "Iteration 31, loss = 0.17194564\n",
      "Iteration 32, loss = 0.16892898\n",
      "Iteration 33, loss = 0.16586498\n",
      "Iteration 34, loss = 0.16230730\n",
      "Iteration 35, loss = 0.15932543\n",
      "Iteration 36, loss = 0.15636561\n",
      "Iteration 37, loss = 0.15394363\n",
      "Iteration 38, loss = 0.15135387\n",
      "Iteration 39, loss = 0.14888210\n",
      "Iteration 40, loss = 0.14669274\n",
      "Iteration 41, loss = 0.14453806\n",
      "Iteration 42, loss = 0.14236932\n",
      "Iteration 43, loss = 0.14054585\n",
      "Iteration 44, loss = 0.13835915\n",
      "Iteration 45, loss = 0.13696617\n",
      "Iteration 46, loss = 0.13503408\n",
      "Iteration 47, loss = 0.13346556\n",
      "Iteration 48, loss = 0.13148008\n",
      "Iteration 49, loss = 0.13041165\n",
      "Iteration 50, loss = 0.12856187\n",
      "Iteration 51, loss = 0.12739524\n",
      "Iteration 52, loss = 0.12565387\n",
      "Iteration 53, loss = 0.12440547\n",
      "Iteration 54, loss = 0.12356961\n",
      "Iteration 55, loss = 0.12176197\n",
      "Iteration 56, loss = 0.12034704\n",
      "Iteration 57, loss = 0.11902800\n",
      "Iteration 58, loss = 0.11789521\n",
      "Iteration 59, loss = 0.11675293\n",
      "Iteration 60, loss = 0.11559977\n",
      "Iteration 61, loss = 0.11437793\n",
      "Iteration 62, loss = 0.11339498\n",
      "Iteration 63, loss = 0.11255901\n",
      "Iteration 64, loss = 0.11130105\n",
      "Iteration 65, loss = 0.11018832\n",
      "Iteration 66, loss = 0.10946200\n",
      "Iteration 67, loss = 0.10831232\n",
      "Iteration 68, loss = 0.10773567\n",
      "Iteration 69, loss = 0.10679533\n",
      "Iteration 70, loss = 0.10570585\n",
      "Iteration 71, loss = 0.10492109\n",
      "Iteration 72, loss = 0.10425406\n",
      "Iteration 73, loss = 0.10329976\n",
      "Iteration 74, loss = 0.10250068\n",
      "Iteration 75, loss = 0.10169383\n",
      "Iteration 76, loss = 0.10090635\n",
      "Iteration 77, loss = 0.09986779\n",
      "Iteration 78, loss = 0.09903328\n",
      "Iteration 79, loss = 0.09858706\n",
      "Iteration 80, loss = 0.09756699\n",
      "Iteration 81, loss = 0.09715108\n",
      "Iteration 82, loss = 0.09637927\n",
      "Iteration 83, loss = 0.09535485\n",
      "Iteration 84, loss = 0.09484661\n",
      "Iteration 85, loss = 0.09418419\n",
      "Iteration 86, loss = 0.09372106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87, loss = 0.09297896\n",
      "Iteration 88, loss = 0.09230871\n",
      "Iteration 89, loss = 0.09217677\n",
      "Iteration 90, loss = 0.09123493\n",
      "Iteration 91, loss = 0.09050868\n",
      "Iteration 92, loss = 0.08994455\n",
      "Iteration 93, loss = 0.08919011\n",
      "Iteration 94, loss = 0.08894020\n",
      "Iteration 95, loss = 0.08849108\n",
      "Iteration 96, loss = 0.08798831\n",
      "Iteration 97, loss = 0.08721624\n",
      "Iteration 98, loss = 0.08667306\n",
      "Iteration 99, loss = 0.08629121\n",
      "Iteration 100, loss = 0.08618353\n",
      "Iteration 101, loss = 0.08514037\n",
      "Iteration 102, loss = 0.08463517\n",
      "Iteration 103, loss = 0.08405927\n",
      "Iteration 104, loss = 0.08344320\n",
      "Iteration 105, loss = 0.08313638\n",
      "Iteration 106, loss = 0.08235315\n",
      "Iteration 107, loss = 0.08285115\n",
      "Iteration 108, loss = 0.08206824\n",
      "Iteration 109, loss = 0.08123141\n",
      "Iteration 110, loss = 0.08078251\n",
      "Iteration 111, loss = 0.08066069\n",
      "Iteration 112, loss = 0.08009994\n",
      "Iteration 113, loss = 0.07967530\n",
      "Iteration 114, loss = 0.07907102\n",
      "Iteration 115, loss = 0.07879922\n",
      "Iteration 116, loss = 0.07827800\n",
      "Iteration 117, loss = 0.07830329\n",
      "Iteration 118, loss = 0.07795602\n",
      "Iteration 119, loss = 0.07740018\n",
      "Iteration 120, loss = 0.07682708\n",
      "Iteration 121, loss = 0.07689208\n",
      "Iteration 122, loss = 0.07613100\n",
      "Iteration 123, loss = 0.07560143\n",
      "Iteration 124, loss = 0.07522831\n",
      "Iteration 125, loss = 0.07477540\n",
      "Iteration 126, loss = 0.07501504\n",
      "Iteration 127, loss = 0.07420917\n",
      "Iteration 128, loss = 0.07360654\n",
      "Iteration 129, loss = 0.07391092\n",
      "Iteration 130, loss = 0.07308668\n",
      "Iteration 131, loss = 0.07269487\n",
      "Iteration 132, loss = 0.07237530\n",
      "Iteration 133, loss = 0.07222812\n",
      "Iteration 134, loss = 0.07179818\n",
      "Iteration 135, loss = 0.07132529\n",
      "Iteration 136, loss = 0.07120530\n",
      "Iteration 137, loss = 0.07072598\n",
      "Iteration 138, loss = 0.07038370\n",
      "Iteration 139, loss = 0.07003445\n",
      "Iteration 140, loss = 0.07029445\n",
      "Iteration 141, loss = 0.06954683\n",
      "Iteration 142, loss = 0.06933170\n",
      "Iteration 143, loss = 0.06898150\n",
      "Iteration 144, loss = 0.06875573\n",
      "Iteration 145, loss = 0.06827410\n",
      "Iteration 146, loss = 0.06801083\n",
      "Iteration 147, loss = 0.06773522\n",
      "Iteration 148, loss = 0.06738643\n",
      "Iteration 149, loss = 0.06810432\n",
      "Iteration 150, loss = 0.06704457\n",
      "Iteration 151, loss = 0.06662763\n",
      "Iteration 152, loss = 0.06621161\n",
      "Iteration 153, loss = 0.06608630\n",
      "Iteration 154, loss = 0.06580427\n",
      "Iteration 155, loss = 0.06555250\n",
      "Iteration 156, loss = 0.06536174\n",
      "Iteration 157, loss = 0.06504607\n",
      "Iteration 158, loss = 0.06497363\n",
      "Iteration 159, loss = 0.06475571\n",
      "Iteration 160, loss = 0.06423136\n",
      "Iteration 161, loss = 0.06409728\n",
      "Iteration 162, loss = 0.06395638\n",
      "Iteration 163, loss = 0.06340675\n",
      "Iteration 164, loss = 0.06320173\n",
      "Iteration 165, loss = 0.06297219\n",
      "Iteration 166, loss = 0.06300109\n",
      "Iteration 167, loss = 0.06238430\n",
      "Iteration 168, loss = 0.06263932\n",
      "Iteration 169, loss = 0.06213417\n",
      "Iteration 170, loss = 0.06207494\n",
      "Iteration 171, loss = 0.06153525\n",
      "Iteration 172, loss = 0.06142557\n",
      "Iteration 173, loss = 0.06105009\n",
      "Iteration 174, loss = 0.06091454\n",
      "Iteration 175, loss = 0.06089427\n",
      "Iteration 176, loss = 0.06148716\n",
      "Iteration 177, loss = 0.06124123\n",
      "Iteration 178, loss = 0.06030012\n",
      "Iteration 179, loss = 0.05969991\n",
      "Iteration 180, loss = 0.05956526\n",
      "Iteration 181, loss = 0.05943761\n",
      "Iteration 182, loss = 0.05948364\n",
      "Iteration 183, loss = 0.05913020\n",
      "Iteration 184, loss = 0.05884022\n",
      "Iteration 185, loss = 0.05888906\n",
      "Iteration 186, loss = 0.05821073\n",
      "Iteration 187, loss = 0.05822804\n",
      "Iteration 188, loss = 0.05836607\n",
      "Iteration 189, loss = 0.05784287\n",
      "Iteration 190, loss = 0.05749593\n",
      "Iteration 191, loss = 0.05738035\n",
      "Iteration 192, loss = 0.05727830\n",
      "Iteration 193, loss = 0.05728687\n",
      "Iteration 194, loss = 0.05677950\n",
      "Iteration 195, loss = 0.05650134\n",
      "Iteration 196, loss = 0.05678756\n",
      "Iteration 197, loss = 0.05640915\n",
      "Iteration 198, loss = 0.05627254\n",
      "Iteration 199, loss = 0.05616728\n",
      "Iteration 200, loss = 0.05565300\n",
      "Iteration 201, loss = 0.05556007\n",
      "Iteration 202, loss = 0.05528391\n",
      "Iteration 203, loss = 0.05521048\n",
      "Iteration 204, loss = 0.05526304\n",
      "Iteration 205, loss = 0.05511354\n",
      "Iteration 206, loss = 0.05494386\n",
      "Iteration 207, loss = 0.05485720\n",
      "Iteration 208, loss = 0.05425773\n",
      "Iteration 209, loss = 0.05422902\n",
      "Iteration 210, loss = 0.05396343\n",
      "Iteration 211, loss = 0.05384133\n",
      "Iteration 212, loss = 0.05371750\n",
      "Iteration 213, loss = 0.05327295\n",
      "Iteration 214, loss = 0.05335848\n",
      "Iteration 215, loss = 0.05313511\n",
      "Iteration 216, loss = 0.05304576\n",
      "Iteration 217, loss = 0.05288258\n",
      "Iteration 218, loss = 0.05266763\n",
      "Iteration 219, loss = 0.05248766\n",
      "Iteration 220, loss = 0.05238652\n",
      "Iteration 221, loss = 0.05236379\n",
      "Iteration 222, loss = 0.05187942\n",
      "Iteration 223, loss = 0.05189456\n",
      "Iteration 224, loss = 0.05160639\n",
      "Iteration 225, loss = 0.05147345\n",
      "Iteration 226, loss = 0.05143232\n",
      "Iteration 227, loss = 0.05116266\n",
      "Iteration 228, loss = 0.05086180\n",
      "Iteration 229, loss = 0.05092993\n",
      "Iteration 230, loss = 0.05065032\n",
      "Iteration 231, loss = 0.05072363\n",
      "Iteration 232, loss = 0.05039637\n",
      "Iteration 233, loss = 0.05029264\n",
      "Iteration 234, loss = 0.05048065\n",
      "Iteration 235, loss = 0.05021784\n",
      "Iteration 236, loss = 0.05016115\n",
      "Iteration 237, loss = 0.04977049\n",
      "Iteration 238, loss = 0.04992706\n",
      "Iteration 239, loss = 0.04946171\n",
      "Iteration 240, loss = 0.04963439\n",
      "Iteration 241, loss = 0.04907947\n",
      "Iteration 242, loss = 0.04941577\n",
      "Iteration 243, loss = 0.04869133\n",
      "Iteration 244, loss = 0.04894401\n",
      "Iteration 245, loss = 0.04910792\n",
      "Iteration 246, loss = 0.04849086\n",
      "Iteration 247, loss = 0.04824621\n",
      "Iteration 248, loss = 0.04813099\n",
      "Iteration 249, loss = 0.04805862\n",
      "Iteration 250, loss = 0.04787486\n",
      "Iteration 251, loss = 0.04777589\n",
      "Iteration 252, loss = 0.04756667\n",
      "Iteration 253, loss = 0.04775053\n",
      "Iteration 254, loss = 0.04730740\n",
      "Iteration 255, loss = 0.04706164\n",
      "Iteration 256, loss = 0.04722068\n",
      "Iteration 257, loss = 0.04696961\n",
      "Iteration 258, loss = 0.04691338\n",
      "Iteration 259, loss = 0.04683540\n",
      "Iteration 260, loss = 0.04643302\n",
      "Iteration 261, loss = 0.04702787\n",
      "Iteration 262, loss = 0.04634997\n",
      "Iteration 263, loss = 0.04640917\n",
      "Iteration 264, loss = 0.04636731\n",
      "Iteration 265, loss = 0.04590719\n",
      "Iteration 266, loss = 0.04586759\n",
      "Iteration 267, loss = 0.04602757\n",
      "Iteration 268, loss = 0.04579024\n",
      "Iteration 269, loss = 0.04584436\n",
      "Iteration 270, loss = 0.04543256\n",
      "Iteration 271, loss = 0.04525656\n",
      "Iteration 272, loss = 0.04522131\n",
      "Iteration 273, loss = 0.04506316\n",
      "Iteration 274, loss = 0.04501449\n",
      "Iteration 275, loss = 0.04528685\n",
      "Iteration 276, loss = 0.04492322\n",
      "Iteration 277, loss = 0.04443416\n",
      "Iteration 278, loss = 0.04461442\n",
      "Iteration 279, loss = 0.04433327\n",
      "Iteration 280, loss = 0.04398487\n",
      "Iteration 281, loss = 0.04417360\n",
      "Iteration 282, loss = 0.04464386\n",
      "Iteration 283, loss = 0.04403364\n",
      "Iteration 284, loss = 0.04382119\n",
      "Iteration 285, loss = 0.04399719\n",
      "Iteration 286, loss = 0.04370605\n",
      "Iteration 287, loss = 0.04345214\n",
      "Iteration 288, loss = 0.04323021\n",
      "Iteration 289, loss = 0.04334159\n",
      "Iteration 290, loss = 0.04360006\n",
      "Iteration 291, loss = 0.04327890\n",
      "Iteration 292, loss = 0.04294339\n",
      "Iteration 293, loss = 0.04322593\n",
      "Iteration 294, loss = 0.04318427\n",
      "Iteration 295, loss = 0.04278792\n",
      "Iteration 296, loss = 0.04281489\n",
      "Iteration 297, loss = 0.04275642\n",
      "Iteration 298, loss = 0.04229496\n",
      "Iteration 299, loss = 0.04256828\n",
      "Iteration 300, loss = 0.04210631\n",
      "Iteration 301, loss = 0.04219180\n",
      "Iteration 302, loss = 0.04193630\n",
      "Iteration 303, loss = 0.04190487\n",
      "Iteration 304, loss = 0.04211596\n",
      "Iteration 305, loss = 0.04222516\n",
      "Iteration 306, loss = 0.04120605\n",
      "Iteration 307, loss = 0.04174278\n",
      "Iteration 308, loss = 0.04169614\n",
      "Iteration 309, loss = 0.04197853\n",
      "Iteration 310, loss = 0.04187997\n",
      "Iteration 311, loss = 0.04124656\n",
      "Iteration 312, loss = 0.04164747\n",
      "Iteration 313, loss = 0.04108043\n",
      "Iteration 314, loss = 0.04081097\n",
      "Iteration 315, loss = 0.04123488\n",
      "Iteration 316, loss = 0.04095859\n",
      "Iteration 317, loss = 0.04094350\n",
      "Iteration 318, loss = 0.04078729\n",
      "Iteration 319, loss = 0.04040273\n",
      "Iteration 320, loss = 0.04083588\n",
      "Iteration 321, loss = 0.04069803\n",
      "Iteration 322, loss = 0.04120595\n",
      "Iteration 323, loss = 0.04019674\n",
      "Iteration 324, loss = 0.04000743\n",
      "Iteration 325, loss = 0.04026521\n",
      "Iteration 326, loss = 0.04020044\n",
      "Iteration 327, loss = 0.03989996\n",
      "Iteration 328, loss = 0.03969002\n",
      "Iteration 329, loss = 0.03977618\n",
      "Iteration 330, loss = 0.03970335\n",
      "Iteration 331, loss = 0.04015456\n",
      "Iteration 332, loss = 0.03994921\n",
      "Iteration 333, loss = 0.03952992\n",
      "Iteration 334, loss = 0.03946031\n",
      "Iteration 335, loss = 0.03975370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 336, loss = 0.03910497\n",
      "Iteration 337, loss = 0.03949339\n",
      "Iteration 338, loss = 0.03918429\n",
      "Iteration 339, loss = 0.03959649\n",
      "Iteration 340, loss = 0.03964514\n",
      "Iteration 341, loss = 0.03922813\n",
      "Iteration 342, loss = 0.03954418\n",
      "Iteration 343, loss = 0.03948535\n",
      "Iteration 344, loss = 0.03942450\n",
      "Iteration 345, loss = 0.03871408\n",
      "Iteration 346, loss = 0.03875615\n",
      "Iteration 347, loss = 0.03849072\n",
      "Iteration 348, loss = 0.03922645\n",
      "Iteration 349, loss = 0.03836241\n",
      "Iteration 350, loss = 0.03845444\n",
      "Iteration 351, loss = 0.03870187\n",
      "Iteration 352, loss = 0.03879055\n",
      "Iteration 353, loss = 0.03804261\n",
      "Iteration 354, loss = 0.03819875\n",
      "Iteration 355, loss = 0.03792217\n",
      "Iteration 356, loss = 0.03771742\n",
      "Iteration 357, loss = 0.03787189\n",
      "Iteration 358, loss = 0.03838811\n",
      "Iteration 359, loss = 0.03753731\n",
      "Iteration 360, loss = 0.03843163\n",
      "Iteration 361, loss = 0.03758147\n",
      "Iteration 362, loss = 0.03740381\n",
      "Iteration 363, loss = 0.03811207\n",
      "Iteration 364, loss = 0.03750831\n",
      "Iteration 365, loss = 0.03771932\n",
      "Iteration 366, loss = 0.03749510\n",
      "Iteration 367, loss = 0.03757797\n",
      "Iteration 368, loss = 0.03744782\n",
      "Iteration 369, loss = 0.03723666\n",
      "Iteration 370, loss = 0.03780245\n",
      "Iteration 371, loss = 0.03702381\n",
      "Iteration 372, loss = 0.03707587\n",
      "Iteration 373, loss = 0.03696800\n",
      "Iteration 374, loss = 0.03746644\n",
      "Iteration 375, loss = 0.03721174\n",
      "Iteration 376, loss = 0.03779821\n",
      "Iteration 377, loss = 0.03743065\n",
      "Iteration 378, loss = 0.03661338\n",
      "Iteration 379, loss = 0.03648189\n",
      "Iteration 380, loss = 0.03710540\n",
      "Iteration 381, loss = 0.03660061\n",
      "Iteration 382, loss = 0.03642484\n",
      "Iteration 383, loss = 0.03697264\n",
      "Iteration 384, loss = 0.03676618\n",
      "Iteration 385, loss = 0.03654068\n",
      "Iteration 386, loss = 0.03637460\n",
      "Iteration 387, loss = 0.03662491\n",
      "Iteration 388, loss = 0.03661172\n",
      "Iteration 389, loss = 0.03611222\n",
      "Iteration 390, loss = 0.03640870\n",
      "Iteration 391, loss = 0.03680002\n",
      "Iteration 392, loss = 0.03631376\n",
      "Iteration 393, loss = 0.03594999\n",
      "Iteration 394, loss = 0.03587562\n",
      "Iteration 395, loss = 0.03637768\n",
      "Iteration 396, loss = 0.03672750\n",
      "Iteration 397, loss = 0.03587029\n",
      "Iteration 398, loss = 0.03584463\n",
      "Iteration 399, loss = 0.03557054\n",
      "Iteration 400, loss = 0.03637488\n",
      "Iteration 401, loss = 0.03562806\n",
      "Iteration 402, loss = 0.03555788\n",
      "Iteration 403, loss = 0.03509644\n",
      "Iteration 404, loss = 0.03528973\n",
      "Iteration 405, loss = 0.03551868\n",
      "Iteration 406, loss = 0.03680521\n",
      "Iteration 407, loss = 0.03530968\n",
      "Iteration 408, loss = 0.03535040\n",
      "Iteration 409, loss = 0.03495806\n",
      "Iteration 410, loss = 0.03491933\n",
      "Iteration 411, loss = 0.03511178\n",
      "Iteration 412, loss = 0.03532765\n",
      "Iteration 413, loss = 0.03509971\n",
      "Iteration 414, loss = 0.03486226\n",
      "Iteration 415, loss = 0.03471575\n",
      "Iteration 416, loss = 0.03520367\n",
      "Iteration 417, loss = 0.03527573\n",
      "Iteration 418, loss = 0.03479202\n",
      "Iteration 419, loss = 0.03466203\n",
      "Iteration 420, loss = 0.03469525\n",
      "Iteration 421, loss = 0.03467588\n",
      "Iteration 422, loss = 0.03486003\n",
      "Iteration 423, loss = 0.03436753\n",
      "Iteration 424, loss = 0.03500276\n",
      "Iteration 425, loss = 0.03492322\n",
      "Iteration 426, loss = 0.03459650\n",
      "Iteration 427, loss = 0.03437973\n",
      "Iteration 428, loss = 0.03448106\n",
      "Iteration 429, loss = 0.03490494\n",
      "Iteration 430, loss = 0.03443676\n",
      "Iteration 431, loss = 0.03431071\n",
      "Iteration 432, loss = 0.03419634\n",
      "Iteration 433, loss = 0.03401873\n",
      "Iteration 434, loss = 0.03433931\n",
      "Iteration 435, loss = 0.03478725\n",
      "Iteration 436, loss = 0.03461458\n",
      "Iteration 437, loss = 0.03539042\n",
      "Iteration 438, loss = 0.03415234\n",
      "Iteration 439, loss = 0.03387334\n",
      "Iteration 440, loss = 0.03421261\n",
      "Iteration 441, loss = 0.03424100\n",
      "Iteration 442, loss = 0.03389386\n",
      "Iteration 443, loss = 0.03376932\n",
      "Iteration 444, loss = 0.03423917\n",
      "Iteration 445, loss = 0.03431135\n",
      "Iteration 446, loss = 0.03366571\n",
      "Iteration 447, loss = 0.03446318\n",
      "Iteration 448, loss = 0.03463437\n",
      "Iteration 449, loss = 0.03368023\n",
      "Iteration 450, loss = 0.03339237\n",
      "Iteration 451, loss = 0.03370640\n",
      "Iteration 452, loss = 0.03343176\n",
      "Iteration 453, loss = 0.03398362\n",
      "Iteration 454, loss = 0.03395721\n",
      "Iteration 455, loss = 0.03304970\n",
      "Iteration 456, loss = 0.03335278\n",
      "Iteration 457, loss = 0.03318219\n",
      "Iteration 458, loss = 0.03368166\n",
      "Iteration 459, loss = 0.03528049\n",
      "Iteration 460, loss = 0.03365378\n",
      "Iteration 461, loss = 0.03341521\n",
      "Iteration 462, loss = 0.03334494\n",
      "Iteration 463, loss = 0.03331799\n",
      "Iteration 464, loss = 0.03309743\n",
      "Iteration 465, loss = 0.03318591\n",
      "Iteration 466, loss = 0.03280536\n",
      "Iteration 467, loss = 0.03316859\n",
      "Iteration 468, loss = 0.03341485\n",
      "Iteration 469, loss = 0.03338323\n",
      "Iteration 470, loss = 0.03260043\n",
      "Iteration 471, loss = 0.03285008\n",
      "Iteration 472, loss = 0.03291766\n",
      "Iteration 473, loss = 0.03276338\n",
      "Iteration 474, loss = 0.03283286\n",
      "Iteration 475, loss = 0.03263998\n",
      "Iteration 476, loss = 0.03300616\n",
      "Iteration 477, loss = 0.03244055\n",
      "Iteration 478, loss = 0.03257901\n",
      "Iteration 479, loss = 0.03267702\n",
      "Iteration 480, loss = 0.03253857\n",
      "Iteration 481, loss = 0.03245344\n",
      "Iteration 482, loss = 0.03321600\n",
      "Iteration 483, loss = 0.03259493\n",
      "Iteration 484, loss = 0.03238871\n",
      "Iteration 485, loss = 0.03244064\n",
      "Iteration 486, loss = 0.03293415\n",
      "Iteration 487, loss = 0.03216420\n",
      "Iteration 488, loss = 0.03235355\n",
      "Iteration 489, loss = 0.03241692\n",
      "Iteration 490, loss = 0.03228128\n",
      "Iteration 491, loss = 0.03247807\n",
      "Iteration 492, loss = 0.03239664\n",
      "Iteration 493, loss = 0.03216484\n",
      "Iteration 494, loss = 0.03210270\n",
      "Iteration 495, loss = 0.03225119\n",
      "Iteration 496, loss = 0.03227109\n",
      "Iteration 497, loss = 0.03200238\n",
      "Iteration 498, loss = 0.03201857\n",
      "Iteration 499, loss = 0.03229223\n",
      "Iteration 500, loss = 0.03209780\n",
      "Iteration 501, loss = 0.03210317\n",
      "Iteration 502, loss = 0.03232822\n",
      "Iteration 503, loss = 0.03171399\n",
      "Iteration 504, loss = 0.03179854\n",
      "Iteration 505, loss = 0.03180478\n",
      "Iteration 506, loss = 0.03176282\n",
      "Iteration 507, loss = 0.03219620\n",
      "Iteration 508, loss = 0.03180609\n",
      "Iteration 509, loss = 0.03206227\n",
      "Iteration 510, loss = 0.03199053\n",
      "Iteration 511, loss = 0.03176904\n",
      "Iteration 512, loss = 0.03167830\n",
      "Iteration 513, loss = 0.03201468\n",
      "Iteration 514, loss = 0.03178575\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 91.26738794435857\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.68997427\n",
      "Iteration 2, loss = 0.68028237\n",
      "Iteration 3, loss = 0.66568047\n",
      "Iteration 4, loss = 0.64463410\n",
      "Iteration 5, loss = 0.61718750\n",
      "Iteration 6, loss = 0.58435681\n",
      "Iteration 7, loss = 0.54812913\n",
      "Iteration 8, loss = 0.51178252\n",
      "Iteration 9, loss = 0.47445494\n",
      "Iteration 10, loss = 0.43976025\n",
      "Iteration 11, loss = 0.40693445\n",
      "Iteration 12, loss = 0.37776256\n",
      "Iteration 13, loss = 0.35194042\n",
      "Iteration 14, loss = 0.32929798\n",
      "Iteration 15, loss = 0.30993958\n",
      "Iteration 16, loss = 0.29298514\n",
      "Iteration 17, loss = 0.27761954\n",
      "Iteration 18, loss = 0.26425803\n",
      "Iteration 19, loss = 0.25263563\n",
      "Iteration 20, loss = 0.24234117\n",
      "Iteration 21, loss = 0.23328071\n",
      "Iteration 22, loss = 0.22513488\n",
      "Iteration 23, loss = 0.21766456\n",
      "Iteration 24, loss = 0.21107504\n",
      "Iteration 25, loss = 0.20525084\n",
      "Iteration 26, loss = 0.19956256\n",
      "Iteration 27, loss = 0.19439864\n",
      "Iteration 28, loss = 0.18957389\n",
      "Iteration 29, loss = 0.18533036\n",
      "Iteration 30, loss = 0.18142536\n",
      "Iteration 31, loss = 0.17729529\n",
      "Iteration 32, loss = 0.17402571\n",
      "Iteration 33, loss = 0.17043614\n",
      "Iteration 34, loss = 0.16702715\n",
      "Iteration 35, loss = 0.16446441\n",
      "Iteration 36, loss = 0.16145938\n",
      "Iteration 37, loss = 0.15873176\n",
      "Iteration 38, loss = 0.15620164\n",
      "Iteration 39, loss = 0.15384397\n",
      "Iteration 40, loss = 0.15169444\n",
      "Iteration 41, loss = 0.14972777\n",
      "Iteration 42, loss = 0.14714584\n",
      "Iteration 43, loss = 0.14506656\n",
      "Iteration 44, loss = 0.14427569\n",
      "Iteration 45, loss = 0.14159786\n",
      "Iteration 46, loss = 0.13998942\n",
      "Iteration 47, loss = 0.13788490\n",
      "Iteration 48, loss = 0.13616165\n",
      "Iteration 49, loss = 0.13452314\n",
      "Iteration 50, loss = 0.13317204\n",
      "Iteration 51, loss = 0.13129569\n",
      "Iteration 52, loss = 0.13012363\n",
      "Iteration 53, loss = 0.12847138\n",
      "Iteration 54, loss = 0.12734357\n",
      "Iteration 55, loss = 0.12584966\n",
      "Iteration 56, loss = 0.12454028\n",
      "Iteration 57, loss = 0.12334738\n",
      "Iteration 58, loss = 0.12206151\n",
      "Iteration 59, loss = 0.12088511\n",
      "Iteration 60, loss = 0.11971578\n",
      "Iteration 61, loss = 0.11851144\n",
      "Iteration 62, loss = 0.11759575\n",
      "Iteration 63, loss = 0.11639372\n",
      "Iteration 64, loss = 0.11550712\n",
      "Iteration 65, loss = 0.11441812\n",
      "Iteration 66, loss = 0.11348929\n",
      "Iteration 67, loss = 0.11251232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 0.11138100\n",
      "Iteration 69, loss = 0.11074238\n",
      "Iteration 70, loss = 0.10960006\n",
      "Iteration 71, loss = 0.10867195\n",
      "Iteration 72, loss = 0.10782115\n",
      "Iteration 73, loss = 0.10706090\n",
      "Iteration 74, loss = 0.10636176\n",
      "Iteration 75, loss = 0.10565008\n",
      "Iteration 76, loss = 0.10483490\n",
      "Iteration 77, loss = 0.10437864\n",
      "Iteration 78, loss = 0.10319482\n",
      "Iteration 79, loss = 0.10355381\n",
      "Iteration 80, loss = 0.10127889\n",
      "Iteration 81, loss = 0.10118472\n",
      "Iteration 82, loss = 0.10016231\n",
      "Iteration 83, loss = 0.09964937\n",
      "Iteration 84, loss = 0.09880711\n",
      "Iteration 85, loss = 0.09922968\n",
      "Iteration 86, loss = 0.09756867\n",
      "Iteration 87, loss = 0.09676469\n",
      "Iteration 88, loss = 0.09611081\n",
      "Iteration 89, loss = 0.09568208\n",
      "Iteration 90, loss = 0.09504892\n",
      "Iteration 91, loss = 0.09528374\n",
      "Iteration 92, loss = 0.09384591\n",
      "Iteration 93, loss = 0.09342203\n",
      "Iteration 94, loss = 0.09268262\n",
      "Iteration 95, loss = 0.09242973\n",
      "Iteration 96, loss = 0.09156614\n",
      "Iteration 97, loss = 0.09103066\n",
      "Iteration 98, loss = 0.09081474\n",
      "Iteration 99, loss = 0.08984516\n",
      "Iteration 100, loss = 0.08921591\n",
      "Iteration 101, loss = 0.08936902\n",
      "Iteration 102, loss = 0.08827480\n",
      "Iteration 103, loss = 0.08773378\n",
      "Iteration 104, loss = 0.08724259\n",
      "Iteration 105, loss = 0.08679852\n",
      "Iteration 106, loss = 0.08625113\n",
      "Iteration 107, loss = 0.08601280\n",
      "Iteration 108, loss = 0.08526136\n",
      "Iteration 109, loss = 0.08491860\n",
      "Iteration 110, loss = 0.08430317\n",
      "Iteration 111, loss = 0.08422693\n",
      "Iteration 112, loss = 0.08353022\n",
      "Iteration 113, loss = 0.08304444\n",
      "Iteration 114, loss = 0.08300226\n",
      "Iteration 115, loss = 0.08230908\n",
      "Iteration 116, loss = 0.08236300\n",
      "Iteration 117, loss = 0.08121971\n",
      "Iteration 118, loss = 0.08090803\n",
      "Iteration 119, loss = 0.08055264\n",
      "Iteration 120, loss = 0.08041807\n",
      "Iteration 121, loss = 0.07970315\n",
      "Iteration 122, loss = 0.07951695\n",
      "Iteration 123, loss = 0.07903570\n",
      "Iteration 124, loss = 0.07860042\n",
      "Iteration 125, loss = 0.07855425\n",
      "Iteration 126, loss = 0.07800795\n",
      "Iteration 127, loss = 0.07758745\n",
      "Iteration 128, loss = 0.07721808\n",
      "Iteration 129, loss = 0.07709904\n",
      "Iteration 130, loss = 0.07703103\n",
      "Iteration 131, loss = 0.07640745\n",
      "Iteration 132, loss = 0.07579993\n",
      "Iteration 133, loss = 0.07553002\n",
      "Iteration 134, loss = 0.07531688\n",
      "Iteration 135, loss = 0.07485834\n",
      "Iteration 136, loss = 0.07451472\n",
      "Iteration 137, loss = 0.07437489\n",
      "Iteration 138, loss = 0.07384105\n",
      "Iteration 139, loss = 0.07361179\n",
      "Iteration 140, loss = 0.07315628\n",
      "Iteration 141, loss = 0.07286566\n",
      "Iteration 142, loss = 0.07265316\n",
      "Iteration 143, loss = 0.07226651\n",
      "Iteration 144, loss = 0.07212878\n",
      "Iteration 145, loss = 0.07197739\n",
      "Iteration 146, loss = 0.07154741\n",
      "Iteration 147, loss = 0.07132423\n",
      "Iteration 148, loss = 0.07099843\n",
      "Iteration 149, loss = 0.07071502\n",
      "Iteration 150, loss = 0.07059938\n",
      "Iteration 151, loss = 0.07021497\n",
      "Iteration 152, loss = 0.07052473\n",
      "Iteration 153, loss = 0.06952677\n",
      "Iteration 154, loss = 0.06923382\n",
      "Iteration 155, loss = 0.06971232\n",
      "Iteration 156, loss = 0.06908171\n",
      "Iteration 157, loss = 0.06908845\n",
      "Iteration 158, loss = 0.06835075\n",
      "Iteration 159, loss = 0.06821356\n",
      "Iteration 160, loss = 0.06739186\n",
      "Iteration 161, loss = 0.06774492\n",
      "Iteration 162, loss = 0.06740243\n",
      "Iteration 163, loss = 0.06690131\n",
      "Iteration 164, loss = 0.06674168\n",
      "Iteration 165, loss = 0.06649463\n",
      "Iteration 166, loss = 0.06666506\n",
      "Iteration 167, loss = 0.06595484\n",
      "Iteration 168, loss = 0.06578053\n",
      "Iteration 169, loss = 0.06574026\n",
      "Iteration 170, loss = 0.06573356\n",
      "Iteration 171, loss = 0.06499577\n",
      "Iteration 172, loss = 0.06506794\n",
      "Iteration 173, loss = 0.06478485\n",
      "Iteration 174, loss = 0.06453217\n",
      "Iteration 175, loss = 0.06427936\n",
      "Iteration 176, loss = 0.06420624\n",
      "Iteration 177, loss = 0.06403710\n",
      "Iteration 178, loss = 0.06363497\n",
      "Iteration 179, loss = 0.06355595\n",
      "Iteration 180, loss = 0.06320289\n",
      "Iteration 181, loss = 0.06312728\n",
      "Iteration 182, loss = 0.06276497\n",
      "Iteration 183, loss = 0.06270049\n",
      "Iteration 184, loss = 0.06247691\n",
      "Iteration 185, loss = 0.06203512\n",
      "Iteration 186, loss = 0.06179781\n",
      "Iteration 187, loss = 0.06185734\n",
      "Iteration 188, loss = 0.06199652\n",
      "Iteration 189, loss = 0.06116173\n",
      "Iteration 190, loss = 0.06158423\n",
      "Iteration 191, loss = 0.06119343\n",
      "Iteration 192, loss = 0.06084543\n",
      "Iteration 193, loss = 0.06066797\n",
      "Iteration 194, loss = 0.06031994\n",
      "Iteration 195, loss = 0.06031990\n",
      "Iteration 196, loss = 0.06034176\n",
      "Iteration 197, loss = 0.05959488\n",
      "Iteration 198, loss = 0.05951657\n",
      "Iteration 199, loss = 0.05951619\n",
      "Iteration 200, loss = 0.05952908\n",
      "Iteration 201, loss = 0.05912836\n",
      "Iteration 202, loss = 0.05927502\n",
      "Iteration 203, loss = 0.05885316\n",
      "Iteration 204, loss = 0.05836227\n",
      "Iteration 205, loss = 0.05840843\n",
      "Iteration 206, loss = 0.05801766\n",
      "Iteration 207, loss = 0.05789117\n",
      "Iteration 208, loss = 0.05763058\n",
      "Iteration 209, loss = 0.05781880\n",
      "Iteration 210, loss = 0.05738080\n",
      "Iteration 211, loss = 0.05748903\n",
      "Iteration 212, loss = 0.05704347\n",
      "Iteration 213, loss = 0.05749793\n",
      "Iteration 214, loss = 0.05714906\n",
      "Iteration 215, loss = 0.05645016\n",
      "Iteration 216, loss = 0.05646535\n",
      "Iteration 217, loss = 0.05640845\n",
      "Iteration 218, loss = 0.05654034\n",
      "Iteration 219, loss = 0.05613984\n",
      "Iteration 220, loss = 0.05625799\n",
      "Iteration 221, loss = 0.05579622\n",
      "Iteration 222, loss = 0.05561378\n",
      "Iteration 223, loss = 0.05554559\n",
      "Iteration 224, loss = 0.05520030\n",
      "Iteration 225, loss = 0.05528172\n",
      "Iteration 226, loss = 0.05505151\n",
      "Iteration 227, loss = 0.05456121\n",
      "Iteration 228, loss = 0.05457745\n",
      "Iteration 229, loss = 0.05481945\n",
      "Iteration 230, loss = 0.05414448\n",
      "Iteration 231, loss = 0.05408157\n",
      "Iteration 232, loss = 0.05449599\n",
      "Iteration 233, loss = 0.05414446\n",
      "Iteration 234, loss = 0.05365861\n",
      "Iteration 235, loss = 0.05365413\n",
      "Iteration 236, loss = 0.05361105\n",
      "Iteration 237, loss = 0.05351481\n",
      "Iteration 238, loss = 0.05330143\n",
      "Iteration 239, loss = 0.05280398\n",
      "Iteration 240, loss = 0.05356734\n",
      "Iteration 241, loss = 0.05318190\n",
      "Iteration 242, loss = 0.05251394\n",
      "Iteration 243, loss = 0.05279448\n",
      "Iteration 244, loss = 0.05217007\n",
      "Iteration 245, loss = 0.05255872\n",
      "Iteration 246, loss = 0.05183945\n",
      "Iteration 247, loss = 0.05178597\n",
      "Iteration 248, loss = 0.05226235\n",
      "Iteration 249, loss = 0.05183853\n",
      "Iteration 250, loss = 0.05144905\n",
      "Iteration 251, loss = 0.05152565\n",
      "Iteration 252, loss = 0.05115906\n",
      "Iteration 253, loss = 0.05119413\n",
      "Iteration 254, loss = 0.05128430\n",
      "Iteration 255, loss = 0.05139212\n",
      "Iteration 256, loss = 0.05126308\n",
      "Iteration 257, loss = 0.05093889\n",
      "Iteration 258, loss = 0.05085159\n",
      "Iteration 259, loss = 0.05077783\n",
      "Iteration 260, loss = 0.05043533\n",
      "Iteration 261, loss = 0.05055133\n",
      "Iteration 262, loss = 0.05017367\n",
      "Iteration 263, loss = 0.04970292\n",
      "Iteration 264, loss = 0.04971127\n",
      "Iteration 265, loss = 0.04968217\n",
      "Iteration 266, loss = 0.04959752\n",
      "Iteration 267, loss = 0.04982286\n",
      "Iteration 268, loss = 0.04956421\n",
      "Iteration 269, loss = 0.04955074\n",
      "Iteration 270, loss = 0.04955783\n",
      "Iteration 271, loss = 0.04896024\n",
      "Iteration 272, loss = 0.04869362\n",
      "Iteration 273, loss = 0.04904889\n",
      "Iteration 274, loss = 0.04883805\n",
      "Iteration 275, loss = 0.04891225\n",
      "Iteration 276, loss = 0.04840375\n",
      "Iteration 277, loss = 0.04834841\n",
      "Iteration 278, loss = 0.04849116\n",
      "Iteration 279, loss = 0.04877101\n",
      "Iteration 280, loss = 0.04909078\n",
      "Iteration 281, loss = 0.04954552\n",
      "Iteration 282, loss = 0.04791224\n",
      "Iteration 283, loss = 0.04784997\n",
      "Iteration 284, loss = 0.04791307\n",
      "Iteration 285, loss = 0.04794447\n",
      "Iteration 286, loss = 0.04740139\n",
      "Iteration 287, loss = 0.04769241\n",
      "Iteration 288, loss = 0.04740345\n",
      "Iteration 289, loss = 0.04744443\n",
      "Iteration 290, loss = 0.04774944\n",
      "Iteration 291, loss = 0.04693593\n",
      "Iteration 292, loss = 0.04676381\n",
      "Iteration 293, loss = 0.04702050\n",
      "Iteration 294, loss = 0.04684387\n",
      "Iteration 295, loss = 0.04642840\n",
      "Iteration 296, loss = 0.04739969\n",
      "Iteration 297, loss = 0.04623659\n",
      "Iteration 298, loss = 0.04663283\n",
      "Iteration 299, loss = 0.04626533\n",
      "Iteration 300, loss = 0.04599076\n",
      "Iteration 301, loss = 0.04706015\n",
      "Iteration 302, loss = 0.04677946\n",
      "Iteration 303, loss = 0.04581111\n",
      "Iteration 304, loss = 0.04603651\n",
      "Iteration 305, loss = 0.04583647\n",
      "Iteration 306, loss = 0.04552397\n",
      "Iteration 307, loss = 0.04549133\n",
      "Iteration 308, loss = 0.04567579\n",
      "Iteration 309, loss = 0.04537513\n",
      "Iteration 310, loss = 0.04578165\n",
      "Iteration 311, loss = 0.04507880\n",
      "Iteration 312, loss = 0.04488213\n",
      "Iteration 313, loss = 0.04499810\n",
      "Iteration 314, loss = 0.04544381\n",
      "Iteration 315, loss = 0.04511199\n",
      "Iteration 316, loss = 0.04449518\n",
      "Iteration 317, loss = 0.04486290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 318, loss = 0.04477571\n",
      "Iteration 319, loss = 0.04464033\n",
      "Iteration 320, loss = 0.04425474\n",
      "Iteration 321, loss = 0.04463268\n",
      "Iteration 322, loss = 0.04427718\n",
      "Iteration 323, loss = 0.04422384\n",
      "Iteration 324, loss = 0.04425253\n",
      "Iteration 325, loss = 0.04433072\n",
      "Iteration 326, loss = 0.04390793\n",
      "Iteration 327, loss = 0.04406592\n",
      "Iteration 328, loss = 0.04403342\n",
      "Iteration 329, loss = 0.04383537\n",
      "Iteration 330, loss = 0.04375072\n",
      "Iteration 331, loss = 0.04360312\n",
      "Iteration 332, loss = 0.04376878\n",
      "Iteration 333, loss = 0.04363985\n",
      "Iteration 334, loss = 0.04351032\n",
      "Iteration 335, loss = 0.04352236\n",
      "Iteration 336, loss = 0.04402507\n",
      "Iteration 337, loss = 0.04312747\n",
      "Iteration 338, loss = 0.04320071\n",
      "Iteration 339, loss = 0.04290901\n",
      "Iteration 340, loss = 0.04311314\n",
      "Iteration 341, loss = 0.04286969\n",
      "Iteration 342, loss = 0.04294012\n",
      "Iteration 343, loss = 0.04297940\n",
      "Iteration 344, loss = 0.04355834\n",
      "Iteration 345, loss = 0.04253561\n",
      "Iteration 346, loss = 0.04303133\n",
      "Iteration 347, loss = 0.04266280\n",
      "Iteration 348, loss = 0.04234930\n",
      "Iteration 349, loss = 0.04241315\n",
      "Iteration 350, loss = 0.04248851\n",
      "Iteration 351, loss = 0.04213384\n",
      "Iteration 352, loss = 0.04244683\n",
      "Iteration 353, loss = 0.04213309\n",
      "Iteration 354, loss = 0.04238357\n",
      "Iteration 355, loss = 0.04218073\n",
      "Iteration 356, loss = 0.04238458\n",
      "Iteration 357, loss = 0.04219566\n",
      "Iteration 358, loss = 0.04190638\n",
      "Iteration 359, loss = 0.04175271\n",
      "Iteration 360, loss = 0.04255900\n",
      "Iteration 361, loss = 0.04196745\n",
      "Iteration 362, loss = 0.04189870\n",
      "Iteration 363, loss = 0.04144443\n",
      "Iteration 364, loss = 0.04156417\n",
      "Iteration 365, loss = 0.04157430\n",
      "Iteration 366, loss = 0.04137696\n",
      "Iteration 367, loss = 0.04127354\n",
      "Iteration 368, loss = 0.04114627\n",
      "Iteration 369, loss = 0.04180613\n",
      "Iteration 370, loss = 0.04118156\n",
      "Iteration 371, loss = 0.04107476\n",
      "Iteration 372, loss = 0.04085200\n",
      "Iteration 373, loss = 0.04080833\n",
      "Iteration 374, loss = 0.04080966\n",
      "Iteration 375, loss = 0.04054039\n",
      "Iteration 376, loss = 0.04113624\n",
      "Iteration 377, loss = 0.04090475\n",
      "Iteration 378, loss = 0.04075662\n",
      "Iteration 379, loss = 0.04056375\n",
      "Iteration 380, loss = 0.04031717\n",
      "Iteration 381, loss = 0.04026436\n",
      "Iteration 382, loss = 0.04120706\n",
      "Iteration 383, loss = 0.04096015\n",
      "Iteration 384, loss = 0.04039193\n",
      "Iteration 385, loss = 0.04030850\n",
      "Iteration 386, loss = 0.04049931\n",
      "Iteration 387, loss = 0.03976908\n",
      "Iteration 388, loss = 0.04182448\n",
      "Iteration 389, loss = 0.03996461\n",
      "Iteration 390, loss = 0.04015644\n",
      "Iteration 391, loss = 0.03997892\n",
      "Iteration 392, loss = 0.03987176\n",
      "Iteration 393, loss = 0.04000146\n",
      "Iteration 394, loss = 0.03950035\n",
      "Iteration 395, loss = 0.03989213\n",
      "Iteration 396, loss = 0.03953476\n",
      "Iteration 397, loss = 0.03992643\n",
      "Iteration 398, loss = 0.03959120\n",
      "Iteration 399, loss = 0.03959072\n",
      "Iteration 400, loss = 0.03938000\n",
      "Iteration 401, loss = 0.03927573\n",
      "Iteration 402, loss = 0.03941160\n",
      "Iteration 403, loss = 0.03919310\n",
      "Iteration 404, loss = 0.03925911\n",
      "Iteration 405, loss = 0.03956987\n",
      "Iteration 406, loss = 0.03937950\n",
      "Iteration 407, loss = 0.03919052\n",
      "Iteration 408, loss = 0.03943236\n",
      "Iteration 409, loss = 0.03895399\n",
      "Iteration 410, loss = 0.03899664\n",
      "Iteration 411, loss = 0.03871537\n",
      "Iteration 412, loss = 0.03891814\n",
      "Iteration 413, loss = 0.03856528\n",
      "Iteration 414, loss = 0.03908581\n",
      "Iteration 415, loss = 0.03870787\n",
      "Iteration 416, loss = 0.03893922\n",
      "Iteration 417, loss = 0.03863826\n",
      "Iteration 418, loss = 0.03843441\n",
      "Iteration 419, loss = 0.03844735\n",
      "Iteration 420, loss = 0.03835477\n",
      "Iteration 421, loss = 0.03828195\n",
      "Iteration 422, loss = 0.03841867\n",
      "Iteration 423, loss = 0.03805804\n",
      "Iteration 424, loss = 0.03803291\n",
      "Iteration 425, loss = 0.03823796\n",
      "Iteration 426, loss = 0.03811910\n",
      "Iteration 427, loss = 0.03795844\n",
      "Iteration 428, loss = 0.03858014\n",
      "Iteration 429, loss = 0.03822874\n",
      "Iteration 430, loss = 0.03791008\n",
      "Iteration 431, loss = 0.03779071\n",
      "Iteration 432, loss = 0.03807286\n",
      "Iteration 433, loss = 0.03778831\n",
      "Iteration 434, loss = 0.03781444\n",
      "Iteration 435, loss = 0.03823352\n",
      "Iteration 436, loss = 0.03803723\n",
      "Iteration 437, loss = 0.03750601\n",
      "Iteration 438, loss = 0.03760732\n",
      "Iteration 439, loss = 0.03716897\n",
      "Iteration 440, loss = 0.03758495\n",
      "Iteration 441, loss = 0.03728457\n",
      "Iteration 442, loss = 0.03759674\n",
      "Iteration 443, loss = 0.03789336\n",
      "Iteration 444, loss = 0.03720307\n",
      "Iteration 445, loss = 0.03740968\n",
      "Iteration 446, loss = 0.03747518\n",
      "Iteration 447, loss = 0.03728516\n",
      "Iteration 448, loss = 0.03775446\n",
      "Iteration 449, loss = 0.03721157\n",
      "Iteration 450, loss = 0.03798480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 92.19474497681608\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Iteration 1, loss = 0.68950625\n",
      "Iteration 2, loss = 0.67983224\n",
      "Iteration 3, loss = 0.66513909\n",
      "Iteration 4, loss = 0.64478200\n",
      "Iteration 5, loss = 0.61826720\n",
      "Iteration 6, loss = 0.58648355\n",
      "Iteration 7, loss = 0.55147899\n",
      "Iteration 8, loss = 0.51499233\n",
      "Iteration 9, loss = 0.47885054\n",
      "Iteration 10, loss = 0.44448672\n",
      "Iteration 11, loss = 0.41278299\n",
      "Iteration 12, loss = 0.38390996\n",
      "Iteration 13, loss = 0.35808881\n",
      "Iteration 14, loss = 0.33567733\n",
      "Iteration 15, loss = 0.31558966\n",
      "Iteration 16, loss = 0.29819188\n",
      "Iteration 17, loss = 0.28371119\n",
      "Iteration 18, loss = 0.26944772\n",
      "Iteration 19, loss = 0.25767457\n",
      "Iteration 20, loss = 0.24713474\n",
      "Iteration 21, loss = 0.23767301\n",
      "Iteration 22, loss = 0.22928017\n",
      "Iteration 23, loss = 0.22163181\n",
      "Iteration 24, loss = 0.21499040\n",
      "Iteration 25, loss = 0.20844905\n",
      "Iteration 26, loss = 0.20276467\n",
      "Iteration 27, loss = 0.19807353\n",
      "Iteration 28, loss = 0.19283289\n",
      "Iteration 29, loss = 0.18827150\n",
      "Iteration 30, loss = 0.18360806\n",
      "Iteration 31, loss = 0.17972392\n",
      "Iteration 32, loss = 0.17616964\n",
      "Iteration 33, loss = 0.17285636\n",
      "Iteration 34, loss = 0.16962282\n",
      "Iteration 35, loss = 0.16651286\n",
      "Iteration 36, loss = 0.16337033\n",
      "Iteration 37, loss = 0.16049142\n",
      "Iteration 38, loss = 0.15778031\n",
      "Iteration 39, loss = 0.15536700\n",
      "Iteration 40, loss = 0.15288161\n",
      "Iteration 41, loss = 0.15092881\n",
      "Iteration 42, loss = 0.14859604\n",
      "Iteration 43, loss = 0.14644776\n",
      "Iteration 44, loss = 0.14435564\n",
      "Iteration 45, loss = 0.14242752\n",
      "Iteration 46, loss = 0.14064742\n",
      "Iteration 47, loss = 0.13924611\n",
      "Iteration 48, loss = 0.13745605\n",
      "Iteration 49, loss = 0.13563017\n",
      "Iteration 50, loss = 0.13392821\n",
      "Iteration 51, loss = 0.13218266\n",
      "Iteration 52, loss = 0.13112058\n",
      "Iteration 53, loss = 0.12936112\n",
      "Iteration 54, loss = 0.12785579\n",
      "Iteration 55, loss = 0.12664109\n",
      "Iteration 56, loss = 0.12547011\n",
      "Iteration 57, loss = 0.12417100\n",
      "Iteration 58, loss = 0.12314641\n",
      "Iteration 59, loss = 0.12132489\n",
      "Iteration 60, loss = 0.12036855\n",
      "Iteration 61, loss = 0.11922822\n",
      "Iteration 62, loss = 0.11791531\n",
      "Iteration 63, loss = 0.11687763\n",
      "Iteration 64, loss = 0.11558878\n",
      "Iteration 65, loss = 0.11462204\n",
      "Iteration 66, loss = 0.11363698\n",
      "Iteration 67, loss = 0.11268417\n",
      "Iteration 68, loss = 0.11163380\n",
      "Iteration 69, loss = 0.11092779\n",
      "Iteration 70, loss = 0.10975833\n",
      "Iteration 71, loss = 0.10880526\n",
      "Iteration 72, loss = 0.10798327\n",
      "Iteration 73, loss = 0.10741530\n",
      "Iteration 74, loss = 0.10643056\n",
      "Iteration 75, loss = 0.10542586\n",
      "Iteration 76, loss = 0.10458069\n",
      "Iteration 77, loss = 0.10376372\n",
      "Iteration 78, loss = 0.10301854\n",
      "Iteration 79, loss = 0.10236201\n",
      "Iteration 80, loss = 0.10154136\n",
      "Iteration 81, loss = 0.10100830\n",
      "Iteration 82, loss = 0.09995097\n",
      "Iteration 83, loss = 0.09908311\n",
      "Iteration 84, loss = 0.09842594\n",
      "Iteration 85, loss = 0.09772175\n",
      "Iteration 86, loss = 0.09732906\n",
      "Iteration 87, loss = 0.09658235\n",
      "Iteration 88, loss = 0.09585961\n",
      "Iteration 89, loss = 0.09521944\n",
      "Iteration 90, loss = 0.09451699\n",
      "Iteration 91, loss = 0.09391222\n",
      "Iteration 92, loss = 0.09337920\n",
      "Iteration 93, loss = 0.09284007\n",
      "Iteration 94, loss = 0.09276711\n",
      "Iteration 95, loss = 0.09164424\n",
      "Iteration 96, loss = 0.09107093\n",
      "Iteration 97, loss = 0.09049693\n",
      "Iteration 98, loss = 0.08986285\n",
      "Iteration 99, loss = 0.08962781\n",
      "Iteration 100, loss = 0.08949627\n",
      "Iteration 101, loss = 0.08856636\n",
      "Iteration 102, loss = 0.08805461\n",
      "Iteration 103, loss = 0.08752578\n",
      "Iteration 104, loss = 0.08665879\n",
      "Iteration 105, loss = 0.08626289\n",
      "Iteration 106, loss = 0.08576968\n",
      "Iteration 107, loss = 0.08539089\n",
      "Iteration 108, loss = 0.08494071\n",
      "Iteration 109, loss = 0.08453717\n",
      "Iteration 110, loss = 0.08382007\n",
      "Iteration 111, loss = 0.08365523\n",
      "Iteration 112, loss = 0.08386538\n",
      "Iteration 113, loss = 0.08308947\n",
      "Iteration 114, loss = 0.08224426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 115, loss = 0.08192545\n",
      "Iteration 116, loss = 0.08146437\n",
      "Iteration 117, loss = 0.08102135\n",
      "Iteration 118, loss = 0.08034448\n",
      "Iteration 119, loss = 0.08027563\n",
      "Iteration 120, loss = 0.07980090\n",
      "Iteration 121, loss = 0.07910424\n",
      "Iteration 122, loss = 0.07879459\n",
      "Iteration 123, loss = 0.07836230\n",
      "Iteration 124, loss = 0.07812101\n",
      "Iteration 125, loss = 0.07773576\n",
      "Iteration 126, loss = 0.07719514\n",
      "Iteration 127, loss = 0.07695913\n",
      "Iteration 128, loss = 0.07668968\n",
      "Iteration 129, loss = 0.07607906\n",
      "Iteration 130, loss = 0.07590730\n",
      "Iteration 131, loss = 0.07539786\n",
      "Iteration 132, loss = 0.07559503\n",
      "Iteration 133, loss = 0.07520768\n",
      "Iteration 134, loss = 0.07470246\n",
      "Iteration 135, loss = 0.07410383\n",
      "Iteration 136, loss = 0.07372119\n",
      "Iteration 137, loss = 0.07375703\n",
      "Iteration 138, loss = 0.07322946\n",
      "Iteration 139, loss = 0.07293921\n",
      "Iteration 140, loss = 0.07262541\n",
      "Iteration 141, loss = 0.07239516\n",
      "Iteration 142, loss = 0.07190200\n",
      "Iteration 143, loss = 0.07211186\n",
      "Iteration 144, loss = 0.07198709\n",
      "Iteration 145, loss = 0.07109117\n",
      "Iteration 146, loss = 0.07074105\n",
      "Iteration 147, loss = 0.07074091\n",
      "Iteration 148, loss = 0.07029441\n",
      "Iteration 149, loss = 0.07001502\n",
      "Iteration 150, loss = 0.06968125\n",
      "Iteration 151, loss = 0.06942845\n",
      "Iteration 152, loss = 0.06897382\n",
      "Iteration 153, loss = 0.06912580\n",
      "Iteration 154, loss = 0.06930178\n",
      "Iteration 155, loss = 0.06845580\n",
      "Iteration 156, loss = 0.06808138\n",
      "Iteration 157, loss = 0.06787521\n",
      "Iteration 158, loss = 0.06786731\n",
      "Iteration 159, loss = 0.06737247\n",
      "Iteration 160, loss = 0.06750893\n",
      "Iteration 161, loss = 0.06691875\n",
      "Iteration 162, loss = 0.06673686\n",
      "Iteration 163, loss = 0.06625106\n",
      "Iteration 164, loss = 0.06594258\n",
      "Iteration 165, loss = 0.06616922\n",
      "Iteration 166, loss = 0.06553601\n",
      "Iteration 167, loss = 0.06591857\n",
      "Iteration 168, loss = 0.06513137\n",
      "Iteration 169, loss = 0.06543251\n",
      "Iteration 170, loss = 0.06546726\n",
      "Iteration 171, loss = 0.06467944\n",
      "Iteration 172, loss = 0.06409551\n",
      "Iteration 173, loss = 0.06397736\n",
      "Iteration 174, loss = 0.06407352\n",
      "Iteration 175, loss = 0.06388968\n",
      "Iteration 176, loss = 0.06371589\n",
      "Iteration 177, loss = 0.06306222\n",
      "Iteration 178, loss = 0.06287652\n",
      "Iteration 179, loss = 0.06286638\n",
      "Iteration 180, loss = 0.06249509\n",
      "Iteration 181, loss = 0.06229558\n",
      "Iteration 182, loss = 0.06238845\n",
      "Iteration 183, loss = 0.06184770\n",
      "Iteration 184, loss = 0.06155209\n",
      "Iteration 185, loss = 0.06181795\n",
      "Iteration 186, loss = 0.06132124\n",
      "Iteration 187, loss = 0.06112064\n",
      "Iteration 188, loss = 0.06076959\n",
      "Iteration 189, loss = 0.06071757\n",
      "Iteration 190, loss = 0.06051268\n",
      "Iteration 191, loss = 0.06028938\n",
      "Iteration 192, loss = 0.06014769\n",
      "Iteration 193, loss = 0.06015131\n",
      "Iteration 194, loss = 0.06000364\n",
      "Iteration 195, loss = 0.05947385\n",
      "Iteration 196, loss = 0.05951291\n",
      "Iteration 197, loss = 0.05977107\n",
      "Iteration 198, loss = 0.05900450\n",
      "Iteration 199, loss = 0.05899343\n",
      "Iteration 200, loss = 0.05900823\n",
      "Iteration 201, loss = 0.05853732\n",
      "Iteration 202, loss = 0.05848720\n",
      "Iteration 203, loss = 0.05797315\n",
      "Iteration 204, loss = 0.05801865\n",
      "Iteration 205, loss = 0.05836093\n",
      "Iteration 206, loss = 0.05742766\n",
      "Iteration 207, loss = 0.05738468\n",
      "Iteration 208, loss = 0.05736030\n",
      "Iteration 209, loss = 0.05727136\n",
      "Iteration 210, loss = 0.05718251\n",
      "Iteration 211, loss = 0.05698485\n",
      "Iteration 212, loss = 0.05686424\n",
      "Iteration 213, loss = 0.05636676\n",
      "Iteration 214, loss = 0.05628714\n",
      "Iteration 215, loss = 0.05629873\n",
      "Iteration 216, loss = 0.05644869\n",
      "Iteration 217, loss = 0.05552626\n",
      "Iteration 218, loss = 0.05593123\n",
      "Iteration 219, loss = 0.05566508\n",
      "Iteration 220, loss = 0.05488909\n",
      "Iteration 221, loss = 0.05570196\n",
      "Iteration 222, loss = 0.05526015\n",
      "Iteration 223, loss = 0.05529747\n",
      "Iteration 224, loss = 0.05459089\n",
      "Iteration 225, loss = 0.05472555\n",
      "Iteration 226, loss = 0.05433697\n",
      "Iteration 227, loss = 0.05402488\n",
      "Iteration 228, loss = 0.05421114\n",
      "Iteration 229, loss = 0.05387004\n",
      "Iteration 230, loss = 0.05416364\n",
      "Iteration 231, loss = 0.05406151\n",
      "Iteration 232, loss = 0.05348299\n",
      "Iteration 233, loss = 0.05318178\n",
      "Iteration 234, loss = 0.05332590\n",
      "Iteration 235, loss = 0.05312601\n",
      "Iteration 236, loss = 0.05254750\n",
      "Iteration 237, loss = 0.05272496\n",
      "Iteration 238, loss = 0.05255261\n",
      "Iteration 239, loss = 0.05248944\n",
      "Iteration 240, loss = 0.05229479\n",
      "Iteration 241, loss = 0.05226350\n",
      "Iteration 242, loss = 0.05240681\n",
      "Iteration 243, loss = 0.05240313\n",
      "Iteration 244, loss = 0.05149389\n",
      "Iteration 245, loss = 0.05185948\n",
      "Iteration 246, loss = 0.05147837\n",
      "Iteration 247, loss = 0.05128556\n",
      "Iteration 248, loss = 0.05098652\n",
      "Iteration 249, loss = 0.05108121\n",
      "Iteration 250, loss = 0.05134630\n",
      "Iteration 251, loss = 0.05096045\n",
      "Iteration 252, loss = 0.05075741\n",
      "Iteration 253, loss = 0.05040437\n",
      "Iteration 254, loss = 0.05030753\n",
      "Iteration 255, loss = 0.05043447\n",
      "Iteration 256, loss = 0.05068227\n",
      "Iteration 257, loss = 0.05013754\n",
      "Iteration 258, loss = 0.05084825\n",
      "Iteration 259, loss = 0.04973067\n",
      "Iteration 260, loss = 0.04966253\n",
      "Iteration 261, loss = 0.04977619\n",
      "Iteration 262, loss = 0.04957012\n",
      "Iteration 263, loss = 0.04926056\n",
      "Iteration 264, loss = 0.04957266\n",
      "Iteration 265, loss = 0.04925358\n",
      "Iteration 266, loss = 0.04912057\n",
      "Iteration 267, loss = 0.04916765\n",
      "Iteration 268, loss = 0.04869085\n",
      "Iteration 269, loss = 0.04848776\n",
      "Iteration 270, loss = 0.04838898\n",
      "Iteration 271, loss = 0.04833375\n",
      "Iteration 272, loss = 0.04829776\n",
      "Iteration 273, loss = 0.04796935\n",
      "Iteration 274, loss = 0.04797008\n",
      "Iteration 275, loss = 0.04820760\n",
      "Iteration 276, loss = 0.04773228\n",
      "Iteration 277, loss = 0.04775104\n",
      "Iteration 278, loss = 0.04767880\n",
      "Iteration 279, loss = 0.04767008\n",
      "Iteration 280, loss = 0.04771704\n",
      "Iteration 281, loss = 0.04743267\n",
      "Iteration 282, loss = 0.04712373\n",
      "Iteration 283, loss = 0.04728811\n",
      "Iteration 284, loss = 0.04680537\n",
      "Iteration 285, loss = 0.04678379\n",
      "Iteration 286, loss = 0.04674465\n",
      "Iteration 287, loss = 0.04702134\n",
      "Iteration 288, loss = 0.04669692\n",
      "Iteration 289, loss = 0.04636149\n",
      "Iteration 290, loss = 0.04650609\n",
      "Iteration 291, loss = 0.04639689\n",
      "Iteration 292, loss = 0.04643310\n",
      "Iteration 293, loss = 0.04615102\n",
      "Iteration 294, loss = 0.04643166\n",
      "Iteration 295, loss = 0.04575130\n",
      "Iteration 296, loss = 0.04633558\n",
      "Iteration 297, loss = 0.04575625\n",
      "Iteration 298, loss = 0.04562301\n",
      "Iteration 299, loss = 0.04536929\n",
      "Iteration 300, loss = 0.04562812\n",
      "Iteration 301, loss = 0.04514911\n",
      "Iteration 302, loss = 0.04544903\n",
      "Iteration 303, loss = 0.04555585\n",
      "Iteration 304, loss = 0.04502614\n",
      "Iteration 305, loss = 0.04562515\n",
      "Iteration 306, loss = 0.04498441\n",
      "Iteration 307, loss = 0.04480705\n",
      "Iteration 308, loss = 0.04473175\n",
      "Iteration 309, loss = 0.04478168\n",
      "Iteration 310, loss = 0.04484759\n",
      "Iteration 311, loss = 0.04475157\n",
      "Iteration 312, loss = 0.04411073\n",
      "Iteration 313, loss = 0.04446021\n",
      "Iteration 314, loss = 0.04431008\n",
      "Iteration 315, loss = 0.04443583\n",
      "Iteration 316, loss = 0.04402531\n",
      "Iteration 317, loss = 0.04427792\n",
      "Iteration 318, loss = 0.04395757\n",
      "Iteration 319, loss = 0.04392458\n",
      "Iteration 320, loss = 0.04391514\n",
      "Iteration 321, loss = 0.04394412\n",
      "Iteration 322, loss = 0.04366173\n",
      "Iteration 323, loss = 0.04349998\n",
      "Iteration 324, loss = 0.04335132\n",
      "Iteration 325, loss = 0.04349601\n",
      "Iteration 326, loss = 0.04325970\n",
      "Iteration 327, loss = 0.04314461\n",
      "Iteration 328, loss = 0.04322935\n",
      "Iteration 329, loss = 0.04287279\n",
      "Iteration 330, loss = 0.04327181\n",
      "Iteration 331, loss = 0.04418153\n",
      "Iteration 332, loss = 0.04345925\n",
      "Iteration 333, loss = 0.04263554\n",
      "Iteration 334, loss = 0.04258193\n",
      "Iteration 335, loss = 0.04259603\n",
      "Iteration 336, loss = 0.04273392\n",
      "Iteration 337, loss = 0.04242234\n",
      "Iteration 338, loss = 0.04231907\n",
      "Iteration 339, loss = 0.04205758\n",
      "Iteration 340, loss = 0.04209662\n",
      "Iteration 341, loss = 0.04236144\n",
      "Iteration 342, loss = 0.04281004\n",
      "Iteration 343, loss = 0.04188419\n",
      "Iteration 344, loss = 0.04189791\n",
      "Iteration 345, loss = 0.04215635\n",
      "Iteration 346, loss = 0.04195689\n",
      "Iteration 347, loss = 0.04253386\n",
      "Iteration 348, loss = 0.04152854\n",
      "Iteration 349, loss = 0.04150142\n",
      "Iteration 350, loss = 0.04129780\n",
      "Iteration 351, loss = 0.04140471\n",
      "Iteration 352, loss = 0.04170818\n",
      "Iteration 353, loss = 0.04188054\n",
      "Iteration 354, loss = 0.04134001\n",
      "Iteration 355, loss = 0.04116496\n",
      "Iteration 356, loss = 0.04110225\n",
      "Iteration 357, loss = 0.04134154\n",
      "Iteration 358, loss = 0.04093510\n",
      "Iteration 359, loss = 0.04081317\n",
      "Iteration 360, loss = 0.04141910\n",
      "Iteration 361, loss = 0.04134474\n",
      "Iteration 362, loss = 0.04121715\n",
      "Iteration 363, loss = 0.04077289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 364, loss = 0.04053320\n",
      "Iteration 365, loss = 0.04057857\n",
      "Iteration 366, loss = 0.04082163\n",
      "Iteration 367, loss = 0.04017253\n",
      "Iteration 368, loss = 0.04048281\n",
      "Iteration 369, loss = 0.04062174\n",
      "Iteration 370, loss = 0.04104918\n",
      "Iteration 371, loss = 0.04051467\n",
      "Iteration 372, loss = 0.04008865\n",
      "Iteration 373, loss = 0.04007970\n",
      "Iteration 374, loss = 0.04003923\n",
      "Iteration 375, loss = 0.04033351\n",
      "Iteration 376, loss = 0.03978333\n",
      "Iteration 377, loss = 0.04033570\n",
      "Iteration 378, loss = 0.03975646\n",
      "Iteration 379, loss = 0.04049116\n",
      "Iteration 380, loss = 0.03999154\n",
      "Iteration 381, loss = 0.03962990\n",
      "Iteration 382, loss = 0.03955296\n",
      "Iteration 383, loss = 0.03943095\n",
      "Iteration 384, loss = 0.03973910\n",
      "Iteration 385, loss = 0.03956111\n",
      "Iteration 386, loss = 0.04056710\n",
      "Iteration 387, loss = 0.03990575\n",
      "Iteration 388, loss = 0.03941446\n",
      "Iteration 389, loss = 0.03906568\n",
      "Iteration 390, loss = 0.03913691\n",
      "Iteration 391, loss = 0.03880768\n",
      "Iteration 392, loss = 0.03886683\n",
      "Iteration 393, loss = 0.03903183\n",
      "Iteration 394, loss = 0.03878149\n",
      "Iteration 395, loss = 0.03915534\n",
      "Iteration 396, loss = 0.03865486\n",
      "Iteration 397, loss = 0.03888031\n",
      "Iteration 398, loss = 0.03845649\n",
      "Iteration 399, loss = 0.03916795\n",
      "Iteration 400, loss = 0.03873011\n",
      "Iteration 401, loss = 0.03850708\n",
      "Iteration 402, loss = 0.03836035\n",
      "Iteration 403, loss = 0.03921512\n",
      "Iteration 404, loss = 0.03826007\n",
      "Iteration 405, loss = 0.03857403\n",
      "Iteration 406, loss = 0.03920134\n",
      "Iteration 407, loss = 0.03820748\n",
      "Iteration 408, loss = 0.03819295\n",
      "Iteration 409, loss = 0.03839521\n",
      "Iteration 410, loss = 0.03803488\n",
      "Iteration 411, loss = 0.03780399\n",
      "Iteration 412, loss = 0.03820545\n",
      "Iteration 413, loss = 0.03780359\n",
      "Iteration 414, loss = 0.03790327\n",
      "Iteration 415, loss = 0.03777205\n",
      "Iteration 416, loss = 0.03777646\n",
      "Iteration 417, loss = 0.03832527\n",
      "Iteration 418, loss = 0.03772546\n",
      "Iteration 419, loss = 0.03774308\n",
      "Iteration 420, loss = 0.03818202\n",
      "Iteration 421, loss = 0.03769123\n",
      "Iteration 422, loss = 0.03789698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 92.11746522411129\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 90.72642967542504 %\n",
      "Fold 1: 91.26738794435857 %\n",
      "Fold 2: 91.26738794435857 %\n",
      "Fold 3: 92.19474497681608 %\n",
      "Fold 4: 92.11746522411129 %\n",
      "Average: 91.51468315301392 %\n",
      "Accuracy:  0.8881334981458591\n",
      "Precision:  0.8851269649334945\n",
      "Recall:  0.8948655256723717\n",
      "F1-Score:  0.8899696048632219\n",
      "AUC:  0.8880577628361859\n"
     ]
    }
   ],
   "source": [
    "clf=k_fold_cv_mlp(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=clf.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Accuracy: 80.370942812983\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Accuracy: 80.52550231839258\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Accuracy: 81.45285935085008\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Accuracy: 82.61205564142195\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Accuracy: 78.20710973724884\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 80.370942812983 %\n",
      "Fold 1: 80.52550231839258 %\n",
      "Fold 2: 81.45285935085008 %\n",
      "Fold 3: 82.61205564142195 %\n",
      "Fold 4: 78.20710973724884 %\n",
      "Average: 80.63369397217929 %\n",
      "Accuracy:  0.784301606922126\n",
      "Precision:  0.7486744432661718\n",
      "Recall:  0.863080684596577\n",
      "F1-Score:  0.8018171493469619\n",
      "AUC:  0.7834153422982886\n"
     ]
    }
   ],
   "source": [
    "random_forest=k_fold_cv_rforest(X_Train_Transformed_FeatureMap,Y_Train_FeatureMap.ravel())\n",
    "y_pred=random_forest.predict(X_Test_Transformed_FeatureMap)\n",
    "print(\"Accuracy: \",accuracy_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Precision: \",precision_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"Recall: \",recall_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"F1-Score: \",f1_score(Y_Test_FeatureMap.ravel(),y_pred))\n",
    "print(\"AUC: \",roc_auc_score(Y_Test_FeatureMap.ravel(),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

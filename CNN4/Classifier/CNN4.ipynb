{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "db59c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "042581a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d7c891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf157b46",
   "metadata": {},
   "source": [
    "#Data \n",
    "0->Covid\n",
    "1->No Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2b0622fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['Covid','No Covid']\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1a866119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape all images to 64x64 and apply tensor transformation\n",
    "dataset = torchvision.datasets.ImageFolder(root=\"./Full\",transform=transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Resize([227,227])\n",
    "                                                            # transforms.Grayscale(num_output_channels=1)\n",
    "                                                            ]))\n",
    "# testset = torchvision.datasets.ImageFolder(root=\"./xray\",train=False,transform=transforms.Compose([transforms.Resize([300,305]),transforms.ToTensor()]))\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "91004469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8088\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "trainset,testset,valset=torch.utils.data.random_split(dataset,[round(0.8*len(dataset)),round(0.1*len(dataset)),round(0.1*len(dataset))],generator=torch.Generator().manual_seed(42))\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=batch_size,shuffle=False)\n",
    "valloader=torch.utils.data.DataLoader(valset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4e670c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8088\n",
      "1617.5 404.5\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "trainset,testset=torch.utils.data.random_split(dataset,[round(0.8*len(dataset)),round(0.2*len(dataset))],generator=torch.Generator().manual_seed(42))\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True)\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=4,shuffle=False)\n",
    "print(len(trainset)/batch_size,len(testset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bf09adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 227, 227]) tensor([1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "dataiter=iter(trainloader)\n",
    "images,labels=dataiter.next()\n",
    "print(images.shape,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ce2958ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img):\n",
    "    npimg=img.numpy()\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2023a",
   "metadata": {},
   "source": [
    "# Preparing The CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b17f48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataloader,model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total,correct=0,0\n",
    "        for data in dataloader:\n",
    "            inputs,labels=data\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "            outputs=model(inputs)\n",
    "    #         print(outputs)\n",
    "    #         print(outputs,labels)\n",
    "            m = nn.Sigmoid()\n",
    "            outputs=m(outputs)\n",
    "            pred=outputs>=0.5\n",
    "            pred=pred.flatten()\n",
    "            total+=labels.size(0)\n",
    "            # labels=torch.add(labels,-1)\n",
    "            # print(pred,labels)\n",
    "    #         print(list(map(lambda a: classes[a],pred)),list(map(lambda a: classes[a],labels)))\n",
    "            correct+=(pred==labels).sum().item()\n",
    "    print(correct,total)\n",
    "    model.train()\n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bbeb0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.representation_network=nn.Sequential(\n",
    "            nn.Conv2d(3,32,3), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(32,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=3),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.classification_network=nn.Sequential(\n",
    "            nn.Linear(3136,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        x=self.representation_network(x)\n",
    "#         print(x.shape)\n",
    "        # flattening of the vector=> same dimension of first index(batch size) , everythign else is flattened(-1)\n",
    "        x=x.view(x.size(0),-1)\n",
    "#        print(x.shape)\n",
    "        x=self.classification_network(x)\n",
    "#        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,dataloader,epochs=15):\n",
    "    loss_fn=nn.BCEWithLogitsLoss().to(device)\n",
    "    opt=optim.Adam(params=net.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        for i,data in enumerate(dataloader,0):\n",
    "            inputs,labels=data\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "            opt.zero_grad()\n",
    "            outputs=net(inputs)\n",
    "            labels=labels.unsqueeze(-1)\n",
    "            labels = labels.type_as(outputs)\n",
    "    #         print(outputs)\n",
    "            loss=loss_fn(outputs,labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            del inputs,labels,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2b9cdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN()\n",
    "# net=torch.load(\"./coronaCNN.pt\")\n",
    "# net.load_state_dict(torch.load(\"./coronaCNN_State.pt\"))\n",
    "opt=optim.Adam(params=net.parameters())\n",
    "net=net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3a052375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(net(images.to(device)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a9b09a22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-90eb102edaf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#         print(outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mloss_this_epoch\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_fn' is not defined"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"705286d1-d7f9-4869-bd4a-94d8665029f1\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"705286d1-d7f9-4869-bd4a-94d8665029f1\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell has completed exec\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Cell has completed exec\"\n",
    "loss_arr=[]\n",
    "loss_epoch=[]\n",
    "epochs=5\n",
    "best_model_loss,best_model=1000000,net.state_dict()\n",
    "n_iters=np.ceil(len(trainset)/batch_size)\n",
    "for epoch in range(epochs):\n",
    "    loss_this_epoch=0\n",
    "    loss_arr=[]\n",
    "    for i,data in enumerate(trainloader,0):\n",
    "        inputs,labels=data\n",
    "        inputs,labels=inputs.to(device),labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        outputs=net(inputs)\n",
    "        labels=labels.unsqueeze(-1)\n",
    "        labels = labels.type_as(outputs)\n",
    "#         print(outputs)\n",
    "        loss=loss_fn(outputs,labels)\n",
    "        loss_this_epoch+=loss.item()\n",
    "        loss_arr.append(loss.item())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        del inputs,labels,outputs\n",
    "        if i%100 == 0:\n",
    "            print(f'iteration {i}/{n_iters}, Loss={sum(loss_arr)} ')\n",
    "#             plt.plot(loss_arr)\n",
    "#             plt.show()\n",
    "    if loss_this_epoch < best_model_loss:\n",
    "            best_model_loss=loss_this_epoch\n",
    "            best_model=copy.deepcopy(net.state_dict())\n",
    "    loss_epoch.append(loss_this_epoch)\n",
    "    print(f'Epoch: {epoch}/{epochs}, Loss={loss_this_epoch}')\n",
    "    plt.plot(loss_epoch)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify -m \"completed\"\n",
    "print(evaluation(testloader,net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baac26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),\"./coronaCNN_State.pt\")\n",
    "torch.save(net,\"./coronaCNN.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e295e68",
   "metadata": {},
   "source": [
    "Test Accuracy and K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80659c00",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1398a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "726b77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0faa297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(model,dataset,loss_function,k_folds=5,epochs=10):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    # Initialize optimizer\n",
    "    results = {}\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)): \n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "              dataset, \n",
    "              batch_size=batch_size, sampler=train_subsampler)\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "              dataset,\n",
    "              batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "        # Init the neural network\n",
    "        network = model\n",
    "        network.apply(reset_weights)\n",
    "        optimizer = optim.Adam(network.parameters())\n",
    "        # Run the training loop for defined number of epochs\n",
    "        for epoch in range(0, epochs):\n",
    "\n",
    "            # Print epoch\n",
    "            print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "            # Set current loss value\n",
    "            current_loss = 0.0\n",
    "\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "                # Get inputs\n",
    "                inputs, targets = data\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Perform forward pass\n",
    "                outputs = network(inputs)\n",
    "                targets = targets.unsqueeze(-1)\n",
    "                targets = targets.type_as(outputs)\n",
    "                # Compute loss\n",
    "                loss = loss_function(outputs, targets)\n",
    "\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Perform optimization\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "                current_loss += loss.item()\n",
    "                if i % 500 == 499:\n",
    "                    print('Loss after mini-batch %5d: %.3f' %\n",
    "                      (i + 1, current_loss / 500))\n",
    "                    current_loss = 0.0\n",
    "\n",
    "        # Process is complete.\n",
    "        print('Training process has finished. Saving the trained model.')\n",
    "        save_path = f'./CNN-fold-{fold}.pth'\n",
    "        torch.save(network, save_path)\n",
    "\n",
    "        # Evaluation for this fold\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Iterate over the test data and generate predictions\n",
    "            for i, data in enumerate(testloader, 0):\n",
    "                # Get inputs\n",
    "                inputs, targets = data\n",
    "                inputs,targets=inputs.to(device),targets.to(device)\n",
    "                # Generate outputs\n",
    "                outputs = network(inputs)\n",
    "                m = nn.Sigmoid()\n",
    "                outputs=m(outputs)\n",
    "                pred=outputs>=0.5\n",
    "                pred=pred.flatten()\n",
    "                # Set total and correct\n",
    "                total += targets.size(0)\n",
    "                correct += (pred == targets).sum().item()\n",
    "\n",
    "            # Print accuracy\n",
    "            print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "            print('--------------------------------')\n",
    "            results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    '''\n",
    "    net.load_state_dict(torch.load(\"CNN4_originalstate.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "157b1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Linear(in_features=3136, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=1, bias=True)\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 0.644\n",
      "Loss after mini-batch  1000: 0.500\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.406\n",
      "Loss after mini-batch  1000: 0.359\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.330\n",
      "Loss after mini-batch  1000: 0.308\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.269\n",
      "Loss after mini-batch  1000: 0.285\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.246\n",
      "Loss after mini-batch  1000: 0.249\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   500: 0.216\n",
      "Loss after mini-batch  1000: 0.238\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   500: 0.193\n",
      "Loss after mini-batch  1000: 0.205\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   500: 0.170\n",
      "Loss after mini-batch  1000: 0.198\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   500: 0.189\n",
      "Loss after mini-batch  1000: 0.168\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   500: 0.168\n",
      "Loss after mini-batch  1000: 0.168\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   500: 0.140\n",
      "Loss after mini-batch  1000: 0.138\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   500: 0.162\n",
      "Loss after mini-batch  1000: 0.122\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   500: 0.141\n",
      "Loss after mini-batch  1000: 0.145\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   500: 0.123\n",
      "Loss after mini-batch  1000: 0.128\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   500: 0.107\n",
      "Loss after mini-batch  1000: 0.139\n",
      "Training process has finished. Saving the trained model.\n",
      "Accuracy for fold 0: 93 %\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Linear(in_features=3136, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=1, bias=True)\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 0.694\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.694\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   500: 0.693\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Training process has finished. Saving the trained model.\n",
      "Accuracy for fold 1: 49 %\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Linear(in_features=3136, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=1, bias=True)\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 0.646\n",
      "Loss after mini-batch  1000: 0.480\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.372\n",
      "Loss after mini-batch  1000: 0.361\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.281\n",
      "Loss after mini-batch  1000: 0.301\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.254\n",
      "Loss after mini-batch  1000: 0.244\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.229\n",
      "Loss after mini-batch  1000: 0.225\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   500: 0.203\n",
      "Loss after mini-batch  1000: 0.192\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   500: 0.182\n",
      "Loss after mini-batch  1000: 0.194\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   500: 0.196\n",
      "Loss after mini-batch  1000: 0.180\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   500: 0.161\n",
      "Loss after mini-batch  1000: 0.160\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   500: 0.146\n",
      "Loss after mini-batch  1000: 0.169\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   500: 0.141\n",
      "Loss after mini-batch  1000: 0.132\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   500: 0.133\n",
      "Loss after mini-batch  1000: 0.155\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   500: 0.129\n",
      "Loss after mini-batch  1000: 0.118\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   500: 0.139\n",
      "Loss after mini-batch  1000: 0.135\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   500: 0.105\n",
      "Loss after mini-batch  1000: 0.135\n",
      "Training process has finished. Saving the trained model.\n",
      "Accuracy for fold 2: 92 %\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Linear(in_features=3136, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=1, bias=True)\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 0.658\n",
      "Loss after mini-batch  1000: 0.517\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.390\n",
      "Loss after mini-batch  1000: 0.369\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.311\n",
      "Loss after mini-batch  1000: 0.320\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.273\n",
      "Loss after mini-batch  1000: 0.265\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.236\n",
      "Loss after mini-batch  1000: 0.274\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   500: 0.226\n",
      "Loss after mini-batch  1000: 0.225\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   500: 0.207\n",
      "Loss after mini-batch  1000: 0.220\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   500: 0.187\n",
      "Loss after mini-batch  1000: 0.198\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   500: 0.175\n",
      "Loss after mini-batch  1000: 0.182\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   500: 0.162\n",
      "Loss after mini-batch  1000: 0.178\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   500: 0.151\n",
      "Loss after mini-batch  1000: 0.168\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   500: 0.151\n",
      "Loss after mini-batch  1000: 0.145\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   500: 0.149\n",
      "Loss after mini-batch  1000: 0.134\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   500: 0.162\n",
      "Loss after mini-batch  1000: 0.120\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   500: 0.143\n",
      "Loss after mini-batch  1000: 0.135\n",
      "Training process has finished. Saving the trained model.\n",
      "Accuracy for fold 3: 92 %\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Linear(in_features=3136, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=1, bias=True)\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 0.627\n",
      "Loss after mini-batch  1000: 0.493\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.423\n",
      "Loss after mini-batch  1000: 0.365\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.322\n",
      "Loss after mini-batch  1000: 0.347\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.304\n",
      "Loss after mini-batch  1000: 0.325\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.287\n",
      "Loss after mini-batch  1000: 0.272\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   500: 0.231\n",
      "Loss after mini-batch  1000: 0.232\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   500: 0.237\n",
      "Loss after mini-batch  1000: 0.221\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   500: 0.224\n",
      "Loss after mini-batch  1000: 0.195\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   500: 0.191\n",
      "Loss after mini-batch  1000: 0.197\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   500: 0.163\n",
      "Loss after mini-batch  1000: 0.204\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   500: 0.144\n",
      "Loss after mini-batch  1000: 0.177\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   500: 0.165\n",
      "Loss after mini-batch  1000: 0.158\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   500: 0.134\n",
      "Loss after mini-batch  1000: 0.134\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   500: 0.144\n",
      "Loss after mini-batch  1000: 0.128\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   500: 0.125\n",
      "Loss after mini-batch  1000: 0.140\n",
      "Training process has finished. Saving the trained model.\n",
      "Accuracy for fold 4: 91 %\n",
      "--------------------------------\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 93.81761978361669 %\n",
      "Fold 1: 49.1499227202473 %\n",
      "Fold 2: 92.73570324574962 %\n",
      "Fold 3: 92.73570324574962 %\n",
      "Fold 4: 91.80834621329211 %\n",
      "Average: 84.04945904173107 %\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"75e7dc56-f2c7-4280-aa6a-8b942cc234e2\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"75e7dc56-f2c7-4280-aa6a-8b942cc234e2\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Completed\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Completed\"\n",
    "net=CNN().to(device)\n",
    "loss_fn=nn.BCEWithLogitsLoss().to(device)\n",
    "data_set=trainset\n",
    "k_fold_cv(net,data_set,loss_fn,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_weights(net)\n",
    "train(net,trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f19bcc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493 1618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.27441285537701"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f29672f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net,\"./CNN4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0c42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(dataloader,model):\n",
    "    y_true,y_pred=torch.tensor([]),torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total,correct=0,0\n",
    "        for data in dataloader:\n",
    "            inputs,labels=data\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "            outputs=model(inputs)\n",
    "    #         print(outputs)\n",
    "    #         print(outputs,labels)\n",
    "            m = nn.Sigmoid()\n",
    "            outputs=m(outputs)\n",
    "            pred=outputs>=0.5\n",
    "            pred=pred.flatten()\n",
    "            y_true=torch.cat((y_true,copy.deepcopy(labels.cpu())),0)\n",
    "            y_pred=torch.cat((y_pred,copy.deepcopy(pred.cpu())),0)\n",
    "#             print(y_pred,y_true,y_pred==y_true,pred==labels)\n",
    "            total+=labels.size(0)\n",
    "            # labels=torch.add(labels,-1)\n",
    "            # print(pred,labels)\n",
    "    #         print(list(map(lambda a: classes[a],pred)),list(map(lambda a: classes[a],labels)))\n",
    "            correct+=(pred==labels).sum().item()\n",
    "#             print((pred==labels).sum())\n",
    "    print(\"Accuracy: \",accuracy_score(y_true,y_pred))\n",
    "    print(\"Precision: \",precision_score(y_true,y_pred))\n",
    "    print(\"Recall: \",recall_score(y_true,y_pred))\n",
    "    print(\"F1-Score: \",f1_score(y_true,y_pred))\n",
    "    print(\"AUC: \",roc_auc_score(y_true,y_pred))\n",
    "    print(correct,total)\n",
    "#     print(y_true,y_pred)\n",
    "    y_pred=y_pred.flatten()\n",
    "    y_true=y_true.flatten()\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    model.train()\n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d286879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "560ad72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9227441285537701\n",
      "Precision:  0.9179734620024126\n",
      "Recall:  0.9303178484107579\n",
      "F1-Score:  0.9241044323011537\n",
      "AUC:  0.922658924205379\n",
      "1493 1618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.27441285537701"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(testloader,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2be7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
